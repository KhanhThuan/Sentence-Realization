Test Cumulative: 0
Making Dependency Statistics ....
243606
0
168292
corpara: 9738
Run Indexing => 
1446, 2274, 6645, 5375, 6395, 9606, 191, 6358, 3123, 6822, 9142, 6235, 2582, 2441, 9572, 9219, 874, 4841, 5372, 3909, 3267, 7297, 240, 9571, 9212, 4126, 6340, 9368, 9710, 3373, 3347, 3873, 4237, 5611, 5547, 344, 1343, 4412, 5069, 3316, 7080, 7875, 3389, 4361, 7682, 4833, 4063, 947, 2866, 5772, 6442, 7768, 2344, 4928, 2493, 117, 4674, 66, 5119, 5409, 5447, 5695, 7577, 9416, 6446, 5229, 6593, 454, 5982, 3906, 1337, 76, 7600, 8260, 7403, 4751, 7341, 6210, 9027, 908, 5030, 7559, 156, 4821, 3816, 4867, 3615, 8450, 1995, 7192, 9631, 9412, 5550, 397, 5613, 5575, 6302, 6621, 968, 5048, 7084, 6146, 6322, 3667, 1171, 6003, 8001, 4049, 8882, 2317, 1119, 1566, 308, 651, 3928, 2488, 2279, 284, 6143, 5529, 1494, 5012, 5844, 9008, 1988, 5242, 6623, 3534, 3528, 7086, 4585, 374, 5479, 6084, 1, 504, 3676, 2499, 860, 2430, 8617, 4981, 192, 9659, 7738, 8171, 7083, 8461, 5244, 4449, 8903, 2502, 1277, 3235, 2825, 4289, 1616, 3073, 2780, 234, 2778, 576, 8798, 1714, 7268, 7713, 64, 2985, 5768, 183, 1210, 1679, 6730, 6103, 7891, 586, 7557, 4858, 650, 4510, 5020, 4123, 6681, 7973, 926, 1377, 1067, 653, 237, 1151, 2833, 819, 3436, 4720, 4848, 7754, 696, 7678, 9244, 2288, 8639, 7195, 9539, 3717, 9208, 6678, 3159, 737, 8725, 2544, 8302, 9533, 1245, 5009, 8214, 4281, 2531, 2311, 2459, 4268, 8115, 3098, 5760, 5521, 9232, 507, 2319, 9394, 3363, 6875, 203, 9201, 4786, 5993, 7692, 1604, 3127, 4378, 6506, 3277, 6141, 890, 2119, 3311, 1766, 2808, 6183, 5560, 8799, 2135, 6006, 6253, 8045, 3513, 7685, 5698, 7342, 743, 6474, 120, 1744, 2994, 8792, 2283, 7035, 2638, 1950, 9239, 6804, 1782, 6606, 5814, 2173, 540, 6614, 4958, 8216, 6070, 982, 2519, 846, 2873, 2293, 5870, 5974, 5326, 7198, 2179, 3446, 3016, 6846, 430, 6854, 8650, 1091, 5443, 8596, 4210, 2551, 296, 7477, 6632, 6204, 2147, 2640, 185, 751, 4909, 7490, 6954, 971, 9041, 1293, 3586, 2410, 1544, 6477, 5604, 8601, 5681, 5144, 4584, 3821, 1364, 843, 1027, 3888, 9679, 2740, 8755, 4083, 9513, 7176, 9165, 4261, 9096, 1439, 8843, 1349, 4114, 5819, 4600, 2818, 6012, 9064, 5461, 3910, 8614, 4470, 4798, 9409, 8451, 1465, 2795, 6027, 7772, 3567, 9044, 7628, 8203, 7757, 7701, 2303, 2087, 776, 2637, 3474, 4216, 3372, 7087, 9099, 3823, 8419, 4522, 8356, 5221, 2991, 1534, 4189, 4697, 1154, 4620, 420, 7133, 1562, 4317, 7911, 8663, 8387, 5941, 2697, 6007, 6736, 5485, 806, 2533, 1693, 4030, 7806, 2202, 7457, 1229, 2131, 6930, 5115, 9308, 1849, 3064, 6577, 4492, 1462, 7502, 7430, 213, 5782, 1445, 5498, 1815, 4353, 2063, 3629, 2205, 6950, 4762, 5487, 7352, 3581, 3397, 468, 9126, 6162, 1595, 1725, 63, 1514, 6123, 1082, 3181, 6921, 7752, 1836, 7580, 5593, 7474, 2229, 1466, 8683, 4389, 1444, 2647, 7337, 3058, 9139, 4499, 6932, 5526, 8824, 5101, 5395, 5253, 3487, 6310, 5867, 7721, 8626, 1739, 2539, 4419, 1432, 1086, 9713, 7469, 1034, 976, 3650, 8002, 2057, 2759, 8283, 9037, 3308, 3370, 3248, 1919, 8513, 3991, 513, 7749, 4919, 3194, 6649, 141, 4290, 4278, 4662, 2049, 2297, 9354, 4142, 1777, 6960, 300, 6983, 8999, 8928, 7712, 4708, 7304, 8165, 5175, 623, 9645, 6602, 9383, 861, 7108, 1199, 9446, 7410, 5628, 8674, 7793, 348, 4509, 8038, 3034, 366, 7463, 3499, 994, 9085, 2183, 3559, 8920, 4913, 3005, 3892, 5846, 9390, 45, 4903, 8550, 3769, 4443, 5580, 121, 8071, 7306, 6193, 9357, 5751, 336, 577, 8521, 5657, 7674, 4220, 5678, 5536, 4060, 8794, 7285, 7848, 1335, 1341, 3532, 4478, 8246, 7270, 8618, 1387, 8652, 5075, 7948, 7889, 7512, 2851, 852, 2245, 1406, 7301, 987, 2284, 4729, 5451, 4249, 1572, 1472, 7305, 174, 7586, 1762, 8130, 2936, 4444, 9433, 353, 759, 1924, 4682, 2620, 1856, 5389, 6025, 7739, 7812, 5460, 2108, 4040, 6790, 8148, 2854, 4551, 2751, 544, 1670, 4609, 8516, 84, 8742, 2371, 4887, 7380, 608, 3068, 2120, 2775, 4316, 7900, 1998, 5672, 7340, 3701, 354, 4653, 7438, 2586, 2017, 7027, 457, 3687, 1949, 2762, 4271, 1615, 5494, 8808, 8385, 2314, 5332, 616, 6807, 1242, 3238, 7867, 9301, 3182, 1775, 3750, 6972, 2398, 2133, 4495, 3673, 7466, 4511, 378, 8265, 5873, 2436, 5706, 1390, 5923, 9343, 4983, 3683, 3346, 745, 4002, 4846, 8095, 9292, 3450, 1369, 1555, 795, 4688, 3464, 7881, 3072, 7185, 4012, 8661, 5073, 6612, 6723, 2241, 9020, 9168, 5866, 1927, 6408, 3431, 5152, 2684, 1023, 5135, 3356, 6334, 6722, 4678, 2453, 6802, 9567, 4070, 8089, 478, 8769, 3753, 3157, 1265, 2322, 3552, 8624, 8409, 9164, 3124, 1658, 1951, 6246, 8561, 4148, 2567, 8000, 7649, 4080, 5770, 61, 3327, 201, 1876, 4416, 2614, 2529, 4498, 4345, 2357, 9733, 1411, 4773, 7890, 8344, 5627, 401, 1046, 1060, 1507, 2540, 9470, 4067, 9176, 4451, 2929, 6927, 6880, 5922, 4202, 1279, 4068, 9460, 1237, 5324, 313, 536, 8065, 7834, 1323, 5214, 6783, 6533, 2964, 8454, 8487, 2374, 8242, 7464, 5912, 7961, 5011, 7552, 6242, 6047, 1518, 9650, 7598, 260, 1374, 6964, 2385, 9634, 5083, 2250, 3057, 7254, 8782, 8811, 3916, 3319, 4488, 8948, 8562, 7170, 6845, 8101, 4631, 5481, 6740, 486, 4559, 5364, 7656, 8870, 3558, 4888, 5272, 1272, 9667, 837, 8559, 8647, 4354, 2219, 4149, 7962, 4115, 7379, 6724, 5213, 9355, 7943, 7625, 1723, 4957, 346, 7300, 5577, 5944, 48, 7234, 8966, 4855, 563, 3940, 3711, 7082, 7956, 9674, 9247, 6078, 1952, 7237, 5146, 4715, 5017, 2865, 6318, 7707, 8141, 6015, 2084, 6677, 7515, 1340, 6940, 7321, 4783, 413, 8057, 4009, 6516, 2980, 2416, 4908, 1318, 5474, 1267, 4875, 3205, 8217, 4484, 8149, 5114, 5929, 4576, 5256, 3843, 1694, 7894, 1928, 5802, 3460, 5202, 6369, 4586, 7671, 2478, 8758, 3514, 1624, 4548, 1811, 7856, 4822, 4966, 7399, 521, 1008, 9098, 8790, 3089, 215, 1754, 6530, 3941, 1541, 1116, 3481, 1527, 7427, 3688, 3332, 7, 246, 4977, 425, 8987, 7906, 24, 7992, 6901, 1491, 1583, 1904, 7088, 4780, 992, 7232, 5401, 1274, 3031, 2843, 1032, 7387, 8146, 447, 791, 9194, 8704, 5386, 983, 2997, 7613, 5504, 8620, 4325, 2188, 4992, 632, 4055, 2086, 763, 7590, 9045, 6957, 9258, 5281, 1672, 2329, 3078, 9661, 6988, 9663, 4619, 5931, 1051, 4050, 3152, 83, 6367, 7191, 3345, 673, 9688, 9111, 670, 5815, 2296, 7634, 6352, 1490, 2054, 491, 2469, 753, 4844, 7130, 1962, 5441, 3655, 3376, 8651, 5765, 3184, 8576, 2175, 2711, 1383, 441, 3300, 3729, 3939, 6673, 9504, 7475, 7153, 4912, 4276, 4591, 8939, 6475, 4861, 7067, 1632, 658, 2151, 302, 7264, 8353, 2150, 4911, 5792, 802, 6597, 4635, 4482, 5149, 9032, 1786, 6450, 2916, 8094, 5057, 5806, 1619, 9303, 5798, 4288, 9265, 6066, 4866, 2710, 9406, 3000, 3612, 816, 4018, 3094, 3093, 7710, 9681, 4279, 2472, 939, 1489, 5082, 3506, 7124, 840, 3656, 6432, 4673, 5553, 4459, 2537, 8719, 2207, 1809, 5994, 588, 7882, 9074, 7830, 9197, 7764, 3904, 4503, 6065, 5164, 488, 8028, 8958, 293, 7350, 8673, 4641, 2255, 2148, 7645, 592, 8020, 911, 3955, 7370, 5036, 4079, 7244, 4565, 7271, 131, 6935, 6882, 2341, 781, 3496, 3887, 6924, 1961, 7476, 6911, 1484, 9445, 6776, 6429, 7047, 907, 4886, 4104, 3393, 3561, 2836, 3253, 8273, 848, 9289, 6351, 3030, 1602, 5418, 6553, 6171, 90, 6562, 6716, 7266, 9040, 3908, 8466, 132, 1872, 683, 2566, 5312, 8705, 3737, 3315, 4234, 3021, 4255, 5947, 3126, 878, 4538, 1930, 7667, 855, 2585, 6309, 3262, 8402, 4486, 424, 5255, 1862, 5081, 6720, 1025, 3454, 6192, 979, 5884, 4239, 5874, 6733, 3907, 1603, 8219, 7633, 6089, 8309, 1834, 69, 5670, 6773, 6316, 8573, 1248, 2232, 9424, 4929, 70, 3439, 8795, 7316, 4253, 4987, 3364, 5876, 2768, 3144, 6488, 4618, 8872, 7121, 4445, 4852, 1825, 1402, 6622, 6903, 2216, 1228, 1131, 944, 6841, 5366, 5379, 7527, 9622, 9053, 1537, 8885, 4594, 6161, 1101, 7227, 3846, 9088, 7008, 8643, 6371, 9115, 7452, 687, 4231, 6792, 765, 1325, 1328, 5354, 8878, 7478, 4997, 8675, 3111, 5915, 4627, 9338, 1621, 1662, 1746, 3269, 5483, 4953, 2634, 9325, 5396, 7302, 2904, 9417, 8729, 2704, 2930, 5352, 5072, 8280, 1223, 9441, 379, 3508, 9671, 2888, 6472, 6456, 9223, 8058, 5810, 3802, 2337, 5600, 6216, 6069, 6713, 7178, 331, 8086, 2561, 8881, 6063, 5459, 4385, 8880, 525, 8591, 3643, 1234, 8444, 8032, 9552, 7498, 4504, 6257, 1800, 5891, 8173, 5992, 6019, 3369, 2391, 7101, 928, 3212, 2686, 4362, 8933, 290, 6667, 9640, 2390, 8537, 1667, 5522, 2373, 5559, 1376, 6810, 28, 8963, 8603, 2302, 2861, 1852, 6151, 6798, 9413, 8127, 9577, 4714, 7795, 5068, 1469, 3110, 8357, 3386, 1964, 4532, 1550, 547, 5650, 603, 3716, 3599, 7941, 4535, 2744, 6005, 5824, 7960, 7847, 4377, 7037, 9314, 7844, 8698, 2662, 6262, 9275, 8034, 8426, 3556, 5127, 8, 1410, 3024, 4350, 8937, 717, 5040, 9038, 1075, 841, 4756, 3700, 6699, 3472, 8383, 4203, 2895, 1983, 6076, 5438, 9296, 7360, 8726, 11, 1375, 6625, 4964, 6326, 4258, 8072, 7787, 5946, 836, 3242, 4254, 600, 6919, 2078, 9210, 8730, 4052, 1142, 2943, 6920, 5939, 5299, 8415, 9315, 1205, 1758, 9237, 152, 3896, 1516, 851, 1195, 1304, 3164, 7351, 97, 797, 1294, 8684, 3774, 3165, 3368, 2224, 5374, 1788, 210, 4737, 9463, 5283, 6992, 105, 7929, 2363, 1682, 8167, 4547, 2901, 7562, 6153, 5274, 1397, 5917, 8888, 6830, 9334, 7372, 6086, 4972, 320, 1701, 9474, 5525, 5759, 6430, 8269, 4878, 930, 1436, 8506, 3905, 3805, 6717, 3207, 798, 6929, 642, 4579, 2298, 4910, 221, 8886, 6251, 434, 7939, 8623, 7488, 3427, 8163, 4101, 1916, 4561, 9566, 1959, 9089, 1405, 9477, 6308, 8715, 854, 1547, 7014, 1412, 4157, 8709, 1185, 8128, 4201, 5886, 6073, 9530, 9174, 6686, 2671, 8866, 8090, 8568, 8594, 6535, 3613, 3801, 6030, 4889, 1702, 2591, 3013, 7312, 8512, 1386, 8711, 2064, 5561, 5125, 2172, 4990, 4401, 1634, 906, 7980, 2892, 8365, 1338, 9558, 3685, 4937, 7662, 4649, 7150, 5510, 4563, 7280, 3946, 8604, 4206, 7278, 1947, 9324, 2454, 4134, 3365, 8530, 8600, 8331, 7359, 8313, 3971, 6317, 3674, 5589, 4742, 3609, 7493, 9442, 1458, 5938, 3915, 1588, 9182, 2616, 7295, 7578, 720, 6018, 8900, 5209, 739, 581, 4318, 2847, 5341, 1559, 2580, 1965, 1373, 3466, 8299, 4307, 4621, 3471, 8897, 1487, 3977, 2189, 9127, 725, 7760, 4524, 7816, 5720, 1419, 6906, 6755, 1586, 5390, 5848, 6595, 4305, 2422, 6871, 5445, 3276, 990, 8640, 3322, 6925, 9407, 2912, 8026, 2494, 6053, 8773, 9272, 8396, 391, 8274, 9059, 8364, 2875, 8823, 9218, 4915, 4031, 2235, 1078, 656, 5245, 8770, 1960, 8010, 2773, 3574, 6588, 7166, 5039, 4774, 9300, 9060, 5480, 3375, 523, 7332, 8891, 169, 8338, 3689, 764, 1921, 1394, 109, 3734, 1145, 9328, 636, 375, 2258, 2012, 9086, 8042, 1508, 5715, 6224, 5829, 6847, 5318, 1180, 3779, 3824, 388, 6876, 6956, 770, 6914, 9104, 8902, 707, 9178, 3680, 8957, 6554, 2927, 6548, 3082, 1753, 5572, 1363, 1302, 8221, 5430, 3681, 9680, 7523, 969, 937, 114, 5426, 2631, 2460, 9384, 427, 5899, 7782, 1931, 9356, 2512, 8833, 1647, 7241, 6020, 7866, 1984, 7480, 6492, 4199, 1360, 5509, 7118, 4399, 4411, 256, 1750, 7758, 6338, 5243, 6034, 3820, 4477, 8607, 2118, 8334, 8310, 8819, 8480, 6870, 2042, 3677, 2592, 9603, 3647, 549, 2755, 9250, 7168, 7446, 6411, 7058, 5789, 1429, 9545, 4608, 3746, 8836, 1269, 8780, 275, 5092, 1356, 9476, 5916, 2137, 6900, 8774, 6785, 6296, 6569, 6110, 9599, 9696, 4481, 1594, 6478, 1826, 3282, 7579, 8921, 6939, 9200, 957, 3318, 9554, 7217, 1629, 2242, 9472, 4312, 6655, 3168, 8772, 4021, 7612, 167, 1606, 6254, 1122, 6987, 5300, 5534, 4690, 8835, 1298, 6111, 2877, 5588, 7048, 3857, 8496, 6099, 7987, 7615, 2606, 8629, 8736, 6585, 5237, 2005, 3703, 7627, 1955, 1688, 4223, 4137, 3234, 3610, 9499, 2589, 93, 7719, 6168, 8976, 8507, 2681, 5178, 677, 9593, 4432, 2272, 4487, 7673, 7614, 2557, 5392, 4745, 4175, 2641, 7174, 8412, 4518, 2278, 2708, 796, 7635, 8967, 3954, 5143, 7029, 4061, 8828, 6339, 8261, 8133, 6422, 5167, 3535, 8728, 8689, 1837, 8727, 9514, 3782, 8172, 8200, 1897, 1790, 9144, 995, 3874, 7766, 9618, 2434, 6970, 9401, 5838, 2741, 2600, 4457, 129, 601, 3344, 2601, 2918, 6865, 4768, 8998, 1989, 5749, 6600, 145, 2562, 6750, 5813, 2280, 7657, 7151, 2394, 1480, 9503, 675, 8895, 8717, 5649, 7343, 5172, 2212, 4460, 4765, 1581, 4800, 4430, 8654, 4435, 142, 3878, 8190, 5216, 4003, 1509, 7852, 9133, 5417, 2909, 2310, 1637, 3607, 6082, 1530, 8367, 8441, 6342, 6743, 9662, 2992, 4830, 1068, 3879, 1792, 4820, 4461, 4227, 2810, 3786, 9092, 3170, 3218, 6121, 2425, 2672, 5858, 422, 8587, 5236, 8225, 6469, 9454, 4062, 1895, 3962, 5518, 1557, 524, 5410, 7668, 3845, 2835, 7565, 7243, 8757, 81, 2786, 1681, 9047, 2277, 2774, 3206, 4520, 1238, 1724, 2506, 5766, 2577, 4368, 6470, 7832, 6480, 8319, 5016, 1747, 5131, 8830, 1511, 5433, 9279, 6781, 9169, 5203, 3518, 2669, 674, 3727, 1233, 3158, 7798, 3053, 8063, 3984, 1005, 9437, 5704, 5215, 2766, 222, 2885, 1194, 2223, 8228, 3881, 490, 5199, 3569, 8867, 4418, 6283, 2673, 6537, 462, 9114, 4694, 8578, 808, 6347, 7876, 1347, 3113, 9110, 176, 6055, 1177, 2368, 2705, 8797, 664, 102, 1130, 8860, 692, 7940, 4984, 6605, 5308, 7291, 1764, 3443, 6517, 2136, 7831, 9172, 6737, 1022, 2203, 741, 5100, 5701, 648, 5508, 1286, 3394, 8458, 5666, 1756, 3362, 4761, 5113, 9451, 7222, 4280, 3632, 3409, 4946, 4311, 7730, 7005, 1280, 5953, 7778, 6884, 8899, 483, 1871, 1315, 6337, 9050, 3745, 7448, 6952, 5909, 9623, 6631, 5578, 6330, 5970, 562, 9087, 9657, 6104, 2779, 1385, 825, 6398, 2706, 9120, 2545, 4195, 786, 7630, 5883, 8013, 1810, 9443, 1468, 4324, 6043, 8051, 5732, 1521, 6650, 3813, 4607, 1643, 3841, 6345, 3227, 9714, 1263, 6464, 2215, 582, 2530, 8810, 3576, 4749, 5961, 4982, 9149, 8052, 5691, 6198, 65, 3999, 3762, 3781, 8918, 4501, 3550, 8278, 9160, 5887, 6742, 5981, 6273, 8144, 6656, 630, 3419, 1890, 8391, 1188, 1190, 5788, 1847, 4136, 1536, 2275, 5863, 6354, 2090, 6218, 6626, 4755, 1495, 5420, 2569, 7672, 5804, 2138, 5546, 3597, 5388, 682, 6653, 7085, 1830, 1139, 823, 2670, 9693, 3229, 3921, 5614, 8022, 6668, 7104, 5616, 456, 2872, 3777, 5524, 4164, 4630, 2132, 9, 5329, 4716, 9257, 9145, 873, 4962, 3547, 9682, 8750, 1628, 757, 5412, 9039, 8074, 6313, 2076, 9414, 7203, 1316, 7560, 403, 5182, 4767, 5140, 9345, 9248, 2613, 8472, 5539, 9694, 139, 5228, 3842, 4262, 3201, 5517, 1903, 4636, 2450, 8645, 3374, 5356, 3757, 6426, 2597, 9337, 5468, 3313, 3275, 7376, 6038, 1333, 9057, 3467, 5226, 5044, 1442, 5063, 2165, 5786, 7404, 1099, 3051, 2062, 661, 1738, 6049, 6410, 4178, 8738, 6461, 7644, 7444, 6365, 1717, 4273, 4834, 1617, 211, 1869, 146, 4947, 3865, 6029, 2931, 1016, 8801, 9137, 1454, 7068, 3577, 3043, 2513, 6148, 8528, 4005, 637, 8138, 8490, 4490, 6834, 1939, 8767, 6159, 5118, 7695, 3295, 4638, 3657, 5054, 7631, 7589, 7235, 1711, 6557, 5667, 8439, 6575, 602, 3020, 2806, 7535, 6379, 4015, 9147, 6184, 629, 2785, 5800, 4537, 9507, 2963, 2208, 4064, 787, 6965, 3800, 931, 6549, 289, 6651, 1599, 1824, 7957, 5462, 5424, 3090, 1057, 8621, 3033, 7597, 7391, 8686, 9261, 116, 259, 8030, 2691, 9404, 2830, 5091, 8462, 6628, 5382, 8669, 2080, 6599, 6102, 6797, 6784, 7927, 3554, 8776, 2177, 2650, 1486, 115, 5108, 5475, 1059, 1652, 6494, 8911, 3379, 6000, 5393, 2351, 4270, 2944, 2630, 6473, 68, 5548, 5793, 3658, 4122, 7371, 8372, 7647, 2004, 3461, 575, 9148, 7532, 101, 3077, 945, 3772, 8401, 1115, 138, 8535, 811, 1089, 9385, 4622, 1796, 7858, 6010, 9642, 8084, 3890, 8325, 7283, 4132, 323, 5979, 5826, 4764, 4382, 5313, 1791, 9277, 3150, 7286, 8991, 3473, 8196, 6031, 2692, 2905, 204, 5808, 3455, 8732, 4343, 8118, 7650, 1828, 6596, 7822, 7228, 8040, 1443, 9639, 2554, 4775, 9466, 2213, 2942, 2729, 9701, 1158, 5440, 3616, 5514, 9313, 3382, 8501, 3062, 1434, 6995, 3143, 2285, 4096, 3665, 800, 3883, 6406, 267, 6526, 1281, 4420, 9715, 4717, 6343, 8151, 1549, 7194, 7926, 7530, 0, 2185, 282, 3017, 6786, 8982, 4727, 6277, 9021, 4393, 5378, 4436, 294, 6874, 9452, 4659, 8857, 4733, 8259, 9425, 4205, 2104, 7945, 7706, 9374, 3422, 951, 3583, 918, 6899, 2110, 5617, 1079, 6332, 7432, 8821, 4193, 2632, 8523, 3675, 910, 4917, 9305, 3367, 1148, 8145, 638, 7575, 1284, 6558, 4274, 2856, 8081, 3799, 6187, 3161, 2733, 4246, 6640, 6247, 3343, 7367, 3718, 9297, 3335, 9203, 850, 4082, 4016, 2573, 6948, 726, 5662, 2974, 2819, 2900, 4106, 3233, 2588, 1685, 2458, 8949, 4750, 6412, 584, 1125, 1243, 4138, 5010, 1882, 2508, 1030, 839, 6243, 9195, 7396, 4211, 2509, 8846, 9158, 2886, 7546, 3604, 3850, 2715, 3770, 3970, 2605, 3219, 5018, 459, 2276, 2629, 5338, 195, 6890, 4879, 933, 5820, 9400, 7596, 7805, 3666, 2490, 3882, 6067, 6769, 8482, 4165, 200, 3827, 5910, 7607, 4546, 2198, 8944, 9246, 7044, 8853, 8004, 1433, 6214, 528, 9492, 5535, 9188, 404, 7324, 6325, 6500, 7031, 269, 3348, 7549, 4568, 4924, 4826, 3503, 1666, 3758, 2510, 6295, 8465, 7792, 245, 5363, 1797, 9256, 4404, 7402, 4718, 7877, 2871, 4516, 900, 5224, 4515, 6258, 2957, 2126, 479, 6017, 2032, 6336, 2462, 9619, 7386, 1114, 5932, 611, 1449, 5179, 2894, 5998, 9082, 6234, 6779, 8756, 5968, 1464, 5456, 9284, 470, 7382, 659, 1612, 7781, 5194, 4760, 5755, 6091, 7303, 2253, 6646, 1861, 4932, 7727, 338, 4291, 9651, 8492, 6463, 7937, 2694, 1278, 7708, 6866, 5603, 940, 3169, 5106, 1636, 8317, 393, 3241, 5605, 5013, 8323, 2009, 7990, 9498, 6848, 5286, 2051, 4590, 3490, 1305, 7715, 2622, 7112, 6115, 3230, 2959, 7282, 1273, 5730, 14, 8608, 9022, 4359, 8980, 3028, 5184, 7969, 3544, 6860, 25, 6635, 1207, 5195, 6189, 2294, 7177, 2608, 2184, 8484, 4222, 4260, 974, 8339, 7510, 4428, 610, 335, 3641, 6482, 8892, 179, 4950, 1039, 5422, 7859, 3052, 640, 5262, 8555, 364, 1463, 2984, 3125, 3722, 6571, 8368, 1232, 8231, 8533, 5141, 5976, 9000, 9028, 6976, 9329, 1641, 6324, 9486, 9295, 2718, 5470, 4969, 9381, 7978, 6636, 4294, 5287, 3027, 2243, 2162, 1107, 9393, 1689, 5668, 8413, 8174, 4883, 1565, 5282, 4936, 1977, 7002, 8041, 2945, 198, 2140, 3796, 6144, 5601, 2286, 3501, 8665, 9724, 2536, 8012, 6837, 991, 9280, 2197, 7813, 8293, 4069, 9648, 3, 8855, 2998, 6966, 724, 5942, 3036, 731, 2982, 7720, 6211, 209, 8237, 7113, 6684, 4342, 1582, 2348, 3459, 8033, 7791, 909, 2181, 1045, 3252, 4890, 51, 7096, 6806, 8659, 1121, 3815, 7165, 4196, 4527, 6238, 1482, 828, 7065, 1221, 3831, 9660, 9033, 8416, 820, 8112, 5791, 3631, 1070, 1096, 4087, 439, 2456, 3981, 824, 3049, 6384, 6762, 446, 6998, 144, 280, 4119, 2355, 829, 1475, 7308, 9518, 5122, 9455, 357, 9586, 8125, 8825, 6236, 5065, 1181, 8212, 4045, 9500, 4320, 232, 5664, 5752, 8471, 1535, 4666, 2996, 5097, 3220, 2893, 9068, 6756, 7503, 9722, 1319, 5176, 6056, 6942, 7566, 9485, 3097, 6124, 6805, 7658, 5348, 4034, 7443, 2602, 4375, 8478, 1772, 6416, 7988, 4383, 9333, 7437, 4567, 4380, 8842, 2731, 5047, 2270, 5132, 184, 1794, 7216, 5919, 5289, 1622, 1982, 2100, 3360, 7344, 5585, 6644, 4007, 6534, 7603, 3255, 5683, 4172, 2356, 7348, 1368, 3626, 6504, 9723, 5654, 5937, 3433, 821, 8688, 9654, 4315, 5196, 1476, 7423, 3221, 6536, 1798, 1146, 5809, 2800, 2717, 4549, 7769, 4686, 5949, 8400, 17, 7654, 3391, 9260, 5984, 1186, 4292, 8906, 414, 1019, 3851, 8558, 4840, 3310, 3342, 7102, 8768, 440, 5646, 1391, 8930, 1128, 2408, 9562, 7563, 3047, 7901, 3312, 2433, 8369, 3107, 1677, 7924, 570, 5339, 2447, 1757, 621, 3797, 4921, 7783, 5448, 2657, 5581, 1506, 5619, 9439, 8657, 6662, 7349, 7747, 9524, 7921, 9716, 9365, 5343, 7164, 4414, 1726, 170, 2045, 2958, 870, 7319, 175, 6449, 6328, 5659, 1992, 9320, 2552, 5210, 1721, 2677, 9095, 904, 3263, 9512, 3032, 9573, 3187, 4476, 3987, 9484, 7260, 8505, 7366, 4285, 6524, 4528, 5059, 8934, 1502, 5936, 7950, 9450, 2976, 223, 3448, 2727, 7181, 975, 555, 8106, 8327, 2962, 4536, 6811, 1651, 6394, 5041, 2190, 6676, 4587, 719, 1812, 3817, 8476, 5171, 3405, 9616, 1225, 9553, 7326, 6660, 7218, 4372, 884, 4275, 9415, 1675, 3709, 4595, 6021, 349, 8816, 7449, 9366, 4162, 4217, 8968, 5310, 3086, 9581, 6550, 7519, 1755, 9582, 8787, 7786, 5702, 8922, 7311, 6579, 8510, 2703, 3545, 2811, 3931, 2652, 2129, 3432, 7272, 2576, 3627, 1160, 9286, 9526, 9497, 2831, 1773, 7021, 2651, 9617, 2468, 9231, 4168, 5150, 2883, 7811, 188, 9399, 7145, 2370, 6801, 4995, 448, 5845, 2574, 7675, 546, 1857, 8320, 6085, 6402, 1317, 2437, 6697, 119, 9049, 7190, 9190, 9555, 1698, 5554, 4301, 8871, 4400, 147, 7556, 7338, 7496, 8232, 6291, 8672, 7954, 8087, 7365, 8831, 432, 4876, 8428, 7737, 8520, 1880, 9017, 8043, 1560, 7724, 6634, 8786, 1009, 4027, 7652, 7880, 8635, 3728, 7915, 1905, 9003, 8188, 4028, 3140, 297, 5831, 7653, 8178, 4326, 881, 822, 2157, 5084, 2429, 5610, 3044, 4, 4174, 3542, 1029, 8218, 74, 3384, 898, 3965, 6033, 6672, 6967, 9012, 8849, 9432, 9350, 5298, 1902, 8292, 9336, 520, 8205, 7629, 5803, 3640, 2483, 3185, 8677, 1533, 7355, 4238, 7215, 3968, 4120, 5507, 6709, 9611, 6083, 1164, 6037, 6046, 4145, 4794, 305, 5029, 7788, 3989, 341, 3948, 3562, 761, 2404, 2570, 7910, 1716, 5323, 3961, 5499, 3155, 7998, 6898, 6618, 9264, 4658, 9402, 2244, 5259, 7568, 303, 6230, 6520, 5477, 5690, 4877, 2860, 6208, 2191, 5647, 4000, 892, 7179, 5449, 3134, 7075, 4891, 688, 4541, 406, 6839, 9471, 7004, 2999, 283, 6544, 9326, 9112, 5602, 3818, 5828, 6423, 9423, 5157, 4198, 8932, 4208, 9604, 2769, 6271, 3247, 1769, 831, 189, 2313, 779, 7242, 7288, 5219, 1178, 8706, 7330, 3066, 5206, 5713, 4117, 337, 2815, 3306, 8908, 7274, 8007, 4441, 3440, 2265, 2534, 7642, 7132, 9635, 4985, 8297, 5843, 1167, 263, 4089, 6278, 927, 9537, 1690, 522, 7497, 6540, 3197, 127, 3430, 2636, 4170, 326, 3428, 3420, 6666, 2899, 5609, 3101, 8111, 8380, 7053, 7459, 1416, 417, 6439, 506, 6113, 8754, 8414, 1044, 967, 4699, 1948, 941, 8088, 6833, 7564, 9717, 5952, 1922, 2724, 5064, 7392, 5136, 1236, 1976, 7175, 6058, 1149, 253, 4823, 4854, 1674, 8154, 5024, 4829, 7056, 7893, 2558, 3232, 999, 6679, 7531, 9094, 1878, 7912, 6485, 6001, 2343, 3932, 4464, 1015, 3135, 1973, 3074, 4244, 1296, 2813, 4857, 3301, 3992, 2048, 2164, 1326, 7681, 7986, 8281, 1270, 5078, 924, 8947, 1865, 7362, 2846, 2649, 9281, 1883, 6627, 5527, 8923, 458, 5812, 7823, 6228, 8316, 1822, 5758, 5686, 8854, 6728, 7955, 8914, 2023, 5737, 8526, 9161, 3918, 8554, 9734, 8850, 8207, 2523, 5964, 9543, 1209, 2518, 9100, 6941, 7817, 4634, 3719, 9431, 633, 7820, 3509, 5351, 8694, 9291, 7949, 500, 8992, 6128, 5273, 8153, 1428, 2033, 8753, 1695, 7001, 5856, 5187, 6943, 5748, 541, 949, 3934, 1968, 543, 2077, 2609, 9729, 8080, 9221, 3775, 7808, 4440, 4180, 5957, 777, 6705, 3659, 6702, 4176, 9624, 6993, 1453, 9709, 5365, 7173, 6186, 496, 7186, 5635, 4505, 6414, 9316, 2809, 5624, 2571, 1912, 3179, 3810, 7513, 6584, 3768, 442, 4434, 1073, 1102, 3593, 953, 7406, 7694, 8699, 7225, 8262, 8986, 494, 9004, 7886, 8070, 9506, 9018, 6057, 4473, 1839, 8781, 4185, 714, 2479, 2081, 5368, 9483, 7868, 4129, 2421, 6400, 7110, 4269, 9628, 8985, 1608, 5090, 9241, 8180, 9436, 9171, 2787, 7428, 2227, 2301, 7516, 42, 4583, 7103, 2002, 2105, 7054, 5163, 1093, 4610, 1136, 3091, 7487, 23, 3297, 2626, 6022, 6490, 3579, 7522, 1049, 4927, 3998, 8883, 7389, 2256, 1668, 526, 6383, 6753, 1208, 7030, 6209, 8455, 9146, 4868, 6042, 1627, 2318, 9309, 8775, 804, 3751, 1357, 4974, 3099, 8638, 1829, 612, 126, 2664, 1896, 4802, 9344, 7320, 73, 7750, 1415, 6574, 1367, 52, 9046, 6973, 6613, 5716, 2041, 2975, 4322, 9561, 5528, 6459, 2791, 6984, 6777, 9612, 8460, 5712, 5822, 262, 972, 5665, 6281, 2195, 8328, 2022, 6648, 4352, 6139, 1783, 2028, 5457, 9575, 3794, 2598, 6879, 8213, 7010, 4681, 1456, 9403, 5463, 4597, 1240, 7451, 7904, 5331, 6817, 3288, 3482, 2749, 1166, 1395, 8282, 6945, 8215, 6949, 8446, 9206, 8844, 2898, 6176, 6495, 6510, 9422, 3679, 6851, 9730, 1066, 4306, 1496, 2829, 6836, 3645, 4327, 4485, 9482, 6327, 8524, 4703, 4645, 1220, 3291, 9209, 7872, 3012, 1736, 2753, 1129, 8019, 3899, 7591, 6460, 4856, 7036, 5631, 4025, 7878, 5492, 6867, 7509, 3244, 3510, 7684, 4056, 9283, 8083, 4632, 1596, 5644, 4894, 655, 431, 7506, 746, 8641, 7794, 7666, 9193, 345, 2107, 5930, 8193, 5653, 87, 8271, 3302, 9348, 6502, 6008, 3979, 978, 2746, 36, 8378, 2981, 2525, 2325, 4906, 5990, 2328, 106, 2480, 1832, 882, 3588, 3621, 2680, 9459, 3515, 6199, 7702, 5563, 9035, 3839, 6568, 788, 9646, 7265, 8586, 2611, 57, 4221, 2967, 5342, 8152, 4633, 1380, 4347, 1052, 2541, 7015, 5717, 5583, 5959, 5573, 5275, 4923, 8276, 4707, 7256, 5606, 166, 7189, 6237, 7373, 977, 7163, 1329, 3188, 7558, 3854, 372, 1424, 4853, 5086, 327, 6447, 8162, 5651, 5349, 7454, 8427, 5875, 9388, 2924, 3884, 5622, 889, 6292, 1913, 4588, 1137, 2966, 2380, 2645, 2345, 9509, 2095, 2820, 9487, 2431, 8997, 252, 2237, 2732, 6715, 3735, 6641, 9615, 9737, 1350, 5437, 8377, 31, 9638, 8335, 9034, 6694, 8668, 1515, 2862, 5648, 6934, 7802, 5855, 538, 5035, 7677, 3662, 9643, 5419, 452, 5906, 130, 9502, 902, 8734, 6778, 4013, 9293, 3548, 749, 1426, 514, 7315, 7888, 451, 8570, 7538, 9397, 6688, 597, 2837, 4125, 7107, 5775, 8868, 685, 3132, 6443, 8879, 9270, 827, 438, 4759, 6407, 4241, 418, 6772, 4263, 8371, 3741, 6120, 9353, 2826, 4799, 5677, 4941, 4454, 219, 7807, 7605, 4564, 2455, 8894, 8300, 2015, 3151, 5197, 9569, 7409, 7495, 6303, 8951, 4700, 6683, 6910, 5222, 6360, 7381, 3283, 419, 1891, 2267, 5001, 2953, 4153, 6791, 5062, 8907, 5986, 1802, 7821, 6982, 9386, 3856, 7639, 7646, 6380, 3925, 4808, 3293, 6863, 9128, 4365, 8644, 6182, 5586, 8245, 7641, 1981, 4553, 1041, 402, 8082, 9079, 1980, 2340, 5317, 3381, 8199, 2838, 4971, 2069, 6926, 2668, 4215, 3486, 4642, 1953, 5293, 8192, 6547, 1043, 4417, 8778, 2079, 9243, 9109, 5625, 2055, 7057, 6573, 9346, 1332, 4914, 2855, 4781, 3415, 2604, 2438, 4304, 9347, 9029, 7545, 5327, 4560, 2225, 3095, 2607, 498, 913, 6765, 6126, 3014, 1366, 6260, 2378, 4351, 7637, 6819, 5121, 4677, 3488, 728, 3174, 8121, 5674, 4340, 3833, 2392, 6166, 1479, 8197, 4072, 5594, 1037, 1854, 5643, 2290, 6745, 5852, 2111, 9131, 7230, 2214, 7253, 8275, 4286, 3398, 9652, 689, 7196, 6122, 9664, 1817, 2812, 5633, 8746, 6749, 9525, 4963, 2864, 1504, 3557, 4033, 3565, 3691, 3076, 3011, 4948, 1805, 8508, 8865, 7976, 4053, 6793, 103, 5165, 8404, 2675, 3231, 9702, 7033, 3585, 7669, 916, 2025, 1050, 838, 5544, 3847, 4693, 4073, 2400, 4456, 1626, 3538, 8438, 7745, 2463, 3408, 9083, 3457, 8143, 7298, 8584, 6659, 3257, 7209, 4849, 6969, 5362, 3519, 6452, 1932, 8964, 3195, 4283, 3636, 8352, 7548, 6955, 7129, 2707, 1771, 1740, 2852, 8532, 2221, 2246, 4438, 9076, 9084, 8625, 4863, 3975, 2687, 4605, 5252, 3614, 3967, 4692, 2419, 760, 3200, 463, 9071, 833, 7074, 9205, 6105, 1592, 8893, 6552, 7327, 2956, 3328, 7996, 7970, 4639, 6809, 2443, 9469, 5051, 4381, 3108, 8598, 2828, 5032, 7296, 6386, 9043, 7640, 8791, 482, 1123, 277, 46, 8077, 6782, 5387, 4502, 7622, 1704, 6937, 1875, 2850, 2145, 5918, 2661, 4818, 782, 387, 3619, 3573, 8198, 4489, 3990, 4989, 3837, 3015, 1435, 1403, 395, 6922, 5859, 288, 4793, 1239, 3038, 9727, 5074, 2034, 7935, 3630, 3186, 2902, 7776, 789, 6714, 2358, 9493, 47, 4384, 5056, 1571, 8399, 7426, 8100, 4331, 2234, 3891, 2857, 3203, 7679, 7416, 6642, 9051, 4796, 1843, 304, 4363, 3213, 9534, 4155, 9373, 2233, 5250, 8448, 6761, 6690, 2757, 3065, 6604, 6729, 4391, 554, 8691, 7120, 734, 50, 1649, 7126, 6245, 7275, 2053, 6652, 9475, 9340, 6097, 8305, 2869, 4493, 6508, 2760, 6100, 2384, 6731, 1299, 9579, 7429, 1915, 2667, 1975, 2823, 6357, 1999, 7109, 712, 5148, 5104, 2874, 585, 1768, 7000, 1887, 8186, 2955, 1642, 1638, 1313, 4812, 1172, 5877, 2796, 6095, 3571, 6259, 9106, 6665, 5731, 8627, 3392, 2919, 5888, 4141, 4091, 4111, 812, 272, 2928, 4081, 8135, 7069, 6996, 2635, 1396, 3326, 4625, 3759, 307, 4390, 8700, 436, 9438, 1176, 2196, 7920, 265, 3792, 8379, 7072, 8270, 7561, 7431, 4360, 2396, 4074, 732, 9602, 6630, 2418, 3589, 8696, 8098, 7023, 3256, 7119, 4935, 71, 8784, 7550, 6775, 7032, 1092, 4310, 6272, 9010, 2910, 6137, 4103, 3114, 4204, 3592, 3079, 3723, 3512, 5511, 2639, 3265, 6891, 6072, 2489, 6390, 6747, 948, 5421, 6445, 973, 3465, 1901, 9700, 6732, 6133, 1954, 3575, 6959, 5337, 8588, 1213, 1211, 4816, 8241, 218, 3708, 4500, 3290, 9678, 8116, 6314, 7236, 250, 5292, 4942, 1540, 1222, 5402, 9637, 7193, 8159, 8406, 4108, 5186, 6481, 8515, 2467, 6160, 8622, 7249, 8263, 4483, 6270, 8637, 3804, 7718, 6532, 6244, 604, 4071, 4850, 5799, 8425, 9009, 291, 666, 4602, 4713, 5612, 5159, 1776, 2050, 2058, 9430, 7514, 2603, 8721, 8294, 635, 2915, 1751, 5124, 1793, 8929, 2922, 645, 362, 3173, 5246, 2782, 2719, 1779, 426, 4042, 5279, 4098, 7292, 8804, 8762, 5334, 6513, 8048, 8676, 5008, 7581, 5162, 4462, 4431, 807, 6165, 4900, 6293, 6760, 6194, 9565, 3957, 9236, 8474, 2771, 5431, 7850, 7442, 1523, 3570, 5357, 1684, 8424, 1440, 9173, 271, 1609, 5306, 7017, 3871, 8284, 6290, 9505, 5095, 7081, 748, 4102, 1979, 7252, 5407, 7007, 8445, 5207, 7958, 8977, 6217, 1065, 5618, 4036, 5034, 1285, 9689, 9535, 1470, 9213, 5330, 7729, 5996, 4839, 4465, 9016, 212, 7883, 108, 1192, 9129, 7239, 7051, 5411, 5413, 6907, 3414, 8181, 9370, 3740, 4158, 3617, 5033, 9253, 4247, 7076, 6856, 3191, 8905, 1525, 4901, 6397, 1728, 1259, 461, 1646, 4817, 7415, 3475, 8201, 805, 7554, 8456, 5370, 1430, 6362, 60, 2526, 5991, 532, 1713, 6321, 4669, 3366, 7797, 3914, 3705, 9375, 2550, 9719, 8519, 4975, 4088, 5985, 8567, 6999, 5427, 1064, 9254, 3401, 12, 1327, 3352, 997, 1014, 7968, 2970, 5975, 9226, 9093, 2072, 3555, 7077, 9259, 9220, 9620, 8113, 5599, 5025, 4955, 6827, 4272, 4029, 7334, 6938, 8941, 6718, 2211, 5956, 1109, 2971, 3001, 389, 6382, 5682, 4303, 5738, 2832, 1103, 7471, 4392, 7602, 3994, 8703, 583, 1201, 7204, 9596, 1937, 5217, 5466, 5188, 464, 1958, 3083, 7897, 3458, 9191, 6344, 7773, 1918, 3752, 4811, 220, 6002, 4313, 7149, 1661, 3958, 6, 5398, 5914, 8962, 1497, 4589, 6225, 8517, 1934, 5482, 5007, 5566, 2199, 6060, 3803, 2840, 5973, 2685, 551, 1709, 2538, 9130, 2106, 8370, 618, 7092, 8104, 2031, 1513, 4529, 5423, 2397, 6435, 4453, 6878, 8609, 4978, 5767, 6219, 2989, 7638, 4517, 567, 832, 3307, 8059, 3390, 4112, 1706, 2481, 2575, 3844, 4573, 7414, 9494, 3341, 4143, 3080, 1355, 1524, 7528, 2689, 4173, 4679, 7377, 301, 1601, 2415, 5913, 2587, 8373, 699, 7246, 9585, 3780, 5632, 4893, 4422, 3131, 697, 1946, 9594, 7323, 9468, 4369, 7767, 5762, 1140, 7698, 8540, 558, 4545, 2995, 2056, 2194, 9672, 2972, 5028, 2268, 4643, 373, 6800, 1310, 8405, 1104, 1283, 6403, 1567, 2884, 3546, 8442, 8582, 2304, 755, 5696, 4097, 7833, 9692, 3819, 5925, 4256, 5997, 4093, 2473, 7928, 4880, 5983, 1246, 410, 2617, 1648, 8122, 3956, 5714, 1431, 85, 3271, 9150, 8718, 634, 9162, 2333, 3549, 241, 2083, 7576, 6252, 3371, 2714, 7420, 885, 3411, 5761, 4827, 5530, 7544, 5294, 2949, 4895, 6795, 7884, 7853, 8859, 4650, 6205, 3141, 2287, 9376, 5679, 6741, 2289, 6331, 1715, 4683, 3331, 4771, 2113, 7135, 9151, 6633, 1174, 3522, 569, 5871, 5166, 894, 3653, 552, 8326, 5562, 3952, 9647, 4095, 2625, 8006, 1193, 5355, 94, 3361, 2737, 7293, 6531, 1163, 3355, 4581, 6591, 5098, 7289, 5278, 4637, 3116, 9398, 2360, 7714, 3978, 7079, 3973, 2564, 2068, 380, 9721, 2728, 3410, 3720, 6951, 4671, 1266, 6080, 8666, 7828, 8741, 6759, 8018, 1920, 3525, 2950, 3004, 3867, 1088, 7819, 4980, 4447, 1942, 8925, 9066, 7861, 5903, 7863, 2699, 9621, 6425, 6700, 9214, 9712, 3002, 1382, 5892, 5708, 394, 6726, 7439, 7407, 1816, 8208, 6947, 663, 3935, 7433, 4415, 4788, 5862, 2312, 9668, 4769, 7097, 8950, 4014, 3059, 705, 1438, 6674, 7212, 6675, 3582, 6719, 5235, 7862, 7012, 5193, 37, 285, 8605, 1610, 3303, 4676, 5726, 4044, 5729, 9287, 5117, 3897, 3305, 8548, 9725, 9607, 6196, 5126, 3480, 5841, 5590, 1450, 7529, 56, 3529, 7441, 1334, 4295, 4815, 1993, 3216, 4705, 4838, 1352, 5733, 8873, 9653, 5153, 6315, 7207, 7540, 5050, 4209, 3806, 6616, 7378, 1241, 3284, 8287, 9495, 1069, 4843, 7648, 8563, 6188, 2406, 279, 4048, 4300, 986, 6269, 4652, 4152, 1818, 3664, 1529, 3600, 3788, 6248, 2254, 9389, 5776, 7716, 4085, 5230, 5811, 7134, 7325, 2889, 3704, 7582, 7849, 879, 5721, 4242, 7390, 4043, 1306, 2187, 9080, 4358, 895, 6826, 4951, 9704, 2504, 9307, 4346, 9405, 6897, 9156, 7687, 9429, 8585, 5743, 1218, 7100, 5895, 7947, 9015, 1844, 2790, 5442, 123, 5271, 2424, 8464, 3533, 5, 2485, 6125, 3738, 3449, 7091, 3895, 3767, 5111, 6701, 7518, 7137, 8522, 8494, 2449, 1268, 3162, 9465, 7169, 7240, 1579, 2060, 3103, 5397, 2432, 6695, 5780, 9321, 7570, 9408, 5399, 9580, 8788, 2859, 7063, 6079, 1165, 3462, 8268, 8693, 3790, 5827, 8348, 5882, 3209, 744, 7412, 7122, 3286, 7491, 9136, 2088, 7223, 1033, 9547, 4006, 5557, 8740, 950, 6440, 299, 8109, 589, 453, 6746, 9708, 7136, 3995, 26, 4904, 499, 400, 6119, 7183, 1657, 3178, 9225, 6766, 9418, 9456, 3832, 5639, 2347, 736, 4869, 1542, 8243, 8005, 1346, 1842, 9327, 4944, 4943, 1990, 112, 5288, 2402, 1761, 8917, 3470, 9644, 2339, 4613, 208, 8904, 9271, 4472, 2248, 7688, 867, 2516, 6181, 7756, 4116, 150, 9056, 4513, 4779, 5133, 5637, 1126, 1254, 9101, 6150, 8812, 77, 5403, 7309, 5818, 1423, 1929, 1835, 1938, 8027, 7484, 7922, 2282, 1084, 5671, 3129, 1184, 4494, 236, 4194, 3228, 9387, 4898, 8449, 7071, 4604, 3694, 2560, 8093, 7473, 7595, 9548, 8981, 8747, 4870, 5080, 4934, 7846, 5257, 3763, 5865, 9196, 5740, 3976, 8806, 1289, 7999, 1203, 891, 1262, 7903, 3951, 2908, 7039, 329, 4556, 6032, 111, 7790, 8826, 1633, 9282, 5595, 1110, 5129, 915, 321, 5350, 2044, 1053, 1799, 9117, 8766, 943, 8016, 6109, 8565, 587, 3222, 3938, 7383, 3601, 6519, 2204, 29, 1297, 8993, 3791, 2817, 8229, 3859, 4835, 5621, 5512, 4267, 7592, 325, 6051, 5238, 5314, 5103, 8497, 9204, 3540, 8398, 1840, 5608, 2037, 5779, 7485, 405, 2121, 7052, 6388, 605, 5227, 1455, 8306, 5304, 5795, 2167, 5543, 1742, 3340, 4514, 408, 5225, 5778, 1290, 6203, 6098, 7221, 8498, 4039, 1421, 5322, 4463, 7262, 4539, 6727, 2323, 4905, 4837, 5472, 7690, 1923, 631, 6872, 899, 6256, 5538, 2220, 6451, 3742, 3926, 5102, 6075, 5556, 1970, 8815, 9233, 5904, 4744, 6387, 7800, 7705, 5345, 7661, 7617, 5003, 5266, 2822, 2879, 8733, 1006, 2000, 8602, 4425, 7417, 8227, 4508, 7257, 6178, 3325, 8036, 2937, 6586, 1870, 6172, 8187, 7865, 817, 1866, 2382, 7697, 1625, 5675, 3944, 4335, 7445, 646, 3697, 4719, 7899, 3596, 9676, 2720, 3809, 2238, 4939, 3208, 270, 3724, 3594, 4124, 1214, 9479, 1271, 566, 1859, 9058, 6682, 3309, 4130, 676, 7571, 4181, 9728, 5077, 2722, 1460, 6093, 9457, 6462, 2990, 4251, 6149, 2176, 2696, 2965, 9670, 8014, 5567, 1611, 6389, 3421, 1655, 1520, 2174, 8362, 5192, 4371, 3243, 2568, 3943, 9391, 5630, 2182, 8117, 7837, 8314, 5661, 1258, 7887, 3338, 9227, 5258, 6780, 3029, 5837, 5248, 9363, 8079, 5718, 4859, 143, 148, 8247, 6712, 8194, 1910, 7114, 8134, 7020, 6201, 1451, 5500, 7873, 4099, 533, 1563, 9184, 3193, 5169, 4086, 7400, 679, 6158, 8796, 2354, 9522, 6511, 5405, 5966, 2440, 622, 6431, 6689, 7310, 6366, 7824, 2789, 4146, 7977, 1914, 2309, 3493, 1906, 8634, 4319, 8440, 6892, 162, 6372, 3085, 5901, 6862, 7759, 4455, 2748, 2346, 8583, 2920, 4740, 3039, 5825, 1362, 5878, 6195, 1737, 1420, 934, 9234, 6170, 4753, 1162, 3634, 4396, 3654, 4785, 6054, 9005, 7397, 3526, 2500, 3911, 7470, 9019, 385, 3438, 7923, 6525, 6555, 6763, 8837, 4348, 3210, 1741, 9636, 5158, 1553, 3273, 5406, 7447, 7799, 835, 1105, 2446, 6691, 7670, 7013, 5641, 3378, 8619, 5797, 1653, 228, 8553, 7742, 2903, 2315, 4592, 8179, 6603, 961, 4918, 8538, 9285, 8499, 6582, 5623, 7584, 39, 1076, 2770, 3536, 2326, 7601, 4577, 4334, 7974, 560, 8422, 2723, 3435, 4092, 7551, 1831, 7354, 718, 1182, 2739, 8349, 1048, 3524, 1275, 4452, 1292, 4647, 3385, 1528, 2690, 8408, 888, 7762, 9102, 19, 9521, 194, 7979, 5276, 4667, 41, 4790, 7205, 7148, 6861, 6821, 8580, 62, 6563, 5297, 6441, 8360, 3566, 813, 8692, 3793, 3022, 254, 8147, 9584, 4182, 2393, 3733, 9274, 920, 1448, 80, 3119, 2142, 7982, 8546, 9163, 8707, 8386, 8579, 8838, 9496, 8889, 1944, 2379, 1787, 5607, 3145, 1659, 591, 4892, 5373, 2217, 9251, 8459, 1018, 8612, 6350, 450, 2870, 53, 8182, 8184, 1639, 4616, 6014, 6013, 8329, 4991, 6639, 5291, 8552, 1720, 9560, 1143, 9132, 5452, 125, 5988, 9152, 1539, 2969, 3323, 8989, 1021, 7517, 5333, 4413, 5801, 7983, 5344, 8008, 6174, 1884, 2451, 2514, 1359, 6374, 662, 4979, 3901, 7357, 2403, 511, 35, 6823, 7161, 9097, 7214, 3706, 1370, 7855, 4169, 7771, 4163, 8973, 8031, 5234, 5450, 7156, 4512, 5823, 9588, 557, 8525, 44, 4338, 2491, 6288, 7384, 27, 67, 4766, 1680, 1705, 497, 8978, 6040, 7394, 921, 4582, 6859, 7693, 9684, 6803, 6917, 8664, 3136, 7659, 3226, 4778, 7408, 4047, 1322, 2983, 5285, 1141, 9055, 8984, 7709, 9159, 3919, 2428, 4735, 7665, 1012, 6905, 8936, 474, 1867, 7255, 9077, 2066, 8822, 8695, 1483, 6503, 493, 8959, 9179, 6589, 6816, 8257, 1640, 1848, 5697, 6928, 8381, 4023, 2295, 355, 5963, 5907, 2035, 3250, 2399, 1734, 392, 5049, 7210, 8315, 3045, 104, 1007, 9544, 264, 8708, 4836, 6202, 6335, 3826, 3637, 4218, 5897, 2170, 5000, 7418, 6815, 5254, 8793, 1379, 2305, 1971, 3539, 3920, 6364, 244, 5263, 4433, 1512, 6255, 5685, 3217, 4687, 6370, 8107, 1080, 863, 9268, 7003, 9563, 716, 7307, 8202, 903, 809, 1173, 8235, 6483, 5927, 4207, 4865, 3009, 3764, 6638, 5467, 847, 2599, 4110, 5251, 8764, 8539, 214, 472, 8272, 4519, 6587, 6222, 2665, 3732, 1838, 8723, 8330, 1047, 2891, 7224, 370, 3870, 8374, 3007, 5971, 4298, 9595, 9238, 3730, 7200, 8233, 5693, 5464, 3035, 6280, 5495, 3042, 858, 4373, 9331, 6250, 1311, 3146, 1197, 4179, 6617, 1100, 160, 5303, 9649, 8062, 3349, 853, 4133, 3988, 7159, 9230, 7728, 1886, 3177, 4675, 3731, 3268, 9540, 5774, 4287, 5444, 480, 8288, 356, 7465, 872, 4593, 530, 4791, 4171, 5502, 594, 830, 6052, 3760, 9216, 1926, 6902, 5763, 6813, 8491, 1752, 7024, 7934, 476, 2584, 7965, 2439, 1493, 4001, 607, 1732, 936, 2878, 9332, 6026, 4599, 7347, 5834, 3115, 2059, 2097, 3900, 6287, 445, 2226, 938, 8956, 268, 672, 8295, 4296, 9306, 2880, 5408, 1760, 6405, 4864, 134, 3684, 7623, 7267, 1179, 7753, 2093, 1759, 9614, 199, 2387, 3755, 4916, 3828, 2738, 1877, 8581, 7574, 3822, 8025, 2153, 318, 5684, 639, 6767, 6581, 1676, 1020, 9175, 6886, 7128, 6448, 9072, 4757, 4949, 3695, 6112, 6300, 7593, 2230, 1548, 9067, 3500, 596, 4336, 1336, 6274, 2130, 774, 7944, 7167, 4598, 2947, 6023, 5458, 9339, 3651, 7740, 7836, 2563, 8745, 7555, 8527, 8407, 5542, 4871, 2793, 2388, 113, 5857, 2251, 9290, 1940, 9677, 9090, 5784, 4357, 7573, 9529, 8176, 1700, 7460, 1314, 2168, 9123, 8105, 4187, 1226, 8120, 8710, 2125, 7199, 8633, 6601, 5353, 2414, 4240, 1669, 7006, 2149, 2338, 5885, 1260, 1664, 2269, 196, 3993, 8230, 801, 128, 556, 6768, 9183, 2745, 2465, 8139, 8298, 3591, 7801, 4197, 6663, 2271, 758, 3707, 8890, 8075, 7505, 6888, 3280, 5268, 5769, 9546, 3189, 86, 1707, 5833, 8983, 4183, 238, 7736, 3736, 4409, 5680, 3334, 4429, 7892, 8056, 332, 7042, 8157, 433, 7869, 2498, 6377, 8642, 6061, 2747, 1804, 2435, 4403, 4810, 6393, 7061, 2143, 6931, 9025, 5719, 2758, 5673, 1806, 2797, 4065, 2824, 9576, 2858, 2682, 3054, 6497, 5019, 9706, 7845, 4795, 2676, 151, 163, 3876, 9491, 649, 205, 4364, 6692, 4309, 5969, 43, 3872, 4479, 2701, 3359, 5745, 1893, 4689, 4421, 2018, 4144, 6138, 8429, 9154, 5933, 9656, 6518, 4930, 2046, 4410, 766, 2801, 6240, 1889, 6213, 1056, 1011, 197, 8211, 8748, 5847, 8628, 1868, 6990, 4057, 8346, 9686, 8479, 2362, 3478, 503, 5070, 1169, 5921, 2678, 1803, 8712, 8485, 6909, 6312, 8393, 4468, 8534, 6527, 3638, 79, 8164, 3317, 6453, 6734, 5794, 6486, 4213, 794, 8343, 1117, 9367, 9428, 2156, 9698, 6698, 965, 1568, 7854, 8919, 7743, 7936, 3003, 7735, 7691, 5346, 8952, 178, 7995, 1038, 6487, 5889, 6179, 4623, 72, 7273, 9065, 5105, 1219, 4572, 8803, 8818, 7231, 340, 5669, 4076, 2423, 7364, 6607, 5725, 3739, 1551, 8363, 3686, 2528, 13, 7294, 8029, 5301, 1330, 3240, 3523, 4544, 6101, 6580, 2784, 4257, 3521, 8351, 2653, 6788, 519, 4657, 1888, 2457, 5151, 7703, 7018, 8551, 3289, 4789, 8876, 5416, 4054, 7419, 2117, 6944, 6496, 8290, 733, 5315, 9536, 690, 8054, 7651, 9420, 932, 3663, 4020, 7711, 1994, 2546, 4321, 564, 998, 6808, 235, 6200, 5170, 5781, 1538, 8091, 2988, 9278, 9252, 9266, 5687, 4672, 5358, 8547, 627, 5869, 2412, 7059, 1196, 1991, 1644, 6654, 5052, 1543, 9155, 3153, 8039, 5744, 278, 443, 319, 3702, 3246, 5950, 580, 5754, 6392, 4078, 2559, 7142, 9323, 7111, 669, 317, 3112, 1353, 5551, 9063, 8737, 1339, 7655, 7913, 7405, 2336, 7318, 3765, 3423, 1013, 9564, 5692, 5067, 8953, 7827, 390, 5109, 6491, 4026, 667, 9426, 8884, 8946, 901, 6092, 4041, 1969, 1001, 508, 6276, 8589, 2627, 8994, 5676, 2839, 5951, 8068, 2413, 2067, 9427, 5753, 3885, 8340, 8411, 2040, 2486, 6319, 5934, 9199, 1885, 6528, 6468, 9666, 3766, 1748, 8752, 5021, 3605, 407, 6556, 966, 1168, 8469, 2986, 7870, 1441, 1127, 1183, 1452, 3402, 8342, 7401, 5347, 1505, 6378, 1321, 3849, 2128, 7959, 2579, 3156, 1531, 5570, 5660, 9167, 7626, 3853, 7385, 647, 2644, 5764, 8851, 8972, 1573, 7094, 8839, 3320, 8616, 6963, 7259, 4228, 2141, 6515, 5656, 818, 7525, 8970, 2767, 8210, 2292, 2725, 2688, 6521, 6064, 8488, 7009, 4426, 2372, 6035, 2332, 3485, 3196, 1778, 7908, 3725, 6831, 1381, 2868, 1348, 4032, 4998, 3811, 1256, 2948, 886, 4628, 5239, 955, 1354, 1618, 960, 261, 3953, 9419, 7353, 3105, 980, 4726, 9140, 1974, 3266, 6341, 5320, 7689, 1598, 7182, 2783, 1807, 7263, 750, 5958, 4784, 1342, 5817, 110, 4710, 4907, 6594, 361, 5736, 3642, 398, 2482, 9627, 1191, 1004, 1678, 6130, 3337, 1447, 5469, 4121, 5043, 1150, 4752, 869, 4388, 3671, 4736, 8648, 6539, 7991, 1253, 5336, 2612, 1144, 20, 4754, 3426, 2349, 487, 7567, 6011, 3272, 6916, 5120, 5360, 4931, 3563, 6157, 9435, 2160, 1845, 9630, 5454, 9352, 7358, 4323, 3974, 9054, 1492, 747, 1393, 5707, 5854, 2383, 7160, 617, 6090, 5190, 3942, 2273, 609, 1933, 2643, 8606, 2471, 9026, 8240, 2011, 9006, 5465, 1307, 333, 4046, 762, 6294, 3982, 535, 1189, 1003, 1558, 8702, 4140, 6739, 4709, 4188, 4809, 32, 1437, 3403, 2752, 2101, 3564, 8531, 1587, 1712, 7664, 7206, 1035, 7611, 3278, 8254, 6913, 9298, 5694, 7180, 492, 1155, 8251, 9262, 958, 8467, 727, 5138, 3377, 9447, 4842, 509, 7932, 3172, 6190, 190, 8017, 5728, 2887, 8974, 1925, 368, 22, 8979, 1276, 416, 5307, 8960, 1645, 4332, 875, 4406, 4999, 7726, 1708, 6738, 5893, 6499, 6770, 4954, 9134, 2364, 1909, 7098, 3122, 501, 7287, 1407, 9359, 553, 896, 5978, 2890, 7494, 883, 1058, 3254, 6421, 620, 352, 6643, 9014, 3296, 5232, 4862, 516, 247, 2193, 7115, 8322, 3692, 6284, 9364, 6223, 6307, 5267, 3084, 1575, 4803, 8076, 8829, 7361, 2659, 7290, 723, 7453, 6565, 2765, 2642, 172, 2565, 3109, 5549, 6912, 5493, 7898, 5989, 9007, 2361, 3088, 9322, 9113, 5026, 7154, 3087, 3380, 351, 2407, 3835, 5212, 1907, 5541, 7680, 5849, 8820, 8347, 6620, 6624, 2845, 5555, 7997, 935, 3388, 5999, 6373, 9501, 4902, 988, 5496, 7815, 2735, 2594, 7553, 233, 7930, 701, 6231, 2, 5094, 6041, 3572, 5785, 9073, 4337, 3249, 2263, 8253, 8124, 135, 6895, 9732, 5962, 3166, 3339, 2146, 8549, 7763, 8649, 6135, 6629, 5663, 3669, 2452, 9166, 2503, 8463, 1054, 1216, 5240, 3930, 2099, 1589, 5198, 7238, 5568, 6609, 1133, 2210, 3917, 8285, 6267, 7187, 5107, 3761, 4011, 8800, 5093, 7917, 7501, 5658, 2008, 8434, 814, 3527, 4100, 3678, 6132, 1613, 292, 8166, 8195, 7663, 8222, 3789, 4355, 5688, 5478, 4405, 7809, 8044, 3502, 5061, 7879, 8477, 3625, 1388, 4017, 8511, 897, 226, 905, 3329, 6669, 1858, 6261, 6893, 3292, 9488, 8067, 713, 3618, 8542, 4341, 856, 834, 1392, 7533, 4191, 9587, 1956, 6074, 1765, 3924, 6467, 4965, 9578, 8961, 5116, 4896, 7985, 5705, 8137, 6434, 3237, 4374, 8971, 4952, 6232, 6904, 4147, 9235, 2743, 9048, 8938, 7078, 2914, 3251, 3863, 1554, 8861, 9675, 1094, 217, 7946, 6428, 7536, 4150, 5290, 1722, 2043, 3019, 1860, 5394, 8656, 7610, 8102, 7609, 136, 7233, 8615, 3635, 33, 3396, 3945, 710, 8131, 9242, 7141, 386, 2239, 7250, 8160, 2951, 7152, 2359, 5703, 1841, 5965, 4521, 5076, 8437, 6413, 6191, 859, 1727, 868, 7026, 8988, 2721, 8489, 4851, 8678, 4874, 2259, 8337, 6849, 4229, 6566, 4427, 1503, 3452, 9319, 5325, 381, 545, 2610, 1106, 4986, 6576, 6915, 9310, 2827, 7989, 929, 3537, 7038, 7468, 3395, 1743, 8722, 1418, 140, 4037, 9177, 4024, 9107, 3530, 7838, 8375, 9267, 1153, 5861, 4797, 1090, 517, 4159, 6873, 4474, 6881, 4328, 4933, 5638, 3285, 8430, 3354, 460, 5218, 8220, 6375, 4192, 2308, 7158, 1036, 100, 7526, 9480, 4506, 6458, 5058, 5645, 3963, 118, 4094, 2933, 1120, 4038, 628, 6355, 4959, 6619, 8785, 3985, 5371, 6368, 4397, 1985, 9192, 6127, 3351, 8289, 7699, 2171, 2496, 3836, 7751, 3972, 9062, 8760, 3211, 4792, 3580, 8307, 8073, 4698, 1488, 8169, 6329, 2844, 3620, 7744, 5490, 2524, 6131, 2350, 207, 7411, 2161, 9036, 1028, 8435, 5415, 3598, 3081, 684, 2730, 4167, 5270, 9625, 6493, 455, 5096, 1361, 3279, 5954, 7458, 962, 9687, 3264, 9255, 3037, 4702, 5455, 8909, 4747, 8189, 8597, 6812, 7775, 7456, 4580, 6551, 7162, 1873, 3603, 5905, 2583, 8887, 5835, 2660, 5742, 2206, 9371, 5261, 1235, 2619, 3096, 1252, 7741, 7748, 1532, 4232, 518, 5757, 1851, 6664, 6152, 1425, 1614, 9589, 9185, 4248, 8185, 9434, 5446, 2036, 1972, 6206, 4230, 6598, 7620, 8097, 5335, 887, 376, 7317, 9711, 8667, 9078, 5316, 3947, 4734, 9697, 2261, 3649, 3889, 9453, 614, 6239, 9551, 82, 435, 243, 4186, 5655, 2572, 9311, 471, 122, 9263, 5790, 8611, 3611, 6559, 7572, 7696, 7543, 668, 9030, 38, 2218, 3504, 8256, 7624, 6824, 8410, 3875, 680, 8590, 5722, 3314, 8802, 4308, 7140, 5520, 5771, 7534, 778, 3190, 9360, 2896, 572, 206, 9382, 7462, 9541, 6275, 3713, 5839, 8724, 3639, 7683, 3830, 7251, 4446, 5576, 7746, 2047, 9440, 8350, 3696, 3304, 8577, 1170, 1249, 2320, 527, 7279, 7686, 4728, 6212, 2798, 8744, 9690, 8864, 7874, 1461, 4814, 7247, 6455, 2595, 9462, 5850, 2158, 3491, 6285, 8731, 7201, 5727, 4828, 9461, 8495, 5787, 3744, 5519, 6396, 5015, 6433, 8085, 9121, 6016, 4424, 2029, 3447, 1062, 5453, 1251, 5161, 9245, 9641, 5130, 7539, 4566, 7336, 2411, 7138, 1917, 922, 4805, 7841, 2849, 1467, 3406, 298, 449, 1187, 7314, 704, 9532, 1827, 8942, 396, 2020, 2321, 6615, 9629, 7281, 4956, 810, 9731, 227, 2917, 2039, 3747, 1159, 229, 571, 3061, 923, 5038, 1212, 7857, 5328, 8304, 954, 3358, 783, 1231, 412, 864, 2940, 2547, 4408, 7818, 5489, 5615, 4772, 6279, 9556, 3223, 8392, 1061, 286, 91, 4601, 5385, 1620, 5902, 4407, 2925, 8840, 593, 9181, 367, 7905, 6754, 9669, 4776, 4617, 6953, 8092, 3416, 6361, 5436, 8544, 6207, 5558, 154, 8376, 89, 4685, 771, 3927, 5920, 6979, 4712, 9590, 1874, 8354, 4127, 6048, 5361, 5434, 5211, 1217, 665, 7064, 8452, 5037, 565, 1227, 8743, 4109, 989, 3070, 4075, 1400, 382, 1309, 9550, 6233, 7016, 6297, 8749, 8969, 5890, 8110, 606, 7060, 5156, 3198, 8175, 369, 2070, 8927, 3180, 5277, 6409, 7810, 5642, 3437, 8475, 3445, 9444, 5750, 8345, 8685, 8863, 8896, 9105, 849, 1132, 4491, 8055, 844, 7125, 1600, 2679, 4743, 6304, 619, 2805, 9549, 8807, 8046, 4066, 3903, 5864, 3463, 1135, 9228, 7907, 3399, 9609, 7413, 164, 6044, 9626, 9478, 5145, 5491, 6404, 1577, 4626, 6685, 287, 6671, 1731, 4847, 1580, 6704, 8687, 7636, 4184, 6401, 310, 3160, 5532, 5830, 1500, 1097, 5002, 681, 8834, 2026, 9335, 3861, 3778, 9410, 652, 8593, 6465, 1282, 2262, 5935, 6349, 8226, 6197, 5896, 5425, 7095, 5796, 2663, 2003, 1024, 4458, 3199, 2461, 1650, 7284, 99, 2266, 6764, 6359, 6869, 421, 1510, 9726, 7219, 706, 6437, 1820, 7953, 4190, 1570, 660, 7542, 8916, 181, 4804, 5439, 9531, 8049, 1474, 2968, 7333, 6385, 9116, 6116, 2007, 9608, 1522, 1365, 7507, 3748, 350, 2115, 6657, 7871, 1040, 3785, 4224, 1077, 8170, 1287, 4973, 4156, 3400, 9520, 6994, 4226, 314, 149, 8827, 5031, 5168, 6843, 735, 5180, 8311, 6498, 7585, 2252, 8671, 360, 3495, 2475, 2853, 5376, 6923, 2427, 686, 3046, 8569, 752, 7803, 6333, 9351, 1911, 1710, 8238, 2497, 2549, 6363, 2445, 8735, 5305, 2624, 4654, 2257, 2092, 469, 9135, 4663, 5807, 6978, 2154, 6794, 88, 2052, 2228, 3624, 5309, 9189, 6050, 399, 6758, 529, 2623, 2702, 5980, 2109, 1147, 8595, 3175, 5777, 8443, 7328, 1247, 7011, 1000, 1427, 2848, 5977, 700, 3795, 6059, 5940, 5634, 8158, 2159, 4996, 537, 6107, 4008, 6114, 1499, 9516, 437, 769, 8418, 5177, 6479, 6796, 7472, 4988, 9013, 4356, 7785, 5773, 3133, 5960, 8571, 6241, 7046, 5783, 568, 5204, 4277, 8301, 5142, 7034, 107, 2114, 2764, 6857, 4945, 6590, 2448, 9519, 3646, 7851, 2742, 5174, 9379, 711, 9211, 5042, 7599, 2548, 2476, 4090, 2178, 3167, 7994, 6215, 9574, 4640, 274, 1300, 363, 1113, 2352, 6323, 4571, 780, 6883, 4533, 7896, 4302, 9570, 5359, 2763, 9317, 1607, 4656, 3950, 3721, 8697, 6180, 5241, 3543, 8332, 3492, 4624, 4450, 7952, 6545, 2960, 5173, 3862, 5711, 1936, 4819, 2027, 2522, 6381, 5552, 4558, 9141, 3622, 9559, 2695, 7313, 8420, 6985, 5592, 6564, 7398, 3807, 9695, 2236, 1894, 266, 5497, 2527, 124, 8279, 4214, 6399, 1767, 2648, 4379, 5506, 729, 754, 9330, 6356, 6081, 6164, 9358, 7569, 2501, 3142, 981, 9362, 2155, 5741, 865, 8809, 4925, 6177, 9031, 3192, 7424, 5473, 8064, 8119, 6542, 7050, 914, 4825, 772, 3281, 6877, 2761, 8397, 5369, 3425, 1574, 3214, 8394, 2386, 8631, 8975, 8613, 5384, 8771, 8940, 1204, 4897, 8024, 6156, 6417, 5908, 9605, 2881, 3484, 3726, 3413, 4813, 2799, 78, 7171, 6858, 7041, 3894, 6975, 5620, 4058, 5569, 2010, 3118, 6546, 6348, 1175, 6163, 6175, 2993, 9448, 2098, 502, 1291, 4881, 1814, 1978, 6703, 3712, 5085, 8099, 6572, 9481, 9299, 3407, 2484, 8662, 790, 6868, 4212, 1345, 5515, 3261, 7933, 1898, 9138, 3783, 5587, 7704, 5147, 7105, 2772, 8536, 5189, 1257, 963, 4035, 5014, 5747, 1303, 4920, 1749, 4051, 6484, 9124, 7804, 1703, 9421, 3040, 2973, 9464, 5842, 3498, 9610, 6147, 4077, 3204, 8874, 8814, 6415, 1591, 2921, 1124, 1561, 3980, 2713, 2700, 1255, 58, 1935, 5154, 9597, 7732, 7981, 6583, 4219, 15, 3138, 561, 8114, 9600, 5746, 7919, 7329, 3606, 9143, 9091, 3130, 1250, 5503, 4926, 7814, 2633, 1083, 3983, 6541, 8047, 9215, 6154, 1556, 6419, 970, 4004, 3149, 3682, 2897, 9396, 2876, 5545, 3404, 9517, 9240, 4644, 2264, 6991, 6835, 6282, 2581, 157, 1042, 4575, 3517, 4660, 6523, 4394, 4010, 6853, 8423, 1784, 6221, 1409, 2353, 2521, 1785, 4131, 4367, 8504, 8009, 2656, 7025, 2247, 3771, 1389, 9249, 1138, 6842, 2074, 8248, 1098, 4166, 7269, 6908, 3330, 1399, 8264, 4151, 1813, 534, 7375, 7594, 2615, 5006, 1295, 515, 8779, 7914, 2024, 7660, 3784, 9341, 5513, 4200, 158, 155, 9224, 4113, 2013, 2001, 9658, 3442, 9591, 7537, 4968, 7461, 8847, 1264, 3154, 5284, 1398, 5898, 9511, 4299, 876, 3298, 7184, 3336, 2543, 615, 5404, 3929, 4333, 6266, 7422, 4540, 793, 2420, 5597, 5571, 5079, 8720, 177, 721, 893, 2477, 9473, 6744, 5471, 3071, 9024, 1696, 8433, 4128, 7483, 2736, 862, 5367, 6711, 9592, 2726, 7073, 8926, 161, 7022, 7521, 3949, 4701, 5205, 6968, 7588, 3568, 429, 6117, 8680, 2334, 5311, 857, 5414, 7277, 5723, 5868, 2116, 1967, 7093, 4250, 225, 9318, 4507, 8150, 5137, 579, 7643, 7780, 3628, 3147, 4873, 3852, 8050, 6227, 6173, 2556, 1821, 917, 316, 7700, 1312, 137, 3469, 1301, 8713, 4469, 4135, 9069, 330, 6436, 1002, 5484, 2511, 2954, 10, 8126, 946, 3494, 2777, 1331, 7993, 8575, 9342, 1481, 8877, 7825, 2094, 4770, 1730, 5900, 2590, 9542, 4724, 8037, 9222, 5987, 5340, 9061, 6142, 6962, 168, 9119, 1134, 2405, 5699, 2979, 1855, 8670, 2926, 5626, 8108, 7421, 9070, 3067, 306, 1823, 3453, 7925, 5860, 231, 8655, 1697, 8277, 8862, 7188, 2307, 315, 5432, 9508, 4614, 996, 3424, 6885, 1590, 3668, 2367, 2166, 180, 4970, 6249, 444, 1941, 4402, 7840, 8898, 6735, 6832, 7345, 3551, 6894, 4466, 477, 8421, 6710, 1111, 6427, 6850, 1687, 3092, 7777, 6094, 3121, 2389, 5088, 8910, 6997, 6169, 7363, 3661, 2987, 7131, 6814, 8468, 8915, 1156, 4554, 4574, 2444, 3183, 8244, 6611, 2978, 1564, 722, 4059, 2792, 3258, 2814, 5540, 3864, 2376, 242, 1957, 3923, 5533, 383, 1384, 3960, 1576, 2693, 7226, 2466, 7938, 8543, 4084, 3584, 1729, 96, 3698, 465, 423, 5022, 2834, 8901, 8913, 8924, 4236, 3966, 3476, 4448, 9001, 9515, 4019, 9186, 2014, 9103, 9023, 4266, 7524, 1850, 9655, 3652, 7213, 7299, 8355, 3660, 248, 2260, 7090, 8996, 4961, 9685, 6087, 1692, 1244, 1215, 9736, 4612, 8140, 7547, 559, 485, 5582, 4739, 4562, 624, 7971, 9510, 186, 4763, 3270, 7202, 5598, 7621, 347, 2907, 7482, 1085, 2366, 9598, 342, 4467, 7276, 6507, 21, 3560, 4349, 9304, 8183, 8869, 3633, 5564, 8161, 3048, 3117, 495, 2209, 55, 2621, 1987, 2335, 9361, 3511, 803, 3429, 3710, 6505, 7123, 5523, 6265, 9042, 7895, 3959, 826, 7049, 5579, 9527, 8142, 1308, 7826, 3139, 3008, 7616, 1401, 6420, 9269, 6306, 702, 187, 3287, 5027, 1908, 8239, 8636, 3902, 1631, 7486, 8308, 8358, 4872, 4423, 3477, 7434, 8011, 7489, 6561, 5269, 5233, 4721, 3236, 2941, 7229, 49, 8653, 8912, 8015, 6471, 505, 1198, 1892, 2365, 4596, 4293, 4695, 8250, 8660, 7450, 2401, 4366, 7393, 6522, 2300, 7356, 6264, 626, 3417, 993, 4437, 3350, 6825, 1770, 7619, 2231, 7761, 4314, 1986, 8103, 5689, 6305, 8096, 1745, 9288, 8066, 5155, 8679, 4243, 6852, 671, 1660, 3137, 8361, 359, 6289, 3010, 3743, 2712, 8136, 1320, 4386, 7157, 6936, 3102, 5652, 4161, 5185, 1545, 4885, 9705, 8069, 1671, 9458, 1288, 1081, 703, 6118, 6610, 6752, 4845, 5881, 3714, 6989, 8209, 6298, 8514, 2646, 6971, 92, 8023, 2776, 5319, 6721, 8856, 5089, 6670, 7731, 6751, 4899, 4967, 1635, 5391, 4555, 8206, 7779, 3245, 919, 2882, 1031, 6501, 4107, 3055, 173, 4807, 7842, 574, 773, 5505, 8714, 871, 2096, 542, 8832, 8366, 9673, 5128, 9202, 9011, 5911, 8223, 8545, 1457, 4235, 6896, 6560, 3893, 8682, 4245, 3886, 4233, 510, 2139, 4496, 3913, 5160, 7435, 6286, 7765, 8303, 3434, 595, 4831, 9691, 2578, 2911, 845, 2952, 309, 730, 8296, 5700, 2709, 311, 9294, 2750, 6725, 5735, 2186, 6977, 5247, 384, 1371, 6346, 5264, 2938, 4297, 2842, 4578, 7606, 3825, 8132, 2487, 1501, 5220, 3648, 4118, 6887, 5429, 1943, 6077, 539, 2082, 8610, 2316, 3602, 7789, 9467, 5596, 3106, 1900, 1063, 1719, 5967, 3814, 8805, 2804, 9568, 3812, 7220, 98, 1498, 473, 8447, 2532, 6543, 1152, 7062, 2038, 9229, 2674, 2085, 9557, 1819, 6028, 3578, 5640, 9349, 8556, 964, 4670, 8841, 1585, 2474, 8646, 3922, 6708, 2507, 6096, 2821, 7984, 6748, 775, 2923, 5428, 2756, 590, 4552, 7143, 8789, 8995, 249, 1087, 1691, 3260, 3041, 2375, 7106, 8312, 4730, 3912, 7717, 7146, 8965, 6512, 4661, 3623, 5853, 5836, 377, 1789, 6840, 698, 2299, 8658, 5295, 224, 489, 8630, 2470, 2934, 8359, 3489, 8123, 258, 2327, 1072, 8431, 1546, 7339, 6829, 1683, 6299, 8035, 2781, 7511, 5280, 768, 1372, 6844, 7331, 3274, 6570, 6004, 3553, 3986, 9207, 7208, 2134, 5071, 3412, 8204, 3996, 5710, 4691, 466, 2369, 7829, 8848, 5805, 1966, 5894, 8566, 2152, 9489, 1673, 7395, 8945, 2803, 4531, 8384, 4264, 6068, 7835, 9703, 6661, 2112, 1699, 2342, 6693, 8060, 2492, 5260, 8599, 6986, 6476, 6036, 7796, 6489, 2144, 8021, 5435, 8572, 9153, 8493, 8500, 7322, 3100, 1200, 2515, 9720, 4530, 2016, 1118, 6009, 1224, 1157, 2505, 165, 8255, 2306, 6129, 3587, 3148, 4801, 1863, 9392, 1795, 1686, 334, 428, 8053, 6961, 5046, 8777, 54, 942, 3754, 2495, 3176, 7499, 9170, 2381, 9523, 2021, 3006, 657, 6444, 343, 3690, 7972, 2331, 4395, 7942, 6454, 3483, 8470, 6088, 6818, 8321, 6787, 4884, 984, 2863, 8813, 4387, 7676, 6167, 7369, 7632, 4922, 5400, 6062, 7055, 8990, 7492, 2377, 866, 4265, 7587, 4860, 1578, 2127, 5249, 5629, 8388, 4105, 3531, 6958, 5537, 2249, 6136, 7258, 2180, 484, 7211, 2816, 5265, 9187, 4525, 1471, 4960, 7604, 9583, 7467, 8267, 8003, 6680, 5045, 2593, 7839, 6538, 3387, 2442, 8681, 2555, 2517, 5134, 842, 171, 3715, 8457, 8129, 6918, 6045, 3855, 8341, 1781, 2124, 3441, 2698, 7117, 3507, 8701, 8473, 678, 6466, 4940, 8155, 5739, 1477, 7019, 4651, 8509, 9380, 2240, 9108, 4615, 2006, 708, 95, 693, 7368, 5060, 4758, 4570, 2596, 322, 1808, 4711, 3215, 3025, 7070, 1478, 7722, 956, 1597, 8739, 6509, 5591, 4442, 7734, 6024, 2123, 9633, 2071, 3933, 3869, 9601, 7043, 5584, 4706, 2061, 8557, 2201, 1801, 7608, 8783, 799, 573, 3294, 4832, 599, 5501, 4177, 8759, 2409, 9707, 2330, 7885, 3866, 30, 767, 3749, 4339, 3969, 3451, 7733, 5055, 5880, 1774, 578, 4664, 7481, 1071, 5208, 694, 4497, 8390, 3075, 3456, 6774, 6268, 3418, 6155, 3171, 1879, 2030, 2103, 2324, 7963, 5488, 7374, 8765, 276, 512, 691, 411, 1735, 6706, 1718, 8761, 7951, 7155, 3672, 598, 6946, 257, 6529, 1108, 3321, 1593, 6757, 6696, 5223, 5377, 1161, 324, 4139, 273, 5734, 239, 1417, 959, 9490, 912, 4696, 4648, 2417, 312, 7843, 5995, 1996, 34, 255, 4344, 7770, 9683, 1358, 365, 6838, 2802, 9613, 8177, 4655, 8249, 7500, 9122, 6229, 8403, 1656, 230, 5516, 6637, 9276, 815, 5302, 2089, 3541, 6311, 8333, 3693, 7755, 251, 1665, 2618, 8453, 7144, 216, 6320, 3128, 3056, 9699, 7774, 4993, 1074, 7127, 3202, 8541, 1945, 7918, 2075, 2788, 3964, 7967, 1055, 3858, 6707, 4543, 1230, 3516, 2754, 2281, 3860, 4284, 415, 550, 9198, 4471, 3050, 6658, 4259, 2658, 5381, 925, 9411, 4606, 1414, 8817, 4723, 5924, 785, 625, 1344, 1095, 8943, 6855, 5139, 5231, 3520, 8955, 5756, 4526, 7346, 3756, 4225, 3997, 18, 5840, 7864, 7541, 3120, 3069, 6647, 738, 1833, 1026, 4777, 2683, 2932, 467, 8286, 6578, 6391, 756, 4806, 9369, 7618, 4731, 3497, 6864, 3898, 4542, 8518, 7028, 9157, 8560, 7909, 1517, 2961, 8845, 6353, 7139, 5321, 4725, 7261, 4976, 2935, 2291, 3595, 8266, 4668, 7860, 3808, 4629, 8502, 3776, 358, 5948, 3479, 548, 7964, 784, 8324, 6592, 2628, 5183, 5004, 3880, 4550, 5110, 3608, 7172, 182, 4611, 4741, 8503, 1733, 281, 2019, 1654, 7508, 2553, 1378, 7931, 9528, 3848, 2426, 4330, 9217, 3023, 613, 5296, 7066, 742, 4738, 7147, 7902, 3299, 5872, 75, 3773, 8078, 8258, 5816, 5053, 8234, 5383, 643, 328, 3505, 1963, 4557, 4376, 792, 4534, 3383, 5832, 5380, 7425, 8389, 8382, 4824, 9312, 1605, 6687, 2906, 1017, 6039, 695, 6820, 2163, 2200, 1526, 8236, 8875, 5005, 5943, 1552, 6145, 5724, 1010, 9052, 6424, 2913, 153, 6933, 6980, 1459, 5486, 4684, 3060, 6771, 59, 8852, 1408, 8954, 1846, 3834, 5972, 5955, 8336, 2073, 9075, 8291, 8529, 1202, 6608, 7040, 3936, 7335, 6974, 2122, 3353, 9002, 6457, 2102, 740, 4994, 7116, 7388, 877, 1422, 8690, 295, 8168, 40, 8417, 2091, 3670, 2520, 3840, 4480, 2716, 1206, 7975, 3225, 371, 8935, 7725, 5066, 9180, 8931, 4938, 2395, 6828, 3444, 1663, 9302, 4022, 2542, 2946, 1584, 9395, 1623, 8436, 4704, 3798, 5928, 3259, 4475, 8751, 6226, 409, 5709, 5123, 6889, 4370, 3868, 6981, 3026, 5181, 5023, 3163, 475, 9665, 16, 6071, 9372, 7089, 8858, 7099, 4329, 8318, 8432, 644, 3644, 9449, 6185, 8564, 5087, 4722, 4646, 8395, 9378, 3018, 2192, 6799, 1261, 5926, 3224, 9718, 3699, 4732, 8156, 3877, 4439, 2535, 8481, 2065, 9735, 9273, 2222, 3333, 4680, 2464, 5200, 7197, 1899, 4523, 339, 6134, 7436, 2734, 3239, 9377, 5565, 4160, 1763, 3063, 1569, 1485, 481, 5879, 2666, 5191, 3357, 4782, 9125, 7520, 641, 6438, 6789, 531, 8224, 4787, 985, 7440, 1881, 5531, 1519, 8486, 6376, 1404, 4603, 3787, 5636, 8632, 6418, 7966, 6106, 6514, 709, 1473, 5851, 8574, 3324, 4154, 7916, 5945, 4398, 7504, 2654, 2794, 6140, 1324, 7723, 4282, 7455, 5099, 9118, 5112, 715, 6567, 8191, 5821, 1780, 8061, 3829, 5476, 2807, 7248, 4882, 1112, 654, 4665, 4569, 8716, 6220, 880, 952, 9632, 3468, 6263, 4746, 3104, 7784, 7583, 2939, 5201, 2977, 1351, 3838, 5574, 4252, 2169, 7479, 9538, 9081, 6108, 2867, 6301, 159, 4748, 1630, 1997, 3937, 133, 1413, 7045, 1864, 3590, 1853, 202, 2841, 7245, 8483, 8763, 8592, 193, 2655, 8252, 
Train Sentence: 499
Number of candidates => 1559
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 1559
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:40:43 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:40:43 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:40:43 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 2152
...+...+...+...+...500
            ...+...+...+...+...1000
            ...+...+...+...+...1500
            ..
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2152 distinct candidates (about 1 per sentence):
newCandidatesAdded[it=1] = 2152 (about 1 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:40:44 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.8056185401733513)

Final lambda[j=1]: {0.1, 1.3718524407732561E14, 2.7936244446123076E12}
(Final BLEU[j=1]: 0.9209453711204938)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:40:44 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.8690002782258329)

Final lambda[j=2]: {-0.017230818545013626, 6.991546350521361E13, 1.4237425873796E12}
(Final BLEU[j=2]: 0.9209453711204938)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:40:44 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7851254119906101)

Final lambda[j=3]: {-0.5203200954373215, 1.4177735988354112E15, 2.8871201989846266E13}
(Final BLEU[j=3]: 0.9209453711204938)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:40:44 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7751149616262536)

Final lambda[j=4]: {-0.6919952429797738, 1.8855558234639485E15, 3.839700003462395E13}
(Final BLEU[j=4]: 0.9209453711204938)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:40:44 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.8065391828238504)

Final lambda[j=5]: {0.7737957524668755, 1.061533591680953E15, 2.1616947900463047E13}
(Final BLEU[j=5]: 0.9209453711204938)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:40:44 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.9029640813832387)

Final lambda[j=6]: {2.3094998515375762E-14, 0.15636862697411513, 0.4506319453537006}
(Final BLEU[j=6]: 0.9201085835136863)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:40:44 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.8059216178037217)

Final lambda[j=7]: {0.32669861662639876, 4.481822946183623E14, 9.126733051474176E12}
(Final BLEU[j=7]: 0.9209453711204938)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:40:45 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.9020653178462187)

Final lambda[j=8]: {3.4583499866963055E-14, 0.08804197789590318, 0.635582370679554}
(Final BLEU[j=8]: 0.9197954616090633)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:40:45 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7899094208663184)

Final lambda[j=9]: {-0.004553671263154424, 1.8476895696542938E13, 3.762592830826845E11}
(Final BLEU[j=9]: 0.9209453711204938)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:40:45 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.7998511191498399)

Final lambda[j=10]: {0.5053771649119221, 6.9330289719682E14, 1.4118341133696004E13}
(Final BLEU[j=10]: 0.9209453711204938)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:40:45 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.7998511191498399)

Final lambda[j=11]: {0.7980226618452084, 1.0947693364438152E15, 2.2293757017517047E13}
(Final BLEU[j=11]: 0.9209453711204938)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:40:45 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8982029519176593)

Final lambda[j=12]: {0.594593012768021, 8.156938758293702E14, 1.661069664030156E13}
(Final BLEU[j=12]: 0.9209453711204938)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:40:45 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7865053975460816)

Final lambda[j=13]: {-0.21052430917292875, 8.54219700410006E14, 1.7395136255650197E13}
(Final BLEU[j=13]: 0.9209453711204938)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:40:45 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.9029640813832387)

Final lambda[j=14]: {1.278511442429795E-15, 0.00790321507749736, 0.02356355738980116}
(Final BLEU[j=14]: 0.9201085835136863)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:40:45 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.9020029796185671)

Final lambda[j=15]: {0.009954244141451962, 1.365575411962905E13, 2.780842230290176E11}
(Final BLEU[j=15]: 0.9209453711204938)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:40:45 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.8056185401733513)

Final lambda[j=16]: {0.6576106932759427, 9.021448346493095E14, 1.837117325530054E13}
(Final BLEU[j=16]: 0.9209453711204938)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:40:45 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7743750076519536)

Final lambda[j=17]: {-0.871756060640207, 2.3753699659861095E15, 4.83714596086779E13}
(Final BLEU[j=17]: 0.9209453711204938)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:40:45 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7760191154645046)

Final lambda[j=18]: {-0.4629095033959656, 1.2613406215154545E15, 2.5685637158522445E13}
(Final BLEU[j=18]: 0.9209453711204938)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:40:45 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7851254119906101)

Final lambda[j=19]: {-0.8834296944416113, 2.407178404581541E15, 4.901920164546256E13}
(Final BLEU[j=19]: 0.9209453711204938)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:40:45 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7747450412579556)

Final lambda[j=20]: {-0.2916952155637593, 7.948141522108925E14, 1.618540401835734E13}
(Final BLEU[j=20]: 0.9209453711204938)

Best final lambda is lambda[j=1] (BLEU: 0.9209).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:40:45 JST 2015  ---

Next iteration will decode with lambda: {0.1, 1.3718524407732561E14, 2.7936244446123076E12}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:40:45 JST 2015 ---
Redecoding using weight vector {0.1, 1.3718524407732561E14, 2.7936244446123076E12}
Running decoder...
...finished decoding @ Tue Oct 27 10:40:46 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+...+...+...+...500
            ...+...+...+...+...1000
            ...+...+...+...+...1500
            ..
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2152 distinct candidates (about 1 per sentence):
newCandidatesAdded[it=1] = 2152 (about 1 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:40:47 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:40:47 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, 1.3718524407732561E14, 2.7936244446123076E12} (BLEU: 0.9209453711204938)

(OP Lamda) : [0.1,1.3718524407732561E14,2.7936244446123076E12]
Number of candidates => 908
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 908
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:40:48 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:40:48 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:40:50 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 9061
...+...+...+...+...500
            ...+...+...+...+
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 9061 distinct candidates (about 9 per sentence):
newCandidatesAdded[it=1] = 9061 (about 9 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:40:52 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.7908757393312549)

Final lambda[j=1]: {0.1, 0.2, 8.316660289501833E10}
(Final BLEU[j=1]: 0.8559944011248749)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:40:52 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.761043322073903)

Final lambda[j=2]: {5.714200901356255E-13, -0.6577977590753961, 0.4752421657314109}
(Final BLEU[j=2]: 0.8558952416211987)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:40:52 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7296904500164655)

Final lambda[j=3]: {2183.9879998796396, 0.09518077619965482, 1.7587417204415798E15}
(Final BLEU[j=3]: 0.8559944011248749)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:40:52 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7295025065494896)

Final lambda[j=4]: {2904.580696099421, -0.8743327758402839, 2.339023448926629E15}
(Final BLEU[j=4]: 0.8559944011248749)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:40:52 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.7906639669266251)

Final lambda[j=5]: {0.7737957524668755, 0.6493837878144797, 6.435396406726453E11}
(Final BLEU[j=5]: 0.8559944011248749)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:40:52 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.832112954739801)

Final lambda[j=6]: {4.489578561799212E-17, 0.15636862697411513, 3.856326341157016E-5}
(Final BLEU[j=6]: 0.8568891604305179)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:40:52 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.7906639669266251)

Final lambda[j=7]: {0.32669861662639876, 0.009245077311689887, 2.717041411531953E11}
(Final BLEU[j=7]: 0.8559944011248749)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:40:52 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8320638639779733)

Final lambda[j=8]: {8.081071934424937E-17, 0.08804197789590318, 6.777315274844448E-5}
(Final BLEU[j=8]: 0.8569387675839043)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:40:52 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7296904500164655)

Final lambda[j=9]: {27.27291983164887, 0.8593774528720801, 2.196252218952822E13}
(Final BLEU[j=9]: 0.8559944011248749)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:40:52 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.7902384719266103)

Final lambda[j=10]: {0.5053771649119221, -0.5713556223455967, 4.203050198644001E11}
(Final BLEU[j=10]: 0.8559944011248749)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:40:52 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.7902867942108688)

Final lambda[j=11]: {0.7980226618452084, -0.399205792984231, 6.636883381890594E11}
(Final BLEU[j=11]: 0.8559944011248749)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:40:52 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8319656563794524)

Final lambda[j=12]: {0.594593012768021, -0.2974935383022126, 4.945028097703056E11}
(Final BLEU[j=12]: 0.8559944011248749)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:40:52 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7296434777641371)

Final lambda[j=13]: {1260.8711430864712, 0.981831267676273, 1.0153664031607435E15}
(Final BLEU[j=13]: 0.8559944011248749)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:40:52 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.832112954739801)

Final lambda[j=14]: {2.2446833578910783E-18, 0.00790321507749736, 1.8768906270665765E-6}
(Final BLEU[j=14]: 0.8568891604305179)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:40:53 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8320638639779733)

Final lambda[j=15]: {4.489437331121175E-17, 0.02541025170060851, 3.7005396462041096E-5}
(Final BLEU[j=15]: 0.856733253578528)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:40:53 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.7908757393312549)

Final lambda[j=16]: {0.6576106932759427, 0.17534745529356877, 5.469124738719801E11}
(Final BLEU[j=16]: 0.8559944011248749)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:40:53 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7290827283470592)

Final lambda[j=17]: {3659.0627792071023, -0.8495287747458757, 2.946635671657281E15}
(Final BLEU[j=17]: 0.8559944011248749)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:40:53 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7295025065494896)

Final lambda[j=18]: {1943.0228560533892, -0.10291979788243277, 1.5646873214210695E15}
(Final BLEU[j=18]: 0.8559944011248749)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:40:53 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7296904500164655)

Final lambda[j=19]: {3708.1112925923535, 0.3073471861931083, 2.986093895500107E15}
(Final BLEU[j=19]: 0.8559944011248749)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:40:53 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7290827283470592)

Final lambda[j=20]: {1224.35830572624, -0.8022503514522665, 9.859633517210222E14}
(Final BLEU[j=20]: 0.8559944011248749)

Best final lambda is lambda[j=8] (BLEU: 0.8569).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:40:53 JST 2015  ---

Next iteration will decode with lambda: {8.081071934424937E-17, 0.08804197789590318, 6.777315274844448E-5}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:40:53 JST 2015 ---
Redecoding using weight vector {8.081071934424937E-17, 0.08804197789590318, 6.777315274844448E-5}
Running decoder...
...finished decoding @ Tue Oct 27 10:40:55 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+...+...+...+...500
            ...+...+...+...+
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 9061 distinct candidates (about 9 per sentence):
newCandidatesAdded[it=1] = 9061 (about 9 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:40:56 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:40:56 JST 2015
----------------------------------------------------

FINAL lambda: {8.081071934424937E-17, 0.08804197789590318, 6.777315274844448E-5} (BLEU: 0.8569387675839043)

(OP Lamda) : [8.081071934424937E-17,0.08804197789590318,6.777315274844448E-5]
Number of candidates => 608
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 608
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:40:57 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:40:57 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:40:58 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 3143
...+...+...+...+...500
            ...+
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 3143 distinct candidates (about 5 per sentence):
newCandidatesAdded[it=1] = 3143 (about 5 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.7882019793449119)

Final lambda[j=1]: {0.04115925918849377, 0.2, 5.373767389150081E15}
(Final BLEU[j=1]: 0.8357297175192571)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7541523109412845)

Final lambda[j=2]: {4.569595722758032E-18, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.8356860409287858)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.739260728635238)

Final lambda[j=3]: {0.1646370367539751, 0.09518077619965482, 2.1521888490206344E16}
(Final BLEU[j=3]: 0.8357297175192571)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.739260728635238)

Final lambda[j=4]: {0.1646370367539751, -0.8743327758402839, 1.769442278309629E16}
(Final BLEU[j=4]: 0.8357297175192571)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.7882019793449119)

Final lambda[j=5]: {0.3292740735079502, 0.6493837878144797, 4.158198380469344E16}
(Final BLEU[j=5]: 0.8357297175192571)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.813497401710768)

Final lambda[j=6]: {9.139191445516065E-18, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.8357297175192571)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.7878379722108146)

Final lambda[j=7]: {0.1646370367539751, 0.009245077311689887, 1.755602372107386E16}
(Final BLEU[j=7]: 0.8357297175192571)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.813497401710768)

Final lambda[j=8]: {9.139191445516065E-18, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.8357297175192571)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.739260728635238)

Final lambda[j=9]: {0.0026672046560548245, 0.8593774528720801, 1.4592125966260606E14}
(Final BLEU[j=9]: 0.8357297175192571)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.7882019793449119)

Final lambda[j=10]: {0.5053771649119221, -0.5713556223455967, 2.7157793280248088E16}
(Final BLEU[j=10]: 0.8355657189042588)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.7882019793449119)

Final lambda[j=11]: {0.3292740735079502, -0.399205792984231, 4.2883881560265232E16}
(Final BLEU[j=11]: 0.8357297175192571)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.813497401710768)

Final lambda[j=12]: {0.3292740735079502, -0.2974935383022126, 3.1952045418292888E16}
(Final BLEU[j=12]: 0.8357297175192571)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.738859135841677)

Final lambda[j=13]: {0.08231851837698755, 0.981831267676273, 8.70787183933259E15}
(Final BLEU[j=13]: 0.8357297175192571)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.813497401710768)

Final lambda[j=14]: {0.020579629594246886, 0.00790321507749736, 2.017192214367269E15}
(Final BLEU[j=14]: 0.8357297175192571)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8148743972137877)

Final lambda[j=15]: {4.569595722758032E-18, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.8357297175192571)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.7882019793449119)

Final lambda[j=16]: {0.3292740735079502, 0.17534745529356877, 3.5338468982826368E16}
(Final BLEU[j=16]: 0.8357297175192571)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.739260728635238)

Final lambda[j=17]: {0.3292740735079502, -0.8495287747458757, 3.6058258929997784E16}
(Final BLEU[j=17]: 0.8357297175192571)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7388168827241942)

Final lambda[j=18]: {0.1646370367539751, -0.10291979788243277, 1.9147226487132448E16}
(Final BLEU[j=18]: 0.8357297175192571)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.739260728635238)

Final lambda[j=19]: {0.3292740735079502, 0.3073471861931083, 3.6541112940735472E16}
(Final BLEU[j=19]: 0.8357297175192571)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:40:59 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7388168827241942)

Final lambda[j=20]: {0.1646370367539751, -0.8022503514522665, 1.2065326627858768E16}
(Final BLEU[j=20]: 0.8357297175192571)

Best final lambda is lambda[j=1] (BLEU: 0.8357).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:40:59 JST 2015  ---

Next iteration will decode with lambda: {0.04115925918849377, 0.2, 5.373767389150081E15}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:40:59 JST 2015 ---
Redecoding using weight vector {0.04115925918849377, 0.2, 5.373767389150081E15}
Running decoder...
...finished decoding @ Tue Oct 27 10:41:00 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+...+...+...+...500
            ...+
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 3143 distinct candidates (about 5 per sentence):
newCandidatesAdded[it=1] = 3143 (about 5 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:41:00 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:41:00 JST 2015
----------------------------------------------------

FINAL lambda: {0.04115925918849377, 0.2, 5.373767389150081E15} (BLEU: 0.8357297175192571)

(OP Lamda) : [0.04115925918849377,0.2,5.373767389150081E15]
Number of candidates => 417
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 417
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:41:01 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:41:01 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:41:02 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 3460
...+...+...+...+
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 3460 distinct candidates (about 8 per sentence):
newCandidatesAdded[it=1] = 3460 (about 8 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.7936827859260808)

Final lambda[j=1]: {0.1, 0.2, 1.0900645735061925E15}
(Final BLEU[j=1]: 0.8252728314849292)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.758204797670557)

Final lambda[j=2]: {8.743615976448803E-18, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.8252728314849292)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7441887984820093)

Final lambda[j=3]: {0.65687942331747, 0.09518077619965482, 1.259053311284757E16}
(Final BLEU[j=3]: 0.8254223600636365)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7441887984820093)

Final lambda[j=4]: {0.31502196522724435, -0.8743327758402839, 1.6744671399529624E16}
(Final BLEU[j=4]: 0.8252728314849292)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.7935308145656096)

Final lambda[j=5]: {0.7737957524668755, 0.6493837878144797, 8.434873368937078E15}
(Final BLEU[j=5]: 0.8252728314849292)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8093238548519931)

Final lambda[j=6]: {3.4543350037047217E-17, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.8254223600636365)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.793378832482362)

Final lambda[j=7]: {0.32669861662639876, 0.009245077311689887, 3.5612258819791845E15}
(Final BLEU[j=7]: 0.8252728314849292)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8094745835404237)

Final lambda[j=8]: {1.7487231952897606E-17, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.8252728314849292)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7441887984820093)

Final lambda[j=9]: {0.001215386080898238, 0.8593774528720801, 2.6827678450821875E13}
(Final BLEU[j=9]: 0.8254223600636365)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.7935308145656096)

Final lambda[j=10]: {0.5053771649119221, -0.5713556223455967, 5.50893743729483E15}
(Final BLEU[j=10]: 0.8252728314849292)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.7935308145656096)

Final lambda[j=11]: {0.7980226618452084, -0.399205792984231, 8.698962325325736E15}
(Final BLEU[j=11]: 0.8252728314849292)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8091731159439287)

Final lambda[j=12]: {1.7271675018523608E-17, -0.2974935383022126, 0.3275255626321376}
(Final BLEU[j=12]: 0.8254223600636365)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7441887984820093)

Final lambda[j=13]: {0.328439711658735, 0.981831267676273, 5.094197416060428E15}
(Final BLEU[j=13]: 0.8254223600636365)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8090223668125135)

Final lambda[j=14]: {2.279008313029844E-18, 0.00790321507749736, 0.03620028521108054}
(Final BLEU[j=14]: 0.8254223600636365)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8093238548519931)

Final lambda[j=15]: {9.704007462163943E-18, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.8252728314849292)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.7938547953770769)

Final lambda[j=16]: {0.6576106932759427, 0.17534745529356877, 7.168381198989519E15}
(Final BLEU[j=16]: 0.8252728314849292)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7441887984820093)

Final lambda[j=17]: {0.31502196522724435, -0.8495287747458757, 2.109446405023242E16}
(Final BLEU[j=17]: 0.8252728314849292)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7438520745745107)

Final lambda[j=18]: {0.1748118575624082, -0.10291979788243277, 1.120133064601348E16}
(Final BLEU[j=18]: 0.8252728314849292)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7441887984820093)

Final lambda[j=19]: {0.31502196522724435, 0.3073471861931083, 2.1376938769569E16}
(Final BLEU[j=19]: 0.8252728314849292)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:41:03 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7438520745745107)

Final lambda[j=20]: {0.15751098261362217, -0.8022503514522665, 7.058344089762584E15}
(Final BLEU[j=20]: 0.8252728314849292)

Best final lambda is lambda[j=3] (BLEU: 0.8254).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:41:03 JST 2015  ---

Next iteration will decode with lambda: {0.65687942331747, 0.09518077619965482, 1.259053311284757E16}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:41:03 JST 2015 ---
Redecoding using weight vector {0.65687942331747, 0.09518077619965482, 1.259053311284757E16}
Running decoder...
...finished decoding @ Tue Oct 27 10:41:04 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+...+...+...+
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 3460 distinct candidates (about 8 per sentence):
newCandidatesAdded[it=1] = 3460 (about 8 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:41:05 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:41:05 JST 2015
----------------------------------------------------

FINAL lambda: {0.65687942331747, 0.09518077619965482, 1.259053311284757E16} (BLEU: 0.8254223600636365)

(OP Lamda) : [0.65687942331747,0.09518077619965482,1.259053311284757E16]
Number of candidates => 290
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 290
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:41:06 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:41:06 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:41:07 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 7763
...+...+...
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 7763 distinct candidates (about 26 per sentence):
newCandidatesAdded[it=1] = 7763 (about 26 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:41:08 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.795604979042366)

Final lambda[j=1]: {0.04380951991658247, 0.2, 5.44823856470895E15}
(Final BLEU[j=1]: 0.8212926668280485)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:41:08 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7634550600591706)

Final lambda[j=2]: {4.342917580199898E-18, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.8212926668280485)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:41:08 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7584236051774417)

Final lambda[j=3]: {0.15647009596711228, 0.09518077619965482, 2.1955519120830184E16}
(Final BLEU[j=3]: 0.8212926668280485)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:41:08 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7584236051774417)

Final lambda[j=4]: {0.31294019193422457, -0.8743327758402839, 2.9199554124459404E16}
(Final BLEU[j=4]: 0.8212926668280485)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:41:08 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.795604979042366)

Final lambda[j=5]: {0.31294019193422457, 0.6493837878144797, 4.2158238597980112E16}
(Final BLEU[j=5]: 0.8212926668280485)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:41:08 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8078476043664137)

Final lambda[j=6]: {8.685835160399795E-18, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.8212926668280485)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:41:08 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.7951900272785856)

Final lambda[j=7]: {0.15647009596711228, 0.009245077311689887, 1.7799320021410102E16}
(Final BLEU[j=7]: 0.8212926668280485)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:41:09 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8079067029149904)

Final lambda[j=8]: {8.685835160399795E-18, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.8212926668280485)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:41:09 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7584236051774417)

Final lambda[j=9]: {0.0027380949947864042, 0.8593774528720801, 1.5761840653330906E14}
(Final BLEU[j=9]: 0.8212926668280485)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:41:09 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.7954947813495626)

Final lambda[j=10]: {0.31294019193422457, -0.5713556223455967, 2.7534153595964084E16}
(Final BLEU[j=10]: 0.8212926668280485)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:41:09 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.795604979042366)

Final lambda[j=11]: {0.31294019193422457, -0.399205792984231, 4.3478178417767544E16}
(Final BLEU[j=11]: 0.8212926668280485)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:41:09 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8074024790041806)

Final lambda[j=12]: {4.863833770916422E-18, -0.2974935383022126, 0.3}
(Final BLEU[j=12]: 0.8212926668280485)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:41:09 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7584236051774417)

Final lambda[j=13]: {0.07823504798355614, 0.981831267676273, 8.883321124779806E15}
(Final BLEU[j=13]: 0.8212926668280485)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:41:09 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8079067029149904)

Final lambda[j=14]: {6.079792213645528E-19, 0.00790321507749736, 0.08125}
(Final BLEU[j=14]: 0.8212926668280485)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:41:09 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8078476043664137)

Final lambda[j=15]: {4.342917580199898E-18, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.8212926668280485)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:41:09 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.7951900272785856)

Final lambda[j=16]: {0.31294019193422457, 0.17534745529356877, 3.5828199396709792E16}
(Final BLEU[j=16]: 0.8212926668280485)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:41:09 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7584236051774417)

Final lambda[j=17]: {0.31294019193422457, -0.8495287747458757, 3.6784773499856648E16}
(Final BLEU[j=17]: 0.8212926668280485)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:41:09 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7584236051774417)

Final lambda[j=18]: {0.15647009596711228, -0.10291979788243277, 1.9533011586803928E16}
(Final BLEU[j=18]: 0.8212926668280485)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:41:09 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7586206259471697)

Final lambda[j=19]: {0.31294019193422457, 0.3073471861931083, 3.727735622419076E16}
(Final BLEU[j=19]: 0.8212926668280485)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:41:09 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7584236051774417)

Final lambda[j=20]: {0.15647009596711228, -0.8022503514522665, 1.2308423101325852E16}
(Final BLEU[j=20]: 0.8212003989239001)

Best final lambda is lambda[j=1] (BLEU: 0.8213).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:41:09 JST 2015  ---

Next iteration will decode with lambda: {0.04380951991658247, 0.2, 5.44823856470895E15}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:41:09 JST 2015 ---
Redecoding using weight vector {0.04380951991658247, 0.2, 5.44823856470895E15}
Running decoder...
...finished decoding @ Tue Oct 27 10:41:10 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+...+...
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 7763 distinct candidates (about 26 per sentence):
newCandidatesAdded[it=1] = 7763 (about 26 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:41:11 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:41:11 JST 2015
----------------------------------------------------

FINAL lambda: {0.04380951991658247, 0.2, 5.44823856470895E15} (BLEU: 0.8212926668280485)

(OP Lamda) : [0.04380951991658247,0.2,5.44823856470895E15]
Number of candidates => 200
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 200
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:41:12 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:41:12 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:41:12 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 2425
...+...+
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2425 distinct candidates (about 12 per sentence):
newCandidatesAdded[it=1] = 2425 (about 12 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.8068531009827984)

Final lambda[j=1]: {0.1, 0.2, 5.752625189551698E15}
(Final BLEU[j=1]: 0.8271968687033173)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7813064823907225)

Final lambda[j=2]: {4.3383624355222495E-18, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.827427560187975)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7743658608643853)

Final lambda[j=3]: {0.15630597958412928, 0.09518077619965482, 1.8691931777463612E16}
(Final BLEU[j=3]: 0.827427560187975)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7743658608643853)

Final lambda[j=4]: {0.31261195916825857, -0.8743327758402839, 2.4859174161312828E16}
(Final BLEU[j=4]: 0.827427560187975)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.8068531009827984)

Final lambda[j=5]: {0.31261195916825857, 0.6493837878144797, 4.4513569372090576E16}
(Final BLEU[j=5]: 0.827427560187975)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8150865458235481)

Final lambda[j=6]: {8.676724871044499E-18, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.827427560187975)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.8063098138310354)

Final lambda[j=7]: {0.15630597958412928, 0.009245077311689887, 1.8793746913967144E16}
(Final BLEU[j=7]: 0.827427560187975)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8150865458235481)

Final lambda[j=8]: {8.676724871044499E-18, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.827427560187975)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7743658608643853)

Final lambda[j=9]: {0.00244228093100202, 0.8593774528720801, 2.6157874970646606E14}
(Final BLEU[j=9]: 0.827427560187975)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.8068531009827984)

Final lambda[j=10]: {0.5053771649119221, -0.5713556223455967, 2.9072454090965456E16}
(Final BLEU[j=10]: 0.8271968687033173)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.8068531009827984)

Final lambda[j=11]: {0.6252239183365171, -0.399205792984231, 4.5907252663638432E16}
(Final BLEU[j=11]: 0.827427560187975)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8150865458235481)

Final lambda[j=12]: {0.31261195916825857, -0.2974935383022126, 3.420470742780752E16}
(Final BLEU[j=12]: 0.827427560187975)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7743658608643853)

Final lambda[j=13]: {0.07815298979206464, 0.981831267676273, 7.562856132340308E15}
(Final BLEU[j=13]: 0.827427560187975)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8150368772241998)

Final lambda[j=14]: {0.01953824744801616, 0.00790321507749736, 2.1594069679990448E15}
(Final BLEU[j=14]: 0.827427560187975)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8150865458235481)

Final lambda[j=15]: {4.3383624355222495E-18, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.827427560187975)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.8063098138310354)

Final lambda[j=16]: {0.31261195916825857, 0.17534745529356877, 3.7829878390577424E16}
(Final BLEU[j=16]: 0.827427560187975)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7743658608643853)

Final lambda[j=17]: {0.31261195916825857, -0.8495287747458757, 3.1316885422965712E16}
(Final BLEU[j=17]: 0.827427560187975)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7743658608643853)

Final lambda[j=18]: {0.15630597958412928, -0.10291979788243277, 1.66295188913365E16}
(Final BLEU[j=18]: 0.827427560187975)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7743658608643853)

Final lambda[j=19]: {0.31261195916825857, 0.3073471861931083, 3.1736247981752808E16}
(Final BLEU[j=19]: 0.827427560187975)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:41:13 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7743658608643853)

Final lambda[j=20]: {0.07815298979206464, -0.8022503514522665, 1.0478832389795964E16}
(Final BLEU[j=20]: 0.827427560187975)

Best final lambda is lambda[j=2] (BLEU: 0.8274).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:41:13 JST 2015  ---

Next iteration will decode with lambda: {4.3383624355222495E-18, -0.6577977590753961, 0.46016061237234096}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:41:13 JST 2015 ---
Redecoding using weight vector {4.3383624355222495E-18, -0.6577977590753961, 0.46016061237234096}
Running decoder...
...finished decoding @ Tue Oct 27 10:41:14 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+...+
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2425 distinct candidates (about 12 per sentence):
newCandidatesAdded[it=1] = 2425 (about 12 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:41:14 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:41:14 JST 2015
----------------------------------------------------

FINAL lambda: {4.3383624355222495E-18, -0.6577977590753961, 0.46016061237234096} (BLEU: 0.827427560187975)

(OP Lamda) : [4.3383624355222495E-18,-0.6577977590753961,0.46016061237234096]
Number of candidates => 118
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 118
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:41:14 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:41:14 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:41:15 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 2526
...+
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2526 distinct candidates (about 21 per sentence):
newCandidatesAdded[it=1] = 2526 (about 21 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:41:15 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.8031499441786497)

Final lambda[j=1]: {0.1, 0.2, 4.3733651988802156E14}
(Final BLEU[j=1]: 0.8271080971468279)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:41:15 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7871841147095197)

Final lambda[j=2]: {5.144779586612399E-17, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.8271080971468279)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:41:15 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7835858400826642)

Final lambda[j=3]: {0.30893369905561247, 0.09518077619965482, 2.713020635644027E15}
(Final BLEU[j=3]: 0.8271080971468279)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:41:15 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7835858400826642)

Final lambda[j=4]: {0.41191159874081656, -0.8743327758402839, 3.608158497883315E15}
(Final BLEU[j=4]: 0.8271080971468279)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:41:15 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.8031499441786497)

Final lambda[j=5]: {0.25744474921301036, 0.6493837878144797, 2.265332344706374E15}
(Final BLEU[j=5]: 0.8271080971468279)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:41:15 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8135502458849357)

Final lambda[j=6]: {6.859706115483199E-17, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.8271080971468279)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:41:15 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.8031499441786497)

Final lambda[j=7]: {0.10297789968520414, 0.009245077311689887, 9.564293172393578E14}
(Final BLEU[j=7]: 0.8271080971468279)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:41:15 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8135502458849357)

Final lambda[j=8]: {1.143284352580533E-16, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.8271080971468279)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:41:15 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7835858400826642)

Final lambda[j=9]: {0.00965417809548789, 0.8593774528720801, 8.749789345114738E13}
(Final BLEU[j=9]: 0.8271080971468279)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:41:15 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.8031499441786497)

Final lambda[j=10]: {0.15446684952780623, -0.5713556223455967, 1.4795212228823818E15}
(Final BLEU[j=10]: 0.8271080971468279)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:41:15 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.8031499441786497)

Final lambda[j=11]: {0.25744474921301036, -0.399205792984231, 2.3362580395709985E15}
(Final BLEU[j=11]: 0.8271080971468279)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:41:15 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8135502458849357)

Final lambda[j=12]: {4.0014952340318656E-17, -0.2974935383022126, 0.225}
(Final BLEU[j=12]: 0.8271080971468279)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:41:15 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7835858400826642)

Final lambda[j=13]: {0.12872237460650518, 0.981831267676273, 1.0977027412535545E15}
(Final BLEU[j=13]: 0.8271080971468279)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:41:16 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8135502458849357)

Final lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 1.6416635836485688E14}
(Final BLEU[j=14]: 0.8271080971468279)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:41:16 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.813621301411696)

Final lambda[j=15]: {6.859706115483199E-17, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.8271080971468279)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:41:16 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.8031499441786497)

Final lambda[j=16]: {0.23170027429170933, 0.17534745529356877, 1.9251937852509038E15}
(Final BLEU[j=16]: 0.8271080971468279)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:41:16 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7835858400826642)

Final lambda[j=17]: {0.5663784482686228, -0.8495287747458757, 4.545456157669255E15}
(Final BLEU[j=17]: 0.8271080971468279)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:41:16 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7835858400826642)

Final lambda[j=18]: {0.15446684952780623, -0.10291979788243277, 1.4548052726764492E15}
(Final BLEU[j=18]: 0.8271080971468279)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:41:16 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7835858400826642)

Final lambda[j=19]: {0.5663784482686228, 0.3073471861931083, 4.606324091992535E15}
(Final BLEU[j=19]: 0.8271080971468279)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:41:16 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7835858400826642)

Final lambda[j=20]: {0.18021132444910726, -0.8022503514522665, 1.5209390259624175E15}
(Final BLEU[j=20]: 0.8271080971468279)

Best final lambda is lambda[j=1] (BLEU: 0.8271).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:41:16 JST 2015  ---

Next iteration will decode with lambda: {0.1, 0.2, 4.3733651988802156E14}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:41:16 JST 2015 ---
Redecoding using weight vector {0.1, 0.2, 4.3733651988802156E14}
Running decoder...
...finished decoding @ Tue Oct 27 10:41:16 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2526 distinct candidates (about 21 per sentence):
newCandidatesAdded[it=1] = 2526 (about 21 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:41:16 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:41:16 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, 0.2, 4.3733651988802156E14} (BLEU: 0.8271080971468279)

(OP Lamda) : [0.1,0.2,4.3733651988802156E14]
Number of candidates => 67
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 67
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:41:17 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:41:17 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:41:17 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 841
..
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 841 distinct candidates (about 12 per sentence):
newCandidatesAdded[it=1] = 841 (about 12 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.7966209299271624)

Final lambda[j=1]: {0.04156976080758715, 0.2, 3.074896374925797E15}
(Final BLEU[j=1]: 0.8137530486134666)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7758803126375615)

Final lambda[j=2]: {4.615170557674369E-18, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.8137530486134666)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7722150994254059)

Final lambda[j=3]: {0.1662790432303486, 0.09518077619965482, 1.6413493792421576E16}
(Final BLEU[j=3]: 0.8137530486134666)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7725801185508009)

Final lambda[j=4]: {0.1662790432303486, -0.8743327758402839, 2.1828985127871132E16}
(Final BLEU[j=4]: 0.8137530486134666)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.7966209299271624)

Final lambda[j=5]: {0.7737957524668755, 0.6493837878144797, 2.3793417541933744E16}
(Final BLEU[j=5]: 0.8133953874485507)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8036714351671543)

Final lambda[j=6]: {9.230341115348739E-18, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.8137530486134666)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.797342032475588)

Final lambda[j=7]: {0.0831395216151743, 0.009245077311689887, 1.004564391957786E16}
(Final BLEU[j=7]: 0.8137530486134666)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8036714351671543)

Final lambda[j=8]: {9.230341115348739E-18, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.8137530486134666)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7722150994254059)

Final lambda[j=9]: {0.0025981100504741967, 0.8593774528720801, 2.6191171032517006E14}
(Final BLEU[j=9]: 0.8137530486134666)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.797342032475588)

Final lambda[j=10]: {0.1662790432303486, -0.5713556223455967, 1.5539824123579456E16}
(Final BLEU[j=10]: 0.8137530486134666)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.7966209299271624)

Final lambda[j=11]: {0.3325580864606972, -0.399205792984231, 2.4538369900164664E16}
(Final BLEU[j=11]: 0.8137530486134666)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8036714351671543)

Final lambda[j=12]: {4.615170557674369E-18, -0.2974935383022126, 0.3275255626321376}
(Final BLEU[j=12]: 0.8137530486134666)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7725801185508009)

Final lambda[j=13]: {0.0831395216151743, 0.981831267676273, 6.640987868937602E15}
(Final BLEU[j=13]: 0.8137530486134666)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8036714351671543)

Final lambda[j=14]: {3.1980221407780524E-18, 0.00790321507749736, 0.2296875}
(Final BLEU[j=14]: 0.8137530486134666)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8036714351671543)

Final lambda[j=15]: {4.615170557674369E-18, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.8137530486134666)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.7966209299271624)

Final lambda[j=16]: {0.1662790432303486, 0.17534745529356877, 2.022084736866636E16}
(Final BLEU[j=16]: 0.8137530486134666)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7722150994254059)

Final lambda[j=17]: {0.3325580864606972, -0.8495287747458757, 2.7499538870967088E16}
(Final BLEU[j=17]: 0.8137530486134666)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7722150994254059)

Final lambda[j=18]: {0.1662790432303486, -0.10291979788243277, 1.4602477065692914E16}
(Final BLEU[j=18]: 0.8137530486134666)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7722150994254059)

Final lambda[j=19]: {0.3325580864606972, 0.3073471861931083, 2.7867783568057464E16}
(Final BLEU[j=19]: 0.8137530486134666)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:41:17 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7722150994254059)

Final lambda[j=20]: {0.0831395216151743, -0.8022503514522665, 9.201523546598396E15}
(Final BLEU[j=20]: 0.8137530486134666)

Best final lambda is lambda[j=1] (BLEU: 0.8138).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:41:17 JST 2015  ---

Next iteration will decode with lambda: {0.04156976080758715, 0.2, 3.074896374925797E15}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:41:17 JST 2015 ---
Redecoding using weight vector {0.04156976080758715, 0.2, 3.074896374925797E15}
Running decoder...
...finished decoding @ Tue Oct 27 10:41:18 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ..
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 841 distinct candidates (about 12 per sentence):
newCandidatesAdded[it=1] = 841 (about 12 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:41:18 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:41:18 JST 2015
----------------------------------------------------

FINAL lambda: {0.04156976080758715, 0.2, 3.074896374925797E15} (BLEU: 0.8137530486134666)

(OP Lamda) : [0.04156976080758715,0.2,3.074896374925797E15]
Number of candidates => 28
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 28
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:41:18 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:41:18 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:41:18 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 361
.
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 361 distinct candidates (about 12 per sentence):
newCandidatesAdded[it=1] = 361 (about 12 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.8255450151157742)

Final lambda[j=1]: {0.1, 0.2, 4.104483912769569E13}
(Final BLEU[j=1]: 0.8339932219487743)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.8018129833270048)

Final lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 3.8893099797262324E12}
(Final BLEU[j=2]: 0.8352815834749604)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.800204855999261)

Final lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, 1.1744573448729652E14}
(Final BLEU[j=3]: 0.8352815834749604)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.800204855999261)

Final lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, 1.561959460842405E14}
(Final BLEU[j=4]: 0.8352815834749604)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.8255450151157742)

Final lambda[j=5]: {0.7737957524668755, 0.6493837878144797, 3.17603221776971E14}
(Final BLEU[j=5]: 0.8339932219487743)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8287865192268846)

Final lambda[j=6]: {7.9694015298893E-16, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.8339932219487743)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.8255450151157742)

Final lambda[j=7]: {0.32669861662639876, 0.009245077311689887, 1.340929216267125E14}
(Final BLEU[j=7]: 0.8339932219487743)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8287865192268846)

Final lambda[j=8]: {1.1840253701549819E-15, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.8339932219487743)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.800204855999261)

Final lambda[j=9]: {0.0012517785212184471, 0.8593774528720801, 1.0278466482548479E12}
(Final BLEU[j=9]: 0.8339932219487743)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.8255450151157742)

Final lambda[j=10]: {0.5053771649119221, -0.5713556223455967, 2.074312443262075E14}
(Final BLEU[j=10]: 0.8339932219487743)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.8255450151157742)

Final lambda[j=11]: {0.7980226618452084, -0.399205792984231, 3.275471177569204E14}
(Final BLEU[j=11]: 0.8339932219487743)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8271343681860972)

Final lambda[j=12]: {4.041625061586717E-16, -0.2974935383022126, 0.3275255626321376}
(Final BLEU[j=12]: 0.8339932219487743)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.800204855999261)

Final lambda[j=13]: {0.05768195425774604, 0.981831267676273, 4.751917585936053E13}
(Final BLEU[j=13]: 0.8339932219487743)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8287865192268846)

Final lambda[j=14]: {4.411632989760149E-17, 0.00790321507749736, 0.03620028521108054}
(Final BLEU[j=14]: 0.8339932219487743)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8287865192268846)

Final lambda[j=15]: {7.229385673542437E-16, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.8339932219487743)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.8255450151157742)

Final lambda[j=16]: {0.6576106932759427, 0.17534745529356877, 2.6991525114163456E14}
(Final BLEU[j=16]: 0.8339932219487743)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.800204855999261)

Final lambda[j=17]: {0.24034147607394185, -0.8495287747458757, 1.96771240883151E14}
(Final BLEU[j=17]: 0.8339932219487743)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.800204855999261)

Final lambda[j=18]: {0.12657984406560938, -0.10291979788243277, 1.0448711688099303E14}
(Final BLEU[j=18]: 0.8339932219487743)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.800204855999261)

Final lambda[j=19]: {0.24354602908826106, 0.3073471861931083, 1.9940619292126E14}
(Final BLEU[j=19]: 0.8339932219487743)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:41:18 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.800204855999261)

Final lambda[j=20]: {0.08091496361156042, -0.8022503514522665, 6.584092972523453E13}
(Final BLEU[j=20]: 0.8339932219487743)

Best final lambda is lambda[j=2] (BLEU: 0.8353).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:41:18 JST 2015  ---

Next iteration will decode with lambda: {-0.017230818545013626, -0.6577977590753961, 3.8893099797262324E12}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:41:18 JST 2015 ---
Redecoding using weight vector {-0.017230818545013626, -0.6577977590753961, 3.8893099797262324E12}
Running decoder...
...finished decoding @ Tue Oct 27 10:41:19 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: .
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 361 distinct candidates (about 12 per sentence):
newCandidatesAdded[it=1] = 361 (about 12 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:41:19 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:41:19 JST 2015
----------------------------------------------------

FINAL lambda: {-0.017230818545013626, -0.6577977590753961, 3.8893099797262324E12} (BLEU: 0.8352815834749604)

(OP Lamda) : [-0.017230818545013626,-0.6577977590753961,3.8893099797262324E12]
Number of candidates => 15
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 15
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:41:19 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:41:19 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:41:19 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 74

Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 74 distinct candidates (about 4 per sentence):
newCandidatesAdded[it=1] = 74 (about 4 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.848919633709986)

Final lambda[j=1]: {0.1, 0.2, 6.382284855764272E14}
(Final BLEU[j=1]: 0.8626653885447875)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.8249901002156383)

Final lambda[j=2]: {8.250784026057312E-18, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.8626653885447875)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.822070677286207)

Final lambda[j=3]: {0.04219388039384911, 0.09518077619965482, 2.713433469290508E15}
(Final BLEU[j=3]: 0.8626653885447875)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.822070677286207)

Final lambda[j=4]: {0.06928080320194938, -0.8743327758402839, 3.608707542446482E15}
(Final BLEU[j=4]: 0.8626653885447875)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.848919633709986)

Final lambda[j=5]: {0.03967605412955763, 0.6493837878144797, 2.469292456212029E15}
(Final BLEU[j=5]: 0.8626653885447875)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8596924462223581)

Final lambda[j=6]: {2.2939880817988823E-17, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.8626653885447875)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.848919633709986)

Final lambda[j=7]: {0.02960474907239173, 0.009245077311689887, 1.0425418166469012E15}
(Final BLEU[j=7]: 0.8626653885447875)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8596924462223581)

Final lambda[j=8]: {2.6294299246735636E-17, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.8626653885447875)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.822070677286207)

Final lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, 4.374529863568989E13}
(Final BLEU[j=9]: 0.8626653885447875)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.848919633709986)

Final lambda[j=10]: {0.054173845616200506, -0.5713556223455967, 1.6127305130332215E15}
(Final BLEU[j=10]: 0.8626653885447875)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.848919633709986)

Final lambda[j=11]: {0.06172732440907491, -0.399205792984231, 2.546603974625683E15}
(Final BLEU[j=11]: 0.8626653885447875)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8596924462223581)

Final lambda[j=12]: {7.132644549808378E-18, -0.2974935383022126, 0.3275255626321376}
(Final BLEU[j=12]: 0.8626653885447875)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.822070677286207)

Final lambda[j=13]: {0.02960474907239173, 0.981831267676273, 1.0978697759673602E15}
(Final BLEU[j=13]: 0.8626653885447875)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8596924462223581)

Final lambda[j=14]: {8.216968514604886E-19, 0.00790321507749736, 0.03620028521108054}
(Final BLEU[j=14]: 0.8626653885447875)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8596924462223581)

Final lambda[j=15]: {1.8602584958802786E-17, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.8626653885447875)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.848919633709986)

Final lambda[j=16]: {0.05922265773281571, 0.17534745529356877, 2.098529384341846E15}
(Final BLEU[j=16]: 0.8626653885447875)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.822070677286207)

Final lambda[j=17]: {0.1625215368486015, -0.8495287747458757, 4.54614782850133E15}
(Final BLEU[j=17]: 0.8626653885447875)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.822070677286207)

Final lambda[j=18]: {0.05920949814478346, -0.10291979788243277, 2.414041184996995E15}
(Final BLEU[j=18]: 0.8626653885447875)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.822070677286207)

Final lambda[j=19]: {0.12345464881814983, 0.3073471861931083, 4.607025024947775E15}
(Final BLEU[j=19]: 0.8626653885447875)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.822070677286207)

Final lambda[j=20]: {0.054173845616200506, -0.8022503514522665, 1.5211704634958862E15}
(Final BLEU[j=20]: 0.8626653885447875)

Best final lambda is lambda[j=1] (BLEU: 0.8627).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:41:19 JST 2015  ---

Next iteration will decode with lambda: {0.1, 0.2, 6.382284855764272E14}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:41:19 JST 2015 ---
Redecoding using weight vector {0.1, 0.2, 6.382284855764272E14}
Running decoder...
...finished decoding @ Tue Oct 27 10:41:19 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: 
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 74 distinct candidates (about 4 per sentence):
newCandidatesAdded[it=1] = 74 (about 4 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:41:19 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:41:19 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, 0.2, 6.382284855764272E14} (BLEU: 0.8626653885447875)

(OP Lamda) : [0.1,0.2,6.382284855764272E14]
Number of candidates => 8
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 8
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:41:19 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:41:19 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:41:19 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 52

Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 52 distinct candidates (about 6 per sentence):
newCandidatesAdded[it=1] = 52 (about 6 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.8700377656474493)

Final lambda[j=1]: {0.1, 0.2, -0.1}
(Final BLEU[j=1]: 0.8700377656474493)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.8330710287340123)

Final lambda[j=2]: {1.447069752995375E-14, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.8700377656474493)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.8330710287340123)

Final lambda[j=3]: {0.05000019808857066, 0.09518077619965482, 3.974858333550954E11}
(Final BLEU[j=3]: 0.8700377656474493)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.8330710287340123)

Final lambda[j=4]: {0.050002129523272085, -0.8743327758402839, 3.975011876529025E11}
(Final BLEU[j=4]: 0.8700377656474493)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.8700377656474493)

Final lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Final BLEU[j=5]: 0.8700377656474493)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8700377656474493)

Final lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.8700377656474493)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.8628416930896428)

Final lambda[j=7]: {0.32669861662639876, 0.009245077311689887, 2.5971511483948433E12}
(Final BLEU[j=7]: 0.8700377656474493)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8700377656474493)

Final lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.8700377656474493)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.8330710287340123)

Final lambda[j=9]: {0.050000047538361486, 0.8593774528720801, -0.01427276865927607}
(Final BLEU[j=9]: 0.8700377656474493)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.8700377656474493)

Final lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Final BLEU[j=10]: 0.8700377656474493)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.8700377656474493)

Final lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Final BLEU[j=11]: 0.8700377656474493)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8700377656474493)

Final lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Final BLEU[j=12]: 0.8700377656474493)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.8330710287340123)

Final lambda[j=13]: {0.05000269882538541, 0.981831267676273, 3.9750571342547125E11}
(Final BLEU[j=13]: 0.8700377656474493)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8700377656474493)

Final lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Final BLEU[j=14]: 0.8700377656474493)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8700377656474493)

Final lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.8700377656474493)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.8700377656474493)

Final lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Final BLEU[j=16]: 0.8700377656474493)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.8330710287340123)

Final lambda[j=17]: {0.05000132048891283, -0.8495287747458757, 3.974947560844528E11}
(Final BLEU[j=17]: 0.8700377656474493)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.8330710287340123)

Final lambda[j=18]: {0.05000289983135746, -0.10291979788243277, 3.975073113596669E11}
(Final BLEU[j=18]: 0.8700377656474493)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.8330710287340123)

Final lambda[j=19]: {0.05000027494910857, 0.3073471861931083, 3.9748644437217395E11}
(Final BLEU[j=19]: 0.8700377656474493)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:41:19 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.8330710287340123)

Final lambda[j=20]: {0.05000112754134413, -0.8022503514522665, -0.33852947919372234}
(Final BLEU[j=20]: 0.8700377656474493)

Best final lambda is lambda[j=1] (BLEU: 0.8700).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:41:19 JST 2015  ---

No parameter value changed in this iteration; exiting Z-MERT.


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:41:19 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, 0.2, -0.1} (BLEU: 0.8700377656474493)

(OP Lamda) : [0.1,0.2,-0.1]
Number of candidates => 3
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 3
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:41:19 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:41:19 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:41:20 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 4

Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 4 distinct candidates (about 1 per sentence):
newCandidatesAdded[it=1] = 4 (about 1 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.9037664163933351)

Final lambda[j=1]: {0.1, 0.2, -0.1}
(Final BLEU[j=1]: 0.9037664163933351)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.8830710861889355)

Final lambda[j=2]: {0.049999996380437256, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.9037664163933351)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.8830710861889355)

Final lambda[j=3]: {0.050000000467810654, 0.09518077619965482, -0.059473491599185246}
(Final BLEU[j=3]: 0.9037664163933351)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.8830710861889355)

Final lambda[j=4]: {0.05000000502913246, -0.8743327758402839, -0.6393613928095458}
(Final BLEU[j=4]: 0.9037664163933351)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.9037664163933351)

Final lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Final BLEU[j=5]: 0.9037664163933351)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.9037664163933351)

Final lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.9037664163933351)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.9037664163933351)

Final lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Final BLEU[j=7]: 0.9037664163933351)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.9037664163933351)

Final lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.9037664163933351)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.8830710861889355)

Final lambda[j=9]: {0.050000000112267726, 0.8593774528720801, -0.01427276865927607}
(Final BLEU[j=9]: 0.9037664163933351)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.9037664163933351)

Final lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Final BLEU[j=10]: 0.9037664163933351)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.9037664163933351)

Final lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Final BLEU[j=11]: 0.9037664163933351)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.9037664163933351)

Final lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Final BLEU[j=12]: 0.9037664163933351)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.8830710861889355)

Final lambda[j=13]: {0.05000000637360981, 0.981831267676273, -0.8102868749874037}
(Final BLEU[j=13]: 0.9037664163933351)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.9037664163933351)

Final lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Final BLEU[j=14]: 0.9037664163933351)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.9037664163933351)

Final lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.9037664163933351)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.9037664163933351)

Final lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Final BLEU[j=16]: 0.9037664163933351)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.8830710861889355)

Final lambda[j=17]: {0.050000003118497796, -0.8495287747458757, -0.3964594524802063}
(Final BLEU[j=17]: 0.9037664163933351)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.8830710861889355)

Final lambda[j=18]: {0.05000000684831024, -0.10291979788243277, -0.8706362780408492}
(Final BLEU[j=18]: 0.9037664163933351)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.8830710861889355)

Final lambda[j=19]: {0.05000000064932632, 0.3073471861931083, -0.08254985860243691}
(Final BLEU[j=19]: 0.9037664163933351)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.8830710861889355)

Final lambda[j=20]: {0.050000002662828294, -0.8022503514522665, -0.33852947919372234}
(Final BLEU[j=20]: 0.9037664163933351)

Best final lambda is lambda[j=1] (BLEU: 0.9038).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:41:20 JST 2015  ---

No parameter value changed in this iteration; exiting Z-MERT.


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:41:20 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, 0.2, -0.1} (BLEU: 0.9037664163933351)

(OP Lamda) : [0.1,0.2,-0.1]
Number of candidates => 1
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 1
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:41:20 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:41:20 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:41:20 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 2

Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2 distinct candidates (about 2 per sentence):
newCandidatesAdded[it=1] = 2 (about 2 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.9426092134060646)

Final lambda[j=1]: {0.1, 0.2, -0.1}
(Final BLEU[j=1]: 0.9426092134060646)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.8916809066702428)

Final lambda[j=2]: {0.04999983683296236, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.9426092134060646)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.8916809066702428)

Final lambda[j=3]: {0.05000002108853557, 0.09518077619965482, -0.059473491599185246}
(Final BLEU[j=3]: 0.9426092134060646)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.8916809066702428)

Final lambda[j=4]: {0.05000022670933073, -0.8743327758402839, -0.6393613928095458}
(Final BLEU[j=4]: 0.9426092134060646)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.9426092134060646)

Final lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Final BLEU[j=5]: 0.9426092134060646)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.9426092134060646)

Final lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.9426092134060646)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.9426092134060646)

Final lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Final BLEU[j=7]: 0.9426092134060646)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.9426092134060646)

Final lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.9426092134060646)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.8916809066702428)

Final lambda[j=9]: {0.050000005060940295, 0.8593774528720801, -0.01427276865927607}
(Final BLEU[j=9]: 0.9426092134060646)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.9426092134060646)

Final lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Final BLEU[j=10]: 0.9426092134060646)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.9426092134060646)

Final lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Final BLEU[j=11]: 0.9426092134060646)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.9426092134060646)

Final lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Final BLEU[j=12]: 0.9426092134060646)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.8916809066702428)

Final lambda[j=13]: {0.050000287317309405, 0.981831267676273, -0.8102868749874037}
(Final BLEU[j=13]: 0.9426092134060646)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.9426092134060646)

Final lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Final BLEU[j=14]: 0.9426092134060646)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.9426092134060646)

Final lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.9426092134060646)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.9426092134060646)

Final lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Final BLEU[j=16]: 0.9426092134060646)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.8916809066702428)

Final lambda[j=17]: {0.050000140579425256, -0.8495287747458757, -0.3964594524802063}
(Final BLEU[j=17]: 0.9426092134060646)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.8916809066702428)

Final lambda[j=18]: {0.05000030871643191, -0.10291979788243277, -0.8706362780408492}
(Final BLEU[j=18]: 0.9426092134060646)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.8916809066702428)

Final lambda[j=19]: {0.05000002927111866, 0.3073471861931083, -0.08254985860243691}
(Final BLEU[j=19]: 0.9426092134060646)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:41:20 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.8916809066702428)

Final lambda[j=20]: {0.05000012003820142, -0.8022503514522665, -0.33852947919372234}
(Final BLEU[j=20]: 0.9426092134060646)

Best final lambda is lambda[j=1] (BLEU: 0.9426).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:41:20 JST 2015  ---

No parameter value changed in this iteration; exiting Z-MERT.


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:41:20 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, 0.2, -0.1} (BLEU: 0.9426092134060646)

(OP Lamda) : [0.1,0.2,-0.1]
Number of candidates => 0
Processing 526 sentences...

Corpus level score:
 See data/results/bleu_results.txt
Test Cumulative: 113809
Making Dependency Statistics ....
487212
0
168292
corpara: 0
next => 
1446, 2274, 6645, 5375, 6395, 9606, 191, 6358, 3123, 6822, 9142, 6235, 2582, 2441, 9572, 9219, 874, 4841, 5372, 3909, 3267, 7297, 240, 9571, 9212, 4126, 6340, 9368, 9710, 3373, 3347, 3873, 4237, 5611, 5547, 344, 1343, 4412, 5069, 3316, 7080, 7875, 3389, 4361, 7682, 4833, 4063, 947, 2866, 5772, 6442, 7768, 2344, 4928, 2493, 117, 4674, 66, 5119, 5409, 5447, 5695, 7577, 9416, 6446, 5229, 6593, 454, 5982, 3906, 1337, 76, 7600, 8260, 7403, 4751, 7341, 6210, 9027, 908, 5030, 7559, 156, 4821, 3816, 4867, 3615, 8450, 1995, 7192, 9631, 9412, 5550, 397, 5613, 5575, 6302, 6621, 968, 5048, 7084, 6146, 6322, 3667, 1171, 6003, 8001, 4049, 8882, 2317, 1119, 1566, 308, 651, 3928, 2488, 2279, 284, 6143, 5529, 1494, 5012, 5844, 9008, 1988, 5242, 6623, 3534, 3528, 7086, 4585, 374, 5479, 6084, 1, 504, 3676, 2499, 860, 2430, 8617, 4981, 192, 9659, 7738, 8171, 7083, 8461, 5244, 4449, 8903, 2502, 1277, 3235, 2825, 4289, 1616, 3073, 2780, 234, 2778, 576, 8798, 1714, 7268, 7713, 64, 2985, 5768, 183, 1210, 1679, 6730, 6103, 7891, 586, 7557, 4858, 650, 4510, 5020, 4123, 6681, 7973, 926, 1377, 1067, 653, 237, 1151, 2833, 819, 3436, 4720, 4848, 7754, 696, 7678, 9244, 2288, 8639, 7195, 9539, 3717, 9208, 6678, 3159, 737, 8725, 2544, 8302, 9533, 1245, 5009, 8214, 4281, 2531, 2311, 2459, 4268, 8115, 3098, 5760, 5521, 9232, 507, 2319, 9394, 3363, 6875, 203, 9201, 4786, 5993, 7692, 1604, 3127, 4378, 6506, 3277, 6141, 890, 2119, 3311, 1766, 2808, 6183, 5560, 8799, 2135, 6006, 6253, 8045, 3513, 7685, 5698, 7342, 743, 6474, 120, 1744, 2994, 8792, 2283, 7035, 2638, 1950, 9239, 6804, 1782, 6606, 5814, 2173, 540, 6614, 4958, 8216, 6070, 982, 2519, 846, 2873, 2293, 5870, 5974, 5326, 7198, 2179, 3446, 3016, 6846, 430, 6854, 8650, 1091, 5443, 8596, 4210, 2551, 296, 7477, 6632, 6204, 2147, 2640, 185, 751, 4909, 7490, 6954, 971, 9041, 1293, 3586, 2410, 1544, 6477, 5604, 8601, 5681, 5144, 4584, 3821, 1364, 843, 1027, 3888, 9679, 2740, 8755, 4083, 9513, 7176, 9165, 4261, 9096, 1439, 8843, 1349, 4114, 5819, 4600, 2818, 6012, 9064, 5461, 3910, 8614, 4470, 4798, 9409, 8451, 1465, 2795, 6027, 7772, 3567, 9044, 7628, 8203, 7757, 7701, 2303, 2087, 776, 2637, 3474, 4216, 3372, 7087, 9099, 3823, 8419, 4522, 8356, 5221, 2991, 1534, 4189, 4697, 1154, 4620, 420, 7133, 1562, 4317, 7911, 8663, 8387, 5941, 2697, 6007, 6736, 5485, 806, 2533, 1693, 4030, 7806, 2202, 7457, 1229, 2131, 6930, 5115, 9308, 1849, 3064, 6577, 4492, 1462, 7502, 7430, 213, 5782, 1445, 5498, 1815, 4353, 2063, 3629, 2205, 6950, 4762, 5487, 7352, 3581, 3397, 468, 9126, 6162, 1595, 1725, 63, 1514, 6123, 1082, 3181, 6921, 7752, 1836, 7580, 5593, 7474, 2229, 1466, 8683, 4389, 1444, 2647, 7337, 3058, 9139, 4499, 6932, 5526, 8824, 5101, 5395, 5253, 3487, 6310, 5867, 7721, 8626, 1739, 2539, 4419, 1432, 1086, 9713, 7469, 1034, 976, 3650, 8002, 2057, 2759, 8283, 9037, 3308, 3370, 3248, 1919, 8513, 3991, 513, 7749, 4919, 3194, 6649, 141, 4290, 4278, 4662, 2049, 2297, 9354, 4142, 1777, 6960, 300, 6983, 8999, 8928, 7712, 4708, 7304, 8165, 5175, 623, 9645, 6602, 9383, 861, 7108, 1199, 9446, 7410, 5628, 8674, 7793, 348, 4509, 8038, 3034, 366, 7463, 3499, 994, 9085, 2183, 3559, 8920, 4913, 3005, 3892, 5846, 9390, 45, 4903, 8550, 3769, 4443, 5580, 121, 8071, 7306, 6193, 9357, 5751, 336, 577, 8521, 5657, 7674, 4220, 5678, 5536, 4060, 8794, 7285, 7848, 1335, 1341, 3532, 4478, 8246, 7270, 8618, 1387, 8652, 5075, 7948, 7889, 7512, 2851, 852, 2245, 1406, 7301, 987, 2284, 4729, 5451, 4249, 1572, 1472, 7305, 174, 7586, 1762, 8130, 2936, 4444, 9433, 353, 759, 1924, 4682, 2620, 1856, 5389, 6025, 7739, 7812, 5460, 2108, 4040, 6790, 8148, 2854, 4551, 2751, 544, 1670, 4609, 8516, 84, 8742, 2371, 4887, 7380, 608, 3068, 2120, 2775, 4316, 7900, 1998, 5672, 7340, 3701, 354, 4653, 7438, 2586, 2017, 7027, 457, 3687, 1949, 2762, 4271, 1615, 5494, 8808, 8385, 2314, 5332, 616, 6807, 1242, 3238, 7867, 9301, 3182, 1775, 3750, 6972, 2398, 2133, 4495, 3673, 7466, 4511, 378, 8265, 5873, 2436, 5706, 1390, 5923, 9343, 4983, 3683, 3346, 745, 4002, 4846, 8095, 9292, 3450, 1369, 1555, 795, 4688, 3464, 7881, 3072, 7185, 4012, 8661, 5073, 6612, 6723, 2241, 9020, 9168, 5866, 1927, 6408, 3431, 5152, 2684, 1023, 5135, 3356, 6334, 6722, 4678, 2453, 6802, 9567, 4070, 8089, 478, 8769, 3753, 3157, 1265, 2322, 3552, 8624, 8409, 9164, 3124, 1658, 1951, 6246, 8561, 4148, 2567, 8000, 7649, 4080, 5770, 61, 3327, 201, 1876, 4416, 2614, 2529, 4498, 4345, 2357, 9733, 1411, 4773, 7890, 8344, 5627, 401, 1046, 1060, 1507, 2540, 9470, 4067, 9176, 4451, 2929, 6927, 6880, 5922, 4202, 1279, 4068, 9460, 1237, 5324, 313, 536, 8065, 7834, 1323, 5214, 6783, 6533, 2964, 8454, 8487, 2374, 8242, 7464, 5912, 7961, 5011, 7552, 6242, 6047, 1518, 9650, 7598, 260, 1374, 6964, 2385, 9634, 5083, 2250, 3057, 7254, 8782, 8811, 3916, 3319, 4488, 8948, 8562, 7170, 6845, 8101, 4631, 5481, 6740, 486, 4559, 5364, 7656, 8870, 3558, 4888, 5272, 1272, 9667, 837, 8559, 8647, 4354, 2219, 4149, 7962, 4115, 7379, 6724, 5213, 9355, 7943, 7625, 1723, 4957, 346, 7300, 5577, 5944, 48, 7234, 8966, 4855, 563, 3940, 3711, 7082, 7956, 9674, 9247, 6078, 1952, 7237, 5146, 4715, 5017, 2865, 6318, 7707, 8141, 6015, 2084, 6677, 7515, 1340, 6940, 7321, 4783, 413, 8057, 4009, 6516, 2980, 2416, 4908, 1318, 5474, 1267, 4875, 3205, 8217, 4484, 8149, 5114, 5929, 4576, 5256, 3843, 1694, 7894, 1928, 5802, 3460, 5202, 6369, 4586, 7671, 2478, 8758, 3514, 1624, 4548, 1811, 7856, 4822, 4966, 7399, 521, 1008, 9098, 8790, 3089, 215, 1754, 6530, 3941, 1541, 1116, 3481, 1527, 7427, 3688, 3332, 7, 246, 4977, 425, 8987, 7906, 24, 7992, 6901, 1491, 1583, 1904, 7088, 4780, 992, 7232, 5401, 1274, 3031, 2843, 1032, 7387, 8146, 447, 791, 9194, 8704, 5386, 983, 2997, 7613, 5504, 8620, 4325, 2188, 4992, 632, 4055, 2086, 763, 7590, 9045, 6957, 9258, 5281, 1672, 2329, 3078, 9661, 6988, 9663, 4619, 5931, 1051, 4050, 3152, 83, 6367, 7191, 3345, 673, 9688, 9111, 670, 5815, 2296, 7634, 6352, 1490, 2054, 491, 2469, 753, 4844, 7130, 1962, 5441, 3655, 3376, 8651, 5765, 3184, 8576, 2175, 2711, 1383, 441, 3300, 3729, 3939, 6673, 9504, 7475, 7153, 4912, 4276, 4591, 8939, 6475, 4861, 7067, 1632, 658, 2151, 302, 7264, 8353, 2150, 4911, 5792, 802, 6597, 4635, 4482, 5149, 9032, 1786, 6450, 2916, 8094, 5057, 5806, 1619, 9303, 5798, 4288, 9265, 6066, 4866, 2710, 9406, 3000, 3612, 816, 4018, 3094, 3093, 7710, 9681, 4279, 2472, 939, 1489, 5082, 3506, 7124, 840, 3656, 6432, 4673, 5553, 4459, 2537, 8719, 2207, 1809, 5994, 588, 7882, 9074, 7830, 9197, 7764, 3904, 4503, 6065, 5164, 488, 8028, 8958, 293, 7350, 8673, 4641, 2255, 2148, 7645, 592, 8020, 911, 3955, 7370, 5036, 4079, 7244, 4565, 7271, 131, 6935, 6882, 2341, 781, 3496, 3887, 6924, 1961, 7476, 6911, 1484, 9445, 6776, 6429, 7047, 907, 4886, 4104, 3393, 3561, 2836, 3253, 8273, 848, 9289, 6351, 3030, 1602, 5418, 6553, 6171, 90, 6562, 6716, 7266, 9040, 3908, 8466, 132, 1872, 683, 2566, 5312, 8705, 3737, 3315, 4234, 3021, 4255, 5947, 3126, 878, 4538, 1930, 7667, 855, 2585, 6309, 3262, 8402, 4486, 424, 5255, 1862, 5081, 6720, 1025, 3454, 6192, 979, 5884, 4239, 5874, 6733, 3907, 1603, 8219, 7633, 6089, 8309, 1834, 69, 5670, 6773, 6316, 8573, 1248, 2232, 9424, 4929, 70, 3439, 8795, 7316, 4253, 4987, 3364, 5876, 2768, 3144, 6488, 4618, 8872, 7121, 4445, 4852, 1825, 1402, 6622, 6903, 2216, 1228, 1131, 944, 6841, 5366, 5379, 7527, 9622, 9053, 1537, 8885, 4594, 6161, 1101, 7227, 3846, 9088, 7008, 8643, 6371, 9115, 7452, 687, 4231, 6792, 765, 1325, 1328, 5354, 8878, 7478, 4997, 8675, 3111, 5915, 4627, 9338, 1621, 1662, 1746, 3269, 5483, 4953, 2634, 9325, 5396, 7302, 2904, 9417, 8729, 2704, 2930, 5352, 5072, 8280, 1223, 9441, 379, 3508, 9671, 2888, 6472, 6456, 9223, 8058, 5810, 3802, 2337, 5600, 6216, 6069, 6713, 7178, 331, 8086, 2561, 8881, 6063, 5459, 4385, 8880, 525, 8591, 3643, 1234, 8444, 8032, 9552, 7498, 4504, 6257, 1800, 5891, 8173, 5992, 6019, 3369, 2391, 7101, 928, 3212, 2686, 4362, 8933, 290, 6667, 9640, 2390, 8537, 1667, 5522, 2373, 5559, 1376, 6810, 28, 8963, 8603, 2302, 2861, 1852, 6151, 6798, 9413, 8127, 9577, 4714, 7795, 5068, 1469, 3110, 8357, 3386, 1964, 4532, 1550, 547, 5650, 603, 3716, 3599, 7941, 4535, 2744, 6005, 5824, 7960, 7847, 4377, 7037, 9314, 7844, 8698, 2662, 6262, 9275, 8034, 8426, 3556, 5127, 8, 1410, 3024, 4350, 8937, 717, 5040, 9038, 1075, 841, 4756, 3700, 6699, 3472, 8383, 4203, 2895, 1983, 6076, 5438, 9296, 7360, 8726, 11, 1375, 6625, 4964, 6326, 4258, 8072, 7787, 5946, 836, 3242, 4254, 600, 6919, 2078, 9210, 8730, 4052, 1142, 2943, 6920, 5939, 5299, 8415, 9315, 1205, 1758, 9237, 152, 3896, 1516, 851, 1195, 1304, 3164, 7351, 97, 797, 1294, 8684, 3774, 3165, 3368, 2224, 5374, 1788, 210, 4737, 9463, 5283, 6992, 105, 7929, 2363, 1682, 8167, 4547, 2901, 7562, 6153, 5274, 1397, 5917, 8888, 6830, 9334, 7372, 6086, 4972, 320, 1701, 9474, 5525, 5759, 6430, 8269, 4878, 930, 1436, 8506, 3905, 3805, 6717, 3207, 798, 6929, 642, 4579, 2298, 4910, 221, 8886, 6251, 434, 7939, 8623, 7488, 3427, 8163, 4101, 1916, 4561, 9566, 1959, 9089, 1405, 9477, 6308, 8715, 854, 1547, 7014, 1412, 4157, 8709, 1185, 8128, 4201, 5886, 6073, 9530, 9174, 6686, 2671, 8866, 8090, 8568, 8594, 6535, 3613, 3801, 6030, 4889, 1702, 2591, 3013, 7312, 8512, 1386, 8711, 2064, 5561, 5125, 2172, 4990, 4401, 1634, 906, 7980, 2892, 8365, 1338, 9558, 3685, 4937, 7662, 4649, 7150, 5510, 4563, 7280, 3946, 8604, 4206, 7278, 1947, 9324, 2454, 4134, 3365, 8530, 8600, 8331, 7359, 8313, 3971, 6317, 3674, 5589, 4742, 3609, 7493, 9442, 1458, 5938, 3915, 1588, 9182, 2616, 7295, 7578, 720, 6018, 8900, 5209, 739, 581, 4318, 2847, 5341, 1559, 2580, 1965, 1373, 3466, 8299, 4307, 4621, 3471, 8897, 1487, 3977, 2189, 9127, 725, 7760, 4524, 7816, 5720, 1419, 6906, 6755, 1586, 5390, 5848, 6595, 4305, 2422, 6871, 5445, 3276, 990, 8640, 3322, 6925, 9407, 2912, 8026, 2494, 6053, 8773, 9272, 8396, 391, 8274, 9059, 8364, 2875, 8823, 9218, 4915, 4031, 2235, 1078, 656, 5245, 8770, 1960, 8010, 2773, 3574, 6588, 7166, 5039, 4774, 9300, 9060, 5480, 3375, 523, 7332, 8891, 169, 8338, 3689, 764, 1921, 1394, 109, 3734, 1145, 9328, 636, 375, 2258, 2012, 9086, 8042, 1508, 5715, 6224, 5829, 6847, 5318, 1180, 3779, 3824, 388, 6876, 6956, 770, 6914, 9104, 8902, 707, 9178, 3680, 8957, 6554, 2927, 6548, 3082, 1753, 5572, 1363, 1302, 8221, 5430, 3681, 9680, 7523, 969, 937, 114, 5426, 2631, 2460, 9384, 427, 5899, 7782, 1931, 9356, 2512, 8833, 1647, 7241, 6020, 7866, 1984, 7480, 6492, 4199, 1360, 5509, 7118, 4399, 4411, 256, 1750, 7758, 6338, 5243, 6034, 3820, 4477, 8607, 2118, 8334, 8310, 8819, 8480, 6870, 2042, 3677, 2592, 9603, 3647, 549, 2755, 9250, 7168, 7446, 6411, 7058, 5789, 1429, 9545, 4608, 3746, 8836, 1269, 8780, 275, 5092, 1356, 9476, 5916, 2137, 6900, 8774, 6785, 6296, 6569, 6110, 9599, 9696, 4481, 1594, 6478, 1826, 3282, 7579, 8921, 6939, 9200, 957, 3318, 9554, 7217, 1629, 2242, 9472, 4312, 6655, 3168, 8772, 4021, 7612, 167, 1606, 6254, 1122, 6987, 5300, 5534, 4690, 8835, 1298, 6111, 2877, 5588, 7048, 3857, 8496, 6099, 7987, 7615, 2606, 8629, 8736, 6585, 5237, 2005, 3703, 7627, 1955, 1688, 4223, 4137, 3234, 3610, 9499, 2589, 93, 7719, 6168, 8976, 8507, 2681, 5178, 677, 9593, 4432, 2272, 4487, 7673, 7614, 2557, 5392, 4745, 4175, 2641, 7174, 8412, 4518, 2278, 2708, 796, 7635, 8967, 3954, 5143, 7029, 4061, 8828, 6339, 8261, 8133, 6422, 5167, 3535, 8728, 8689, 1837, 8727, 9514, 3782, 8172, 8200, 1897, 1790, 9144, 995, 3874, 7766, 9618, 2434, 6970, 9401, 5838, 2741, 2600, 4457, 129, 601, 3344, 2601, 2918, 6865, 4768, 8998, 1989, 5749, 6600, 145, 2562, 6750, 5813, 2280, 7657, 7151, 2394, 1480, 9503, 675, 8895, 8717, 5649, 7343, 5172, 2212, 4460, 4765, 1581, 4800, 4430, 8654, 4435, 142, 3878, 8190, 5216, 4003, 1509, 7852, 9133, 5417, 2909, 2310, 1637, 3607, 6082, 1530, 8367, 8441, 6342, 6743, 9662, 2992, 4830, 1068, 3879, 1792, 4820, 4461, 4227, 2810, 3786, 9092, 3170, 3218, 6121, 2425, 2672, 5858, 422, 8587, 5236, 8225, 6469, 9454, 4062, 1895, 3962, 5518, 1557, 524, 5410, 7668, 3845, 2835, 7565, 7243, 8757, 81, 2786, 1681, 9047, 2277, 2774, 3206, 4520, 1238, 1724, 2506, 5766, 2577, 4368, 6470, 7832, 6480, 8319, 5016, 1747, 5131, 8830, 1511, 5433, 9279, 6781, 9169, 5203, 3518, 2669, 674, 3727, 1233, 3158, 7798, 3053, 8063, 3984, 1005, 9437, 5704, 5215, 2766, 222, 2885, 1194, 2223, 8228, 3881, 490, 5199, 3569, 8867, 4418, 6283, 2673, 6537, 462, 9114, 4694, 8578, 808, 6347, 7876, 1347, 3113, 9110, 176, 6055, 1177, 2368, 2705, 8797, 664, 102, 1130, 8860, 692, 7940, 4984, 6605, 5308, 7291, 1764, 3443, 6517, 2136, 7831, 9172, 6737, 1022, 2203, 741, 5100, 5701, 648, 5508, 1286, 3394, 8458, 5666, 1756, 3362, 4761, 5113, 9451, 7222, 4280, 3632, 3409, 4946, 4311, 7730, 7005, 1280, 5953, 7778, 6884, 8899, 483, 1871, 1315, 6337, 9050, 3745, 7448, 6952, 5909, 9623, 6631, 5578, 6330, 5970, 562, 9087, 9657, 6104, 2779, 1385, 825, 6398, 2706, 9120, 2545, 4195, 786, 7630, 5883, 8013, 1810, 9443, 1468, 4324, 6043, 8051, 5732, 1521, 6650, 3813, 4607, 1643, 3841, 6345, 3227, 9714, 1263, 6464, 2215, 582, 2530, 8810, 3576, 4749, 5961, 4982, 9149, 8052, 5691, 6198, 65, 3999, 3762, 3781, 8918, 4501, 3550, 8278, 9160, 5887, 6742, 5981, 6273, 8144, 6656, 630, 3419, 1890, 8391, 1188, 1190, 5788, 1847, 4136, 1536, 2275, 5863, 6354, 2090, 6218, 6626, 4755, 1495, 5420, 2569, 7672, 5804, 2138, 5546, 3597, 5388, 682, 6653, 7085, 1830, 1139, 823, 2670, 9693, 3229, 3921, 5614, 8022, 6668, 7104, 5616, 456, 2872, 3777, 5524, 4164, 4630, 2132, 9, 5329, 4716, 9257, 9145, 873, 4962, 3547, 9682, 8750, 1628, 757, 5412, 9039, 8074, 6313, 2076, 9414, 7203, 1316, 7560, 403, 5182, 4767, 5140, 9345, 9248, 2613, 8472, 5539, 9694, 139, 5228, 3842, 4262, 3201, 5517, 1903, 4636, 2450, 8645, 3374, 5356, 3757, 6426, 2597, 9337, 5468, 3313, 3275, 7376, 6038, 1333, 9057, 3467, 5226, 5044, 1442, 5063, 2165, 5786, 7404, 1099, 3051, 2062, 661, 1738, 6049, 6410, 4178, 8738, 6461, 7644, 7444, 6365, 1717, 4273, 4834, 1617, 211, 1869, 146, 4947, 3865, 6029, 2931, 1016, 8801, 9137, 1454, 7068, 3577, 3043, 2513, 6148, 8528, 4005, 637, 8138, 8490, 4490, 6834, 1939, 8767, 6159, 5118, 7695, 3295, 4638, 3657, 5054, 7631, 7589, 7235, 1711, 6557, 5667, 8439, 6575, 602, 3020, 2806, 7535, 6379, 4015, 9147, 6184, 629, 2785, 5800, 4537, 9507, 2963, 2208, 4064, 787, 6965, 3800, 931, 6549, 289, 6651, 1599, 1824, 7957, 5462, 5424, 3090, 1057, 8621, 3033, 7597, 7391, 8686, 9261, 116, 259, 8030, 2691, 9404, 2830, 5091, 8462, 6628, 5382, 8669, 2080, 6599, 6102, 6797, 6784, 7927, 3554, 8776, 2177, 2650, 1486, 115, 5108, 5475, 1059, 1652, 6494, 8911, 3379, 6000, 5393, 2351, 4270, 2944, 2630, 6473, 68, 5548, 5793, 3658, 4122, 7371, 8372, 7647, 2004, 3461, 575, 9148, 7532, 101, 3077, 945, 3772, 8401, 1115, 138, 8535, 811, 1089, 9385, 4622, 1796, 7858, 6010, 9642, 8084, 3890, 8325, 7283, 4132, 323, 5979, 5826, 4764, 4382, 5313, 1791, 9277, 3150, 7286, 8991, 3473, 8196, 6031, 2692, 2905, 204, 5808, 3455, 8732, 4343, 8118, 7650, 1828, 6596, 7822, 7228, 8040, 1443, 9639, 2554, 4775, 9466, 2213, 2942, 2729, 9701, 1158, 5440, 3616, 5514, 9313, 3382, 8501, 3062, 1434, 6995, 3143, 2285, 4096, 3665, 800, 3883, 6406, 267, 6526, 1281, 4420, 9715, 4717, 6343, 8151, 1549, 7194, 7926, 7530, 0, 2185, 282, 3017, 6786, 8982, 4727, 6277, 9021, 4393, 5378, 4436, 294, 6874, 9452, 4659, 8857, 4733, 8259, 9425, 4205, 2104, 7945, 7706, 9374, 3422, 951, 3583, 918, 6899, 2110, 5617, 1079, 6332, 7432, 8821, 4193, 2632, 8523, 3675, 910, 4917, 9305, 3367, 1148, 8145, 638, 7575, 1284, 6558, 4274, 2856, 8081, 3799, 6187, 3161, 2733, 4246, 6640, 6247, 3343, 7367, 3718, 9297, 3335, 9203, 850, 4082, 4016, 2573, 6948, 726, 5662, 2974, 2819, 2900, 4106, 3233, 2588, 1685, 2458, 8949, 4750, 6412, 584, 1125, 1243, 4138, 5010, 1882, 2508, 1030, 839, 6243, 9195, 7396, 4211, 2509, 8846, 9158, 2886, 7546, 3604, 3850, 2715, 3770, 3970, 2605, 3219, 5018, 459, 2276, 2629, 5338, 195, 6890, 4879, 933, 5820, 9400, 7596, 7805, 3666, 2490, 3882, 6067, 6769, 8482, 4165, 200, 3827, 5910, 7607, 4546, 2198, 8944, 9246, 7044, 8853, 8004, 1433, 6214, 528, 9492, 5535, 9188, 404, 7324, 6325, 6500, 7031, 269, 3348, 7549, 4568, 4924, 4826, 3503, 1666, 3758, 2510, 6295, 8465, 7792, 245, 5363, 1797, 9256, 4404, 7402, 4718, 7877, 2871, 4516, 900, 5224, 4515, 6258, 2957, 2126, 479, 6017, 2032, 6336, 2462, 9619, 7386, 1114, 5932, 611, 1449, 5179, 2894, 5998, 9082, 6234, 6779, 8756, 5968, 1464, 5456, 9284, 470, 7382, 659, 1612, 7781, 5194, 4760, 5755, 6091, 7303, 2253, 6646, 1861, 4932, 7727, 338, 4291, 9651, 8492, 6463, 7937, 2694, 1278, 7708, 6866, 5603, 940, 3169, 5106, 1636, 8317, 393, 3241, 5605, 5013, 8323, 2009, 7990, 9498, 6848, 5286, 2051, 4590, 3490, 1305, 7715, 2622, 7112, 6115, 3230, 2959, 7282, 1273, 5730, 14, 8608, 9022, 4359, 8980, 3028, 5184, 7969, 3544, 6860, 25, 6635, 1207, 5195, 6189, 2294, 7177, 2608, 2184, 8484, 4222, 4260, 974, 8339, 7510, 4428, 610, 335, 3641, 6482, 8892, 179, 4950, 1039, 5422, 7859, 3052, 640, 5262, 8555, 364, 1463, 2984, 3125, 3722, 6571, 8368, 1232, 8231, 8533, 5141, 5976, 9000, 9028, 6976, 9329, 1641, 6324, 9486, 9295, 2718, 5470, 4969, 9381, 7978, 6636, 4294, 5287, 3027, 2243, 2162, 1107, 9393, 1689, 5668, 8413, 8174, 4883, 1565, 5282, 4936, 1977, 7002, 8041, 2945, 198, 2140, 3796, 6144, 5601, 2286, 3501, 8665, 9724, 2536, 8012, 6837, 991, 9280, 2197, 7813, 8293, 4069, 9648, 3, 8855, 2998, 6966, 724, 5942, 3036, 731, 2982, 7720, 6211, 209, 8237, 7113, 6684, 4342, 1582, 2348, 3459, 8033, 7791, 909, 2181, 1045, 3252, 4890, 51, 7096, 6806, 8659, 1121, 3815, 7165, 4196, 4527, 6238, 1482, 828, 7065, 1221, 3831, 9660, 9033, 8416, 820, 8112, 5791, 3631, 1070, 1096, 4087, 439, 2456, 3981, 824, 3049, 6384, 6762, 446, 6998, 144, 280, 4119, 2355, 829, 1475, 7308, 9518, 5122, 9455, 357, 9586, 8125, 8825, 6236, 5065, 1181, 8212, 4045, 9500, 4320, 232, 5664, 5752, 8471, 1535, 4666, 2996, 5097, 3220, 2893, 9068, 6756, 7503, 9722, 1319, 5176, 6056, 6942, 7566, 9485, 3097, 6124, 6805, 7658, 5348, 4034, 7443, 2602, 4375, 8478, 1772, 6416, 7988, 4383, 9333, 7437, 4567, 4380, 8842, 2731, 5047, 2270, 5132, 184, 1794, 7216, 5919, 5289, 1622, 1982, 2100, 3360, 7344, 5585, 6644, 4007, 6534, 7603, 3255, 5683, 4172, 2356, 7348, 1368, 3626, 6504, 9723, 5654, 5937, 3433, 821, 8688, 9654, 4315, 5196, 1476, 7423, 3221, 6536, 1798, 1146, 5809, 2800, 2717, 4549, 7769, 4686, 5949, 8400, 17, 7654, 3391, 9260, 5984, 1186, 4292, 8906, 414, 1019, 3851, 8558, 4840, 3310, 3342, 7102, 8768, 440, 5646, 1391, 8930, 1128, 2408, 9562, 7563, 3047, 7901, 3312, 2433, 8369, 3107, 1677, 7924, 570, 5339, 2447, 1757, 621, 3797, 4921, 7783, 5448, 2657, 5581, 1506, 5619, 9439, 8657, 6662, 7349, 7747, 9524, 7921, 9716, 9365, 5343, 7164, 4414, 1726, 170, 2045, 2958, 870, 7319, 175, 6449, 6328, 5659, 1992, 9320, 2552, 5210, 1721, 2677, 9095, 904, 3263, 9512, 3032, 9573, 3187, 4476, 3987, 9484, 7260, 8505, 7366, 4285, 6524, 4528, 5059, 8934, 1502, 5936, 7950, 9450, 2976, 223, 3448, 2727, 7181, 975, 555, 8106, 8327, 2962, 4536, 6811, 1651, 6394, 5041, 2190, 6676, 4587, 719, 1812, 3817, 8476, 5171, 3405, 9616, 1225, 9553, 7326, 6660, 7218, 4372, 884, 4275, 9415, 1675, 3709, 4595, 6021, 349, 8816, 7449, 9366, 4162, 4217, 8968, 5310, 3086, 9581, 6550, 7519, 1755, 9582, 8787, 7786, 5702, 8922, 7311, 6579, 8510, 2703, 3545, 2811, 3931, 2652, 2129, 3432, 7272, 2576, 3627, 1160, 9286, 9526, 9497, 2831, 1773, 7021, 2651, 9617, 2468, 9231, 4168, 5150, 2883, 7811, 188, 9399, 7145, 2370, 6801, 4995, 448, 5845, 2574, 7675, 546, 1857, 8320, 6085, 6402, 1317, 2437, 6697, 119, 9049, 7190, 9190, 9555, 1698, 5554, 4301, 8871, 4400, 147, 7556, 7338, 7496, 8232, 6291, 8672, 7954, 8087, 7365, 8831, 432, 4876, 8428, 7737, 8520, 1880, 9017, 8043, 1560, 7724, 6634, 8786, 1009, 4027, 7652, 7880, 8635, 3728, 7915, 1905, 9003, 8188, 4028, 3140, 297, 5831, 7653, 8178, 4326, 881, 822, 2157, 5084, 2429, 5610, 3044, 4, 4174, 3542, 1029, 8218, 74, 3384, 898, 3965, 6033, 6672, 6967, 9012, 8849, 9432, 9350, 5298, 1902, 8292, 9336, 520, 8205, 7629, 5803, 3640, 2483, 3185, 8677, 1533, 7355, 4238, 7215, 3968, 4120, 5507, 6709, 9611, 6083, 1164, 6037, 6046, 4145, 4794, 305, 5029, 7788, 3989, 341, 3948, 3562, 761, 2404, 2570, 7910, 1716, 5323, 3961, 5499, 3155, 7998, 6898, 6618, 9264, 4658, 9402, 2244, 5259, 7568, 303, 6230, 6520, 5477, 5690, 4877, 2860, 6208, 2191, 5647, 4000, 892, 7179, 5449, 3134, 7075, 4891, 688, 4541, 406, 6839, 9471, 7004, 2999, 283, 6544, 9326, 9112, 5602, 3818, 5828, 6423, 9423, 5157, 4198, 8932, 4208, 9604, 2769, 6271, 3247, 1769, 831, 189, 2313, 779, 7242, 7288, 5219, 1178, 8706, 7330, 3066, 5206, 5713, 4117, 337, 2815, 3306, 8908, 7274, 8007, 4441, 3440, 2265, 2534, 7642, 7132, 9635, 4985, 8297, 5843, 1167, 263, 4089, 6278, 927, 9537, 1690, 522, 7497, 6540, 3197, 127, 3430, 2636, 4170, 326, 3428, 3420, 6666, 2899, 5609, 3101, 8111, 8380, 7053, 7459, 1416, 417, 6439, 506, 6113, 8754, 8414, 1044, 967, 4699, 1948, 941, 8088, 6833, 7564, 9717, 5952, 1922, 2724, 5064, 7392, 5136, 1236, 1976, 7175, 6058, 1149, 253, 4823, 4854, 1674, 8154, 5024, 4829, 7056, 7893, 2558, 3232, 999, 6679, 7531, 9094, 1878, 7912, 6485, 6001, 2343, 3932, 4464, 1015, 3135, 1973, 3074, 4244, 1296, 2813, 4857, 3301, 3992, 2048, 2164, 1326, 7681, 7986, 8281, 1270, 5078, 924, 8947, 1865, 7362, 2846, 2649, 9281, 1883, 6627, 5527, 8923, 458, 5812, 7823, 6228, 8316, 1822, 5758, 5686, 8854, 6728, 7955, 8914, 2023, 5737, 8526, 9161, 3918, 8554, 9734, 8850, 8207, 2523, 5964, 9543, 1209, 2518, 9100, 6941, 7817, 4634, 3719, 9431, 633, 7820, 3509, 5351, 8694, 9291, 7949, 500, 8992, 6128, 5273, 8153, 1428, 2033, 8753, 1695, 7001, 5856, 5187, 6943, 5748, 541, 949, 3934, 1968, 543, 2077, 2609, 9729, 8080, 9221, 3775, 7808, 4440, 4180, 5957, 777, 6705, 3659, 6702, 4176, 9624, 6993, 1453, 9709, 5365, 7173, 6186, 496, 7186, 5635, 4505, 6414, 9316, 2809, 5624, 2571, 1912, 3179, 3810, 7513, 6584, 3768, 442, 4434, 1073, 1102, 3593, 953, 7406, 7694, 8699, 7225, 8262, 8986, 494, 9004, 7886, 8070, 9506, 9018, 6057, 4473, 1839, 8781, 4185, 714, 2479, 2081, 5368, 9483, 7868, 4129, 2421, 6400, 7110, 4269, 9628, 8985, 1608, 5090, 9241, 8180, 9436, 9171, 2787, 7428, 2227, 2301, 7516, 42, 4583, 7103, 2002, 2105, 7054, 5163, 1093, 4610, 1136, 3091, 7487, 23, 3297, 2626, 6022, 6490, 3579, 7522, 1049, 4927, 3998, 8883, 7389, 2256, 1668, 526, 6383, 6753, 1208, 7030, 6209, 8455, 9146, 4868, 6042, 1627, 2318, 9309, 8775, 804, 3751, 1357, 4974, 3099, 8638, 1829, 612, 126, 2664, 1896, 4802, 9344, 7320, 73, 7750, 1415, 6574, 1367, 52, 9046, 6973, 6613, 5716, 2041, 2975, 4322, 9561, 5528, 6459, 2791, 6984, 6777, 9612, 8460, 5712, 5822, 262, 972, 5665, 6281, 2195, 8328, 2022, 6648, 4352, 6139, 1783, 2028, 5457, 9575, 3794, 2598, 6879, 8213, 7010, 4681, 1456, 9403, 5463, 4597, 1240, 7451, 7904, 5331, 6817, 3288, 3482, 2749, 1166, 1395, 8282, 6945, 8215, 6949, 8446, 9206, 8844, 2898, 6176, 6495, 6510, 9422, 3679, 6851, 9730, 1066, 4306, 1496, 2829, 6836, 3645, 4327, 4485, 9482, 6327, 8524, 4703, 4645, 1220, 3291, 9209, 7872, 3012, 1736, 2753, 1129, 8019, 3899, 7591, 6460, 4856, 7036, 5631, 4025, 7878, 5492, 6867, 7509, 3244, 3510, 7684, 4056, 9283, 8083, 4632, 1596, 5644, 4894, 655, 431, 7506, 746, 8641, 7794, 7666, 9193, 345, 2107, 5930, 8193, 5653, 87, 8271, 3302, 9348, 6502, 6008, 3979, 978, 2746, 36, 8378, 2981, 2525, 2325, 4906, 5990, 2328, 106, 2480, 1832, 882, 3588, 3621, 2680, 9459, 3515, 6199, 7702, 5563, 9035, 3839, 6568, 788, 9646, 7265, 8586, 2611, 57, 4221, 2967, 5342, 8152, 4633, 1380, 4347, 1052, 2541, 7015, 5717, 5583, 5959, 5573, 5275, 4923, 8276, 4707, 7256, 5606, 166, 7189, 6237, 7373, 977, 7163, 1329, 3188, 7558, 3854, 372, 1424, 4853, 5086, 327, 6447, 8162, 5651, 5349, 7454, 8427, 5875, 9388, 2924, 3884, 5622, 889, 6292, 1913, 4588, 1137, 2966, 2380, 2645, 2345, 9509, 2095, 2820, 9487, 2431, 8997, 252, 2237, 2732, 6715, 3735, 6641, 9615, 9737, 1350, 5437, 8377, 31, 9638, 8335, 9034, 6694, 8668, 1515, 2862, 5648, 6934, 7802, 5855, 538, 5035, 7677, 3662, 9643, 5419, 452, 5906, 130, 9502, 902, 8734, 6778, 4013, 9293, 3548, 749, 1426, 514, 7315, 7888, 451, 8570, 7538, 9397, 6688, 597, 2837, 4125, 7107, 5775, 8868, 685, 3132, 6443, 8879, 9270, 827, 438, 4759, 6407, 4241, 418, 6772, 4263, 8371, 3741, 6120, 9353, 2826, 4799, 5677, 4941, 4454, 219, 7807, 7605, 4564, 2455, 8894, 8300, 2015, 3151, 5197, 9569, 7409, 7495, 6303, 8951, 4700, 6683, 6910, 5222, 6360, 7381, 3283, 419, 1891, 2267, 5001, 2953, 4153, 6791, 5062, 8907, 5986, 1802, 7821, 6982, 9386, 3856, 7639, 7646, 6380, 3925, 4808, 3293, 6863, 9128, 4365, 8644, 6182, 5586, 8245, 7641, 1981, 4553, 1041, 402, 8082, 9079, 1980, 2340, 5317, 3381, 8199, 2838, 4971, 2069, 6926, 2668, 4215, 3486, 4642, 1953, 5293, 8192, 6547, 1043, 4417, 8778, 2079, 9243, 9109, 5625, 2055, 7057, 6573, 9346, 1332, 4914, 2855, 4781, 3415, 2604, 2438, 4304, 9347, 9029, 7545, 5327, 4560, 2225, 3095, 2607, 498, 913, 6765, 6126, 3014, 1366, 6260, 2378, 4351, 7637, 6819, 5121, 4677, 3488, 728, 3174, 8121, 5674, 4340, 3833, 2392, 6166, 1479, 8197, 4072, 5594, 1037, 1854, 5643, 2290, 6745, 5852, 2111, 9131, 7230, 2214, 7253, 8275, 4286, 3398, 9652, 689, 7196, 6122, 9664, 1817, 2812, 5633, 8746, 6749, 9525, 4963, 2864, 1504, 3557, 4033, 3565, 3691, 3076, 3011, 4948, 1805, 8508, 8865, 7976, 4053, 6793, 103, 5165, 8404, 2675, 3231, 9702, 7033, 3585, 7669, 916, 2025, 1050, 838, 5544, 3847, 4693, 4073, 2400, 4456, 1626, 3538, 8438, 7745, 2463, 3408, 9083, 3457, 8143, 7298, 8584, 6659, 3257, 7209, 4849, 6969, 5362, 3519, 6452, 1932, 8964, 3195, 4283, 3636, 8352, 7548, 6955, 7129, 2707, 1771, 1740, 2852, 8532, 2221, 2246, 4438, 9076, 9084, 8625, 4863, 3975, 2687, 4605, 5252, 3614, 3967, 4692, 2419, 760, 3200, 463, 9071, 833, 7074, 9205, 6105, 1592, 8893, 6552, 7327, 2956, 3328, 7996, 7970, 4639, 6809, 2443, 9469, 5051, 4381, 3108, 8598, 2828, 5032, 7296, 6386, 9043, 7640, 8791, 482, 1123, 277, 46, 8077, 6782, 5387, 4502, 7622, 1704, 6937, 1875, 2850, 2145, 5918, 2661, 4818, 782, 387, 3619, 3573, 8198, 4489, 3990, 4989, 3837, 3015, 1435, 1403, 395, 6922, 5859, 288, 4793, 1239, 3038, 9727, 5074, 2034, 7935, 3630, 3186, 2902, 7776, 789, 6714, 2358, 9493, 47, 4384, 5056, 1571, 8399, 7426, 8100, 4331, 2234, 3891, 2857, 3203, 7679, 7416, 6642, 9051, 4796, 1843, 304, 4363, 3213, 9534, 4155, 9373, 2233, 5250, 8448, 6761, 6690, 2757, 3065, 6604, 6729, 4391, 554, 8691, 7120, 734, 50, 1649, 7126, 6245, 7275, 2053, 6652, 9475, 9340, 6097, 8305, 2869, 4493, 6508, 2760, 6100, 2384, 6731, 1299, 9579, 7429, 1915, 2667, 1975, 2823, 6357, 1999, 7109, 712, 5148, 5104, 2874, 585, 1768, 7000, 1887, 8186, 2955, 1642, 1638, 1313, 4812, 1172, 5877, 2796, 6095, 3571, 6259, 9106, 6665, 5731, 8627, 3392, 2919, 5888, 4141, 4091, 4111, 812, 272, 2928, 4081, 8135, 7069, 6996, 2635, 1396, 3326, 4625, 3759, 307, 4390, 8700, 436, 9438, 1176, 2196, 7920, 265, 3792, 8379, 7072, 8270, 7561, 7431, 4360, 2396, 4074, 732, 9602, 6630, 2418, 3589, 8696, 8098, 7023, 3256, 7119, 4935, 71, 8784, 7550, 6775, 7032, 1092, 4310, 6272, 9010, 2910, 6137, 4103, 3114, 4204, 3592, 3079, 3723, 3512, 5511, 2639, 3265, 6891, 6072, 2489, 6390, 6747, 948, 5421, 6445, 973, 3465, 1901, 9700, 6732, 6133, 1954, 3575, 6959, 5337, 8588, 1213, 1211, 4816, 8241, 218, 3708, 4500, 3290, 9678, 8116, 6314, 7236, 250, 5292, 4942, 1540, 1222, 5402, 9637, 7193, 8159, 8406, 4108, 5186, 6481, 8515, 2467, 6160, 8622, 7249, 8263, 4483, 6270, 8637, 3804, 7718, 6532, 6244, 604, 4071, 4850, 5799, 8425, 9009, 291, 666, 4602, 4713, 5612, 5159, 1776, 2050, 2058, 9430, 7514, 2603, 8721, 8294, 635, 2915, 1751, 5124, 1793, 8929, 2922, 645, 362, 3173, 5246, 2782, 2719, 1779, 426, 4042, 5279, 4098, 7292, 8804, 8762, 5334, 6513, 8048, 8676, 5008, 7581, 5162, 4462, 4431, 807, 6165, 4900, 6293, 6760, 6194, 9565, 3957, 9236, 8474, 2771, 5431, 7850, 7442, 1523, 3570, 5357, 1684, 8424, 1440, 9173, 271, 1609, 5306, 7017, 3871, 8284, 6290, 9505, 5095, 7081, 748, 4102, 1979, 7252, 5407, 7007, 8445, 5207, 7958, 8977, 6217, 1065, 5618, 4036, 5034, 1285, 9689, 9535, 1470, 9213, 5330, 7729, 5996, 4839, 4465, 9016, 212, 7883, 108, 1192, 9129, 7239, 7051, 5411, 5413, 6907, 3414, 8181, 9370, 3740, 4158, 3617, 5033, 9253, 4247, 7076, 6856, 3191, 8905, 1525, 4901, 6397, 1728, 1259, 461, 1646, 4817, 7415, 3475, 8201, 805, 7554, 8456, 5370, 1430, 6362, 60, 2526, 5991, 532, 1713, 6321, 4669, 3366, 7797, 3914, 3705, 9375, 2550, 9719, 8519, 4975, 4088, 5985, 8567, 6999, 5427, 1064, 9254, 3401, 12, 1327, 3352, 997, 1014, 7968, 2970, 5975, 9226, 9093, 2072, 3555, 7077, 9259, 9220, 9620, 8113, 5599, 5025, 4955, 6827, 4272, 4029, 7334, 6938, 8941, 6718, 2211, 5956, 1109, 2971, 3001, 389, 6382, 5682, 4303, 5738, 2832, 1103, 7471, 4392, 7602, 3994, 8703, 583, 1201, 7204, 9596, 1937, 5217, 5466, 5188, 464, 1958, 3083, 7897, 3458, 9191, 6344, 7773, 1918, 3752, 4811, 220, 6002, 4313, 7149, 1661, 3958, 6, 5398, 5914, 8962, 1497, 4589, 6225, 8517, 1934, 5482, 5007, 5566, 2199, 6060, 3803, 2840, 5973, 2685, 551, 1709, 2538, 9130, 2106, 8370, 618, 7092, 8104, 2031, 1513, 4529, 5423, 2397, 6435, 4453, 6878, 8609, 4978, 5767, 6219, 2989, 7638, 4517, 567, 832, 3307, 8059, 3390, 4112, 1706, 2481, 2575, 3844, 4573, 7414, 9494, 3341, 4143, 3080, 1355, 1524, 7528, 2689, 4173, 4679, 7377, 301, 1601, 2415, 5913, 2587, 8373, 699, 7246, 9585, 3780, 5632, 4893, 4422, 3131, 697, 1946, 9594, 7323, 9468, 4369, 7767, 5762, 1140, 7698, 8540, 558, 4545, 2995, 2056, 2194, 9672, 2972, 5028, 2268, 4643, 373, 6800, 1310, 8405, 1104, 1283, 6403, 1567, 2884, 3546, 8442, 8582, 2304, 755, 5696, 4097, 7833, 9692, 3819, 5925, 4256, 5997, 4093, 2473, 7928, 4880, 5983, 1246, 410, 2617, 1648, 8122, 3956, 5714, 1431, 85, 3271, 9150, 8718, 634, 9162, 2333, 3549, 241, 2083, 7576, 6252, 3371, 2714, 7420, 885, 3411, 5761, 4827, 5530, 7544, 5294, 2949, 4895, 6795, 7884, 7853, 8859, 4650, 6205, 3141, 2287, 9376, 5679, 6741, 2289, 6331, 1715, 4683, 3331, 4771, 2113, 7135, 9151, 6633, 1174, 3522, 569, 5871, 5166, 894, 3653, 552, 8326, 5562, 3952, 9647, 4095, 2625, 8006, 1193, 5355, 94, 3361, 2737, 7293, 6531, 1163, 3355, 4581, 6591, 5098, 7289, 5278, 4637, 3116, 9398, 2360, 7714, 3978, 7079, 3973, 2564, 2068, 380, 9721, 2728, 3410, 3720, 6951, 4671, 1266, 6080, 8666, 7828, 8741, 6759, 8018, 1920, 3525, 2950, 3004, 3867, 1088, 7819, 4980, 4447, 1942, 8925, 9066, 7861, 5903, 7863, 2699, 9621, 6425, 6700, 9214, 9712, 3002, 1382, 5892, 5708, 394, 6726, 7439, 7407, 1816, 8208, 6947, 663, 3935, 7433, 4415, 4788, 5862, 2312, 9668, 4769, 7097, 8950, 4014, 3059, 705, 1438, 6674, 7212, 6675, 3582, 6719, 5235, 7862, 7012, 5193, 37, 285, 8605, 1610, 3303, 4676, 5726, 4044, 5729, 9287, 5117, 3897, 3305, 8548, 9725, 9607, 6196, 5126, 3480, 5841, 5590, 1450, 7529, 56, 3529, 7441, 1334, 4295, 4815, 1993, 3216, 4705, 4838, 1352, 5733, 8873, 9653, 5153, 6315, 7207, 7540, 5050, 4209, 3806, 6616, 7378, 1241, 3284, 8287, 9495, 1069, 4843, 7648, 8563, 6188, 2406, 279, 4048, 4300, 986, 6269, 4652, 4152, 1818, 3664, 1529, 3600, 3788, 6248, 2254, 9389, 5776, 7716, 4085, 5230, 5811, 7134, 7325, 2889, 3704, 7582, 7849, 879, 5721, 4242, 7390, 4043, 1306, 2187, 9080, 4358, 895, 6826, 4951, 9704, 2504, 9307, 4346, 9405, 6897, 9156, 7687, 9429, 8585, 5743, 1218, 7100, 5895, 7947, 9015, 1844, 2790, 5442, 123, 5271, 2424, 8464, 3533, 5, 2485, 6125, 3738, 3449, 7091, 3895, 3767, 5111, 6701, 7518, 7137, 8522, 8494, 2449, 1268, 3162, 9465, 7169, 7240, 1579, 2060, 3103, 5397, 2432, 6695, 5780, 9321, 7570, 9408, 5399, 9580, 8788, 2859, 7063, 6079, 1165, 3462, 8268, 8693, 3790, 5827, 8348, 5882, 3209, 744, 7412, 7122, 3286, 7491, 9136, 2088, 7223, 1033, 9547, 4006, 5557, 8740, 950, 6440, 299, 8109, 589, 453, 6746, 9708, 7136, 3995, 26, 4904, 499, 400, 6119, 7183, 1657, 3178, 9225, 6766, 9418, 9456, 3832, 5639, 2347, 736, 4869, 1542, 8243, 8005, 1346, 1842, 9327, 4944, 4943, 1990, 112, 5288, 2402, 1761, 8917, 3470, 9644, 2339, 4613, 208, 8904, 9271, 4472, 2248, 7688, 867, 2516, 6181, 7756, 4116, 150, 9056, 4513, 4779, 5133, 5637, 1126, 1254, 9101, 6150, 8812, 77, 5403, 7309, 5818, 1423, 1929, 1835, 1938, 8027, 7484, 7922, 2282, 1084, 5671, 3129, 1184, 4494, 236, 4194, 3228, 9387, 4898, 8449, 7071, 4604, 3694, 2560, 8093, 7473, 7595, 9548, 8981, 8747, 4870, 5080, 4934, 7846, 5257, 3763, 5865, 9196, 5740, 3976, 8806, 1289, 7999, 1203, 891, 1262, 7903, 3951, 2908, 7039, 329, 4556, 6032, 111, 7790, 8826, 1633, 9282, 5595, 1110, 5129, 915, 321, 5350, 2044, 1053, 1799, 9117, 8766, 943, 8016, 6109, 8565, 587, 3222, 3938, 7383, 3601, 6519, 2204, 29, 1297, 8993, 3791, 2817, 8229, 3859, 4835, 5621, 5512, 4267, 7592, 325, 6051, 5238, 5314, 5103, 8497, 9204, 3540, 8398, 1840, 5608, 2037, 5779, 7485, 405, 2121, 7052, 6388, 605, 5227, 1455, 8306, 5304, 5795, 2167, 5543, 1742, 3340, 4514, 408, 5225, 5778, 1290, 6203, 6098, 7221, 8498, 4039, 1421, 5322, 4463, 7262, 4539, 6727, 2323, 4905, 4837, 5472, 7690, 1923, 631, 6872, 899, 6256, 5538, 2220, 6451, 3742, 3926, 5102, 6075, 5556, 1970, 8815, 9233, 5904, 4744, 6387, 7800, 7705, 5345, 7661, 7617, 5003, 5266, 2822, 2879, 8733, 1006, 2000, 8602, 4425, 7417, 8227, 4508, 7257, 6178, 3325, 8036, 2937, 6586, 1870, 6172, 8187, 7865, 817, 1866, 2382, 7697, 1625, 5675, 3944, 4335, 7445, 646, 3697, 4719, 7899, 3596, 9676, 2720, 3809, 2238, 4939, 3208, 270, 3724, 3594, 4124, 1214, 9479, 1271, 566, 1859, 9058, 6682, 3309, 4130, 676, 7571, 4181, 9728, 5077, 2722, 1460, 6093, 9457, 6462, 2990, 4251, 6149, 2176, 2696, 2965, 9670, 8014, 5567, 1611, 6389, 3421, 1655, 1520, 2174, 8362, 5192, 4371, 3243, 2568, 3943, 9391, 5630, 2182, 8117, 7837, 8314, 5661, 1258, 7887, 3338, 9227, 5258, 6780, 3029, 5837, 5248, 9363, 8079, 5718, 4859, 143, 148, 8247, 6712, 8194, 1910, 7114, 8134, 7020, 6201, 1451, 5500, 7873, 4099, 533, 1563, 9184, 3193, 5169, 4086, 7400, 679, 6158, 8796, 2354, 9522, 6511, 5405, 5966, 2440, 622, 6431, 6689, 7310, 6366, 7824, 2789, 4146, 7977, 1914, 2309, 3493, 1906, 8634, 4319, 8440, 6892, 162, 6372, 3085, 5901, 6862, 7759, 4455, 2748, 2346, 8583, 2920, 4740, 3039, 5825, 1362, 5878, 6195, 1737, 1420, 934, 9234, 6170, 4753, 1162, 3634, 4396, 3654, 4785, 6054, 9005, 7397, 3526, 2500, 3911, 7470, 9019, 385, 3438, 7923, 6525, 6555, 6763, 8837, 4348, 3210, 1741, 9636, 5158, 1553, 3273, 5406, 7447, 7799, 835, 1105, 2446, 6691, 7670, 7013, 5641, 3378, 8619, 5797, 1653, 228, 8553, 7742, 2903, 2315, 4592, 8179, 6603, 961, 4918, 8538, 9285, 8499, 6582, 5623, 7584, 39, 1076, 2770, 3536, 2326, 7601, 4577, 4334, 7974, 560, 8422, 2723, 3435, 4092, 7551, 1831, 7354, 718, 1182, 2739, 8349, 1048, 3524, 1275, 4452, 1292, 4647, 3385, 1528, 2690, 8408, 888, 7762, 9102, 19, 9521, 194, 7979, 5276, 4667, 41, 4790, 7205, 7148, 6861, 6821, 8580, 62, 6563, 5297, 6441, 8360, 3566, 813, 8692, 3793, 3022, 254, 8147, 9584, 4182, 2393, 3733, 9274, 920, 1448, 80, 3119, 2142, 7982, 8546, 9163, 8707, 8386, 8579, 8838, 9496, 8889, 1944, 2379, 1787, 5607, 3145, 1659, 591, 4892, 5373, 2217, 9251, 8459, 1018, 8612, 6350, 450, 2870, 53, 8182, 8184, 1639, 4616, 6014, 6013, 8329, 4991, 6639, 5291, 8552, 1720, 9560, 1143, 9132, 5452, 125, 5988, 9152, 1539, 2969, 3323, 8989, 1021, 7517, 5333, 4413, 5801, 7983, 5344, 8008, 6174, 1884, 2451, 2514, 1359, 6374, 662, 4979, 3901, 7357, 2403, 511, 35, 6823, 7161, 9097, 7214, 3706, 1370, 7855, 4169, 7771, 4163, 8973, 8031, 5234, 5450, 7156, 4512, 5823, 9588, 557, 8525, 44, 4338, 2491, 6288, 7384, 27, 67, 4766, 1680, 1705, 497, 8978, 6040, 7394, 921, 4582, 6859, 7693, 9684, 6803, 6917, 8664, 3136, 7659, 3226, 4778, 7408, 4047, 1322, 2983, 5285, 1141, 9055, 8984, 7709, 9159, 3919, 2428, 4735, 7665, 1012, 6905, 8936, 474, 1867, 7255, 9077, 2066, 8822, 8695, 1483, 6503, 493, 8959, 9179, 6589, 6816, 8257, 1640, 1848, 5697, 6928, 8381, 4023, 2295, 355, 5963, 5907, 2035, 3250, 2399, 1734, 392, 5049, 7210, 8315, 3045, 104, 1007, 9544, 264, 8708, 4836, 6202, 6335, 3826, 3637, 4218, 5897, 2170, 5000, 7418, 6815, 5254, 8793, 1379, 2305, 1971, 3539, 3920, 6364, 244, 5263, 4433, 1512, 6255, 5685, 3217, 4687, 6370, 8107, 1080, 863, 9268, 7003, 9563, 716, 7307, 8202, 903, 809, 1173, 8235, 6483, 5927, 4207, 4865, 3009, 3764, 6638, 5467, 847, 2599, 4110, 5251, 8764, 8539, 214, 472, 8272, 4519, 6587, 6222, 2665, 3732, 1838, 8723, 8330, 1047, 2891, 7224, 370, 3870, 8374, 3007, 5971, 4298, 9595, 9238, 3730, 7200, 8233, 5693, 5464, 3035, 6280, 5495, 3042, 858, 4373, 9331, 6250, 1311, 3146, 1197, 4179, 6617, 1100, 160, 5303, 9649, 8062, 3349, 853, 4133, 3988, 7159, 9230, 7728, 1886, 3177, 4675, 3731, 3268, 9540, 5774, 4287, 5444, 480, 8288, 356, 7465, 872, 4593, 530, 4791, 4171, 5502, 594, 830, 6052, 3760, 9216, 1926, 6902, 5763, 6813, 8491, 1752, 7024, 7934, 476, 2584, 7965, 2439, 1493, 4001, 607, 1732, 936, 2878, 9332, 6026, 4599, 7347, 5834, 3115, 2059, 2097, 3900, 6287, 445, 2226, 938, 8956, 268, 672, 8295, 4296, 9306, 2880, 5408, 1760, 6405, 4864, 134, 3684, 7623, 7267, 1179, 7753, 2093, 1759, 9614, 199, 2387, 3755, 4916, 3828, 2738, 1877, 8581, 7574, 3822, 8025, 2153, 318, 5684, 639, 6767, 6581, 1676, 1020, 9175, 6886, 7128, 6448, 9072, 4757, 4949, 3695, 6112, 6300, 7593, 2230, 1548, 9067, 3500, 596, 4336, 1336, 6274, 2130, 774, 7944, 7167, 4598, 2947, 6023, 5458, 9339, 3651, 7740, 7836, 2563, 8745, 7555, 8527, 8407, 5542, 4871, 2793, 2388, 113, 5857, 2251, 9290, 1940, 9677, 9090, 5784, 4357, 7573, 9529, 8176, 1700, 7460, 1314, 2168, 9123, 8105, 4187, 1226, 8120, 8710, 2125, 7199, 8633, 6601, 5353, 2414, 4240, 1669, 7006, 2149, 2338, 5885, 1260, 1664, 2269, 196, 3993, 8230, 801, 128, 556, 6768, 9183, 2745, 2465, 8139, 8298, 3591, 7801, 4197, 6663, 2271, 758, 3707, 8890, 8075, 7505, 6888, 3280, 5268, 5769, 9546, 3189, 86, 1707, 5833, 8983, 4183, 238, 7736, 3736, 4409, 5680, 3334, 4429, 7892, 8056, 332, 7042, 8157, 433, 7869, 2498, 6377, 8642, 6061, 2747, 1804, 2435, 4403, 4810, 6393, 7061, 2143, 6931, 9025, 5719, 2758, 5673, 1806, 2797, 4065, 2824, 9576, 2858, 2682, 3054, 6497, 5019, 9706, 7845, 4795, 2676, 151, 163, 3876, 9491, 649, 205, 4364, 6692, 4309, 5969, 43, 3872, 4479, 2701, 3359, 5745, 1893, 4689, 4421, 2018, 4144, 6138, 8429, 9154, 5933, 9656, 6518, 4930, 2046, 4410, 766, 2801, 6240, 1889, 6213, 1056, 1011, 197, 8211, 8748, 5847, 8628, 1868, 6990, 4057, 8346, 9686, 8479, 2362, 3478, 503, 5070, 1169, 5921, 2678, 1803, 8712, 8485, 6909, 6312, 8393, 4468, 8534, 6527, 3638, 79, 8164, 3317, 6453, 6734, 5794, 6486, 4213, 794, 8343, 1117, 9367, 9428, 2156, 9698, 6698, 965, 1568, 7854, 8919, 7743, 7936, 3003, 7735, 7691, 5346, 8952, 178, 7995, 1038, 6487, 5889, 6179, 4623, 72, 7273, 9065, 5105, 1219, 4572, 8803, 8818, 7231, 340, 5669, 4076, 2423, 7364, 6607, 5725, 3739, 1551, 8363, 3686, 2528, 13, 7294, 8029, 5301, 1330, 3240, 3523, 4544, 6101, 6580, 2784, 4257, 3521, 8351, 2653, 6788, 519, 4657, 1888, 2457, 5151, 7703, 7018, 8551, 3289, 4789, 8876, 5416, 4054, 7419, 2117, 6944, 6496, 8290, 733, 5315, 9536, 690, 8054, 7651, 9420, 932, 3663, 4020, 7711, 1994, 2546, 4321, 564, 998, 6808, 235, 6200, 5170, 5781, 1538, 8091, 2988, 9278, 9252, 9266, 5687, 4672, 5358, 8547, 627, 5869, 2412, 7059, 1196, 1991, 1644, 6654, 5052, 1543, 9155, 3153, 8039, 5744, 278, 443, 319, 3702, 3246, 5950, 580, 5754, 6392, 4078, 2559, 7142, 9323, 7111, 669, 317, 3112, 1353, 5551, 9063, 8737, 1339, 7655, 7913, 7405, 2336, 7318, 3765, 3423, 1013, 9564, 5692, 5067, 8953, 7827, 390, 5109, 6491, 4026, 667, 9426, 8884, 8946, 901, 6092, 4041, 1969, 1001, 508, 6276, 8589, 2627, 8994, 5676, 2839, 5951, 8068, 2413, 2067, 9427, 5753, 3885, 8340, 8411, 2040, 2486, 6319, 5934, 9199, 1885, 6528, 6468, 9666, 3766, 1748, 8752, 5021, 3605, 407, 6556, 966, 1168, 8469, 2986, 7870, 1441, 1127, 1183, 1452, 3402, 8342, 7401, 5347, 1505, 6378, 1321, 3849, 2128, 7959, 2579, 3156, 1531, 5570, 5660, 9167, 7626, 3853, 7385, 647, 2644, 5764, 8851, 8972, 1573, 7094, 8839, 3320, 8616, 6963, 7259, 4228, 2141, 6515, 5656, 818, 7525, 8970, 2767, 8210, 2292, 2725, 2688, 6521, 6064, 8488, 7009, 4426, 2372, 6035, 2332, 3485, 3196, 1778, 7908, 3725, 6831, 1381, 2868, 1348, 4032, 4998, 3811, 1256, 2948, 886, 4628, 5239, 955, 1354, 1618, 960, 261, 3953, 9419, 7353, 3105, 980, 4726, 9140, 1974, 3266, 6341, 5320, 7689, 1598, 7182, 2783, 1807, 7263, 750, 5958, 4784, 1342, 5817, 110, 4710, 4907, 6594, 361, 5736, 3642, 398, 2482, 9627, 1191, 1004, 1678, 6130, 3337, 1447, 5469, 4121, 5043, 1150, 4752, 869, 4388, 3671, 4736, 8648, 6539, 7991, 1253, 5336, 2612, 1144, 20, 4754, 3426, 2349, 487, 7567, 6011, 3272, 6916, 5120, 5360, 4931, 3563, 6157, 9435, 2160, 1845, 9630, 5454, 9352, 7358, 4323, 3974, 9054, 1492, 747, 1393, 5707, 5854, 2383, 7160, 617, 6090, 5190, 3942, 2273, 609, 1933, 2643, 8606, 2471, 9026, 8240, 2011, 9006, 5465, 1307, 333, 4046, 762, 6294, 3982, 535, 1189, 1003, 1558, 8702, 4140, 6739, 4709, 4188, 4809, 32, 1437, 3403, 2752, 2101, 3564, 8531, 1587, 1712, 7664, 7206, 1035, 7611, 3278, 8254, 6913, 9298, 5694, 7180, 492, 1155, 8251, 9262, 958, 8467, 727, 5138, 3377, 9447, 4842, 509, 7932, 3172, 6190, 190, 8017, 5728, 2887, 8974, 1925, 368, 22, 8979, 1276, 416, 5307, 8960, 1645, 4332, 875, 4406, 4999, 7726, 1708, 6738, 5893, 6499, 6770, 4954, 9134, 2364, 1909, 7098, 3122, 501, 7287, 1407, 9359, 553, 896, 5978, 2890, 7494, 883, 1058, 3254, 6421, 620, 352, 6643, 9014, 3296, 5232, 4862, 516, 247, 2193, 7115, 8322, 3692, 6284, 9364, 6223, 6307, 5267, 3084, 1575, 4803, 8076, 8829, 7361, 2659, 7290, 723, 7453, 6565, 2765, 2642, 172, 2565, 3109, 5549, 6912, 5493, 7898, 5989, 9007, 2361, 3088, 9322, 9113, 5026, 7154, 3087, 3380, 351, 2407, 3835, 5212, 1907, 5541, 7680, 5849, 8820, 8347, 6620, 6624, 2845, 5555, 7997, 935, 3388, 5999, 6373, 9501, 4902, 988, 5496, 7815, 2735, 2594, 7553, 233, 7930, 701, 6231, 2, 5094, 6041, 3572, 5785, 9073, 4337, 3249, 2263, 8253, 8124, 135, 6895, 9732, 5962, 3166, 3339, 2146, 8549, 7763, 8649, 6135, 6629, 5663, 3669, 2452, 9166, 2503, 8463, 1054, 1216, 5240, 3930, 2099, 1589, 5198, 7238, 5568, 6609, 1133, 2210, 3917, 8285, 6267, 7187, 5107, 3761, 4011, 8800, 5093, 7917, 7501, 5658, 2008, 8434, 814, 3527, 4100, 3678, 6132, 1613, 292, 8166, 8195, 7663, 8222, 3789, 4355, 5688, 5478, 4405, 7809, 8044, 3502, 5061, 7879, 8477, 3625, 1388, 4017, 8511, 897, 226, 905, 3329, 6669, 1858, 6261, 6893, 3292, 9488, 8067, 713, 3618, 8542, 4341, 856, 834, 1392, 7533, 4191, 9587, 1956, 6074, 1765, 3924, 6467, 4965, 9578, 8961, 5116, 4896, 7985, 5705, 8137, 6434, 3237, 4374, 8971, 4952, 6232, 6904, 4147, 9235, 2743, 9048, 8938, 7078, 2914, 3251, 3863, 1554, 8861, 9675, 1094, 217, 7946, 6428, 7536, 4150, 5290, 1722, 2043, 3019, 1860, 5394, 8656, 7610, 8102, 7609, 136, 7233, 8615, 3635, 33, 3396, 3945, 710, 8131, 9242, 7141, 386, 2239, 7250, 8160, 2951, 7152, 2359, 5703, 1841, 5965, 4521, 5076, 8437, 6413, 6191, 859, 1727, 868, 7026, 8988, 2721, 8489, 4851, 8678, 4874, 2259, 8337, 6849, 4229, 6566, 4427, 1503, 3452, 9319, 5325, 381, 545, 2610, 1106, 4986, 6576, 6915, 9310, 2827, 7989, 929, 3537, 7038, 7468, 3395, 1743, 8722, 1418, 140, 4037, 9177, 4024, 9107, 3530, 7838, 8375, 9267, 1153, 5861, 4797, 1090, 517, 4159, 6873, 4474, 6881, 4328, 4933, 5638, 3285, 8430, 3354, 460, 5218, 8220, 6375, 4192, 2308, 7158, 1036, 100, 7526, 9480, 4506, 6458, 5058, 5645, 3963, 118, 4094, 2933, 1120, 4038, 628, 6355, 4959, 6619, 8785, 3985, 5371, 6368, 4397, 1985, 9192, 6127, 3351, 8289, 7699, 2171, 2496, 3836, 7751, 3972, 9062, 8760, 3211, 4792, 3580, 8307, 8073, 4698, 1488, 8169, 6329, 2844, 3620, 7744, 5490, 2524, 6131, 2350, 207, 7411, 2161, 9036, 1028, 8435, 5415, 3598, 3081, 684, 2730, 4167, 5270, 9625, 6493, 455, 5096, 1361, 3279, 5954, 7458, 962, 9687, 3264, 9255, 3037, 4702, 5455, 8909, 4747, 8189, 8597, 6812, 7775, 7456, 4580, 6551, 7162, 1873, 3603, 5905, 2583, 8887, 5835, 2660, 5742, 2206, 9371, 5261, 1235, 2619, 3096, 1252, 7741, 7748, 1532, 4232, 518, 5757, 1851, 6664, 6152, 1425, 1614, 9589, 9185, 4248, 8185, 9434, 5446, 2036, 1972, 6206, 4230, 6598, 7620, 8097, 5335, 887, 376, 7317, 9711, 8667, 9078, 5316, 3947, 4734, 9697, 2261, 3649, 3889, 9453, 614, 6239, 9551, 82, 435, 243, 4186, 5655, 2572, 9311, 471, 122, 9263, 5790, 8611, 3611, 6559, 7572, 7696, 7543, 668, 9030, 38, 2218, 3504, 8256, 7624, 6824, 8410, 3875, 680, 8590, 5722, 3314, 8802, 4308, 7140, 5520, 5771, 7534, 778, 3190, 9360, 2896, 572, 206, 9382, 7462, 9541, 6275, 3713, 5839, 8724, 3639, 7683, 3830, 7251, 4446, 5576, 7746, 2047, 9440, 8350, 3696, 3304, 8577, 1170, 1249, 2320, 527, 7279, 7686, 4728, 6212, 2798, 8744, 9690, 8864, 7874, 1461, 4814, 7247, 6455, 2595, 9462, 5850, 2158, 3491, 6285, 8731, 7201, 5727, 4828, 9461, 8495, 5787, 3744, 5519, 6396, 5015, 6433, 8085, 9121, 6016, 4424, 2029, 3447, 1062, 5453, 1251, 5161, 9245, 9641, 5130, 7539, 4566, 7336, 2411, 7138, 1917, 922, 4805, 7841, 2849, 1467, 3406, 298, 449, 1187, 7314, 704, 9532, 1827, 8942, 396, 2020, 2321, 6615, 9629, 7281, 4956, 810, 9731, 227, 2917, 2039, 3747, 1159, 229, 571, 3061, 923, 5038, 1212, 7857, 5328, 8304, 954, 3358, 783, 1231, 412, 864, 2940, 2547, 4408, 7818, 5489, 5615, 4772, 6279, 9556, 3223, 8392, 1061, 286, 91, 4601, 5385, 1620, 5902, 4407, 2925, 8840, 593, 9181, 367, 7905, 6754, 9669, 4776, 4617, 6953, 8092, 3416, 6361, 5436, 8544, 6207, 5558, 154, 8376, 89, 4685, 771, 3927, 5920, 6979, 4712, 9590, 1874, 8354, 4127, 6048, 5361, 5434, 5211, 1217, 665, 7064, 8452, 5037, 565, 1227, 8743, 4109, 989, 3070, 4075, 1400, 382, 1309, 9550, 6233, 7016, 6297, 8749, 8969, 5890, 8110, 606, 7060, 5156, 3198, 8175, 369, 2070, 8927, 3180, 5277, 6409, 7810, 5642, 3437, 8475, 3445, 9444, 5750, 8345, 8685, 8863, 8896, 9105, 849, 1132, 4491, 8055, 844, 7125, 1600, 2679, 4743, 6304, 619, 2805, 9549, 8807, 8046, 4066, 3903, 5864, 3463, 1135, 9228, 7907, 3399, 9609, 7413, 164, 6044, 9626, 9478, 5145, 5491, 6404, 1577, 4626, 6685, 287, 6671, 1731, 4847, 1580, 6704, 8687, 7636, 4184, 6401, 310, 3160, 5532, 5830, 1500, 1097, 5002, 681, 8834, 2026, 9335, 3861, 3778, 9410, 652, 8593, 6465, 1282, 2262, 5935, 6349, 8226, 6197, 5896, 5425, 7095, 5796, 2663, 2003, 1024, 4458, 3199, 2461, 1650, 7284, 99, 2266, 6764, 6359, 6869, 421, 1510, 9726, 7219, 706, 6437, 1820, 7953, 4190, 1570, 660, 7542, 8916, 181, 4804, 5439, 9531, 8049, 1474, 2968, 7333, 6385, 9116, 6116, 2007, 9608, 1522, 1365, 7507, 3748, 350, 2115, 6657, 7871, 1040, 3785, 4224, 1077, 8170, 1287, 4973, 4156, 3400, 9520, 6994, 4226, 314, 149, 8827, 5031, 5168, 6843, 735, 5180, 8311, 6498, 7585, 2252, 8671, 360, 3495, 2475, 2853, 5376, 6923, 2427, 686, 3046, 8569, 752, 7803, 6333, 9351, 1911, 1710, 8238, 2497, 2549, 6363, 2445, 8735, 5305, 2624, 4654, 2257, 2092, 469, 9135, 4663, 5807, 6978, 2154, 6794, 88, 2052, 2228, 3624, 5309, 9189, 6050, 399, 6758, 529, 2623, 2702, 5980, 2109, 1147, 8595, 3175, 5777, 8443, 7328, 1247, 7011, 1000, 1427, 2848, 5977, 700, 3795, 6059, 5940, 5634, 8158, 2159, 4996, 537, 6107, 4008, 6114, 1499, 9516, 437, 769, 8418, 5177, 6479, 6796, 7472, 4988, 9013, 4356, 7785, 5773, 3133, 5960, 8571, 6241, 7046, 5783, 568, 5204, 4277, 8301, 5142, 7034, 107, 2114, 2764, 6857, 4945, 6590, 2448, 9519, 3646, 7851, 2742, 5174, 9379, 711, 9211, 5042, 7599, 2548, 2476, 4090, 2178, 3167, 7994, 6215, 9574, 4640, 274, 1300, 363, 1113, 2352, 6323, 4571, 780, 6883, 4533, 7896, 4302, 9570, 5359, 2763, 9317, 1607, 4656, 3950, 3721, 8697, 6180, 5241, 3543, 8332, 3492, 4624, 4450, 7952, 6545, 2960, 5173, 3862, 5711, 1936, 4819, 2027, 2522, 6381, 5552, 4558, 9141, 3622, 9559, 2695, 7313, 8420, 6985, 5592, 6564, 7398, 3807, 9695, 2236, 1894, 266, 5497, 2527, 124, 8279, 4214, 6399, 1767, 2648, 4379, 5506, 729, 754, 9330, 6356, 6081, 6164, 9358, 7569, 2501, 3142, 981, 9362, 2155, 5741, 865, 8809, 4925, 6177, 9031, 3192, 7424, 5473, 8064, 8119, 6542, 7050, 914, 4825, 772, 3281, 6877, 2761, 8397, 5369, 3425, 1574, 3214, 8394, 2386, 8631, 8975, 8613, 5384, 8771, 8940, 1204, 4897, 8024, 6156, 6417, 5908, 9605, 2881, 3484, 3726, 3413, 4813, 2799, 78, 7171, 6858, 7041, 3894, 6975, 5620, 4058, 5569, 2010, 3118, 6546, 6348, 1175, 6163, 6175, 2993, 9448, 2098, 502, 1291, 4881, 1814, 1978, 6703, 3712, 5085, 8099, 6572, 9481, 9299, 3407, 2484, 8662, 790, 6868, 4212, 1345, 5515, 3261, 7933, 1898, 9138, 3783, 5587, 7704, 5147, 7105, 2772, 8536, 5189, 1257, 963, 4035, 5014, 5747, 1303, 4920, 1749, 4051, 6484, 9124, 7804, 1703, 9421, 3040, 2973, 9464, 5842, 3498, 9610, 6147, 4077, 3204, 8874, 8814, 6415, 1591, 2921, 1124, 1561, 3980, 2713, 2700, 1255, 58, 1935, 5154, 9597, 7732, 7981, 6583, 4219, 15, 3138, 561, 8114, 9600, 5746, 7919, 7329, 3606, 9143, 9091, 3130, 1250, 5503, 4926, 7814, 2633, 1083, 3983, 6541, 8047, 9215, 6154, 1556, 6419, 970, 4004, 3149, 3682, 2897, 9396, 2876, 5545, 3404, 9517, 9240, 4644, 2264, 6991, 6835, 6282, 2581, 157, 1042, 4575, 3517, 4660, 6523, 4394, 4010, 6853, 8423, 1784, 6221, 1409, 2353, 2521, 1785, 4131, 4367, 8504, 8009, 2656, 7025, 2247, 3771, 1389, 9249, 1138, 6842, 2074, 8248, 1098, 4166, 7269, 6908, 3330, 1399, 8264, 4151, 1813, 534, 7375, 7594, 2615, 5006, 1295, 515, 8779, 7914, 2024, 7660, 3784, 9341, 5513, 4200, 158, 155, 9224, 4113, 2013, 2001, 9658, 3442, 9591, 7537, 4968, 7461, 8847, 1264, 3154, 5284, 1398, 5898, 9511, 4299, 876, 3298, 7184, 3336, 2543, 615, 5404, 3929, 4333, 6266, 7422, 4540, 793, 2420, 5597, 5571, 5079, 8720, 177, 721, 893, 2477, 9473, 6744, 5471, 3071, 9024, 1696, 8433, 4128, 7483, 2736, 862, 5367, 6711, 9592, 2726, 7073, 8926, 161, 7022, 7521, 3949, 4701, 5205, 6968, 7588, 3568, 429, 6117, 8680, 2334, 5311, 857, 5414, 7277, 5723, 5868, 2116, 1967, 7093, 4250, 225, 9318, 4507, 8150, 5137, 579, 7643, 7780, 3628, 3147, 4873, 3852, 8050, 6227, 6173, 2556, 1821, 917, 316, 7700, 1312, 137, 3469, 1301, 8713, 4469, 4135, 9069, 330, 6436, 1002, 5484, 2511, 2954, 10, 8126, 946, 3494, 2777, 1331, 7993, 8575, 9342, 1481, 8877, 7825, 2094, 4770, 1730, 5900, 2590, 9542, 4724, 8037, 9222, 5987, 5340, 9061, 6142, 6962, 168, 9119, 1134, 2405, 5699, 2979, 1855, 8670, 2926, 5626, 8108, 7421, 9070, 3067, 306, 1823, 3453, 7925, 5860, 231, 8655, 1697, 8277, 8862, 7188, 2307, 315, 5432, 9508, 4614, 996, 3424, 6885, 1590, 3668, 2367, 2166, 180, 4970, 6249, 444, 1941, 4402, 7840, 8898, 6735, 6832, 7345, 3551, 6894, 4466, 477, 8421, 6710, 1111, 6427, 6850, 1687, 3092, 7777, 6094, 3121, 2389, 5088, 8910, 6997, 6169, 7363, 3661, 2987, 7131, 6814, 8468, 8915, 1156, 4554, 4574, 2444, 3183, 8244, 6611, 2978, 1564, 722, 4059, 2792, 3258, 2814, 5540, 3864, 2376, 242, 1957, 3923, 5533, 383, 1384, 3960, 1576, 2693, 7226, 2466, 7938, 8543, 4084, 3584, 1729, 96, 3698, 465, 423, 5022, 2834, 8901, 8913, 8924, 4236, 3966, 3476, 4448, 9001, 9515, 4019, 9186, 2014, 9103, 9023, 4266, 7524, 1850, 9655, 3652, 7213, 7299, 8355, 3660, 248, 2260, 7090, 8996, 4961, 9685, 6087, 1692, 1244, 1215, 9736, 4612, 8140, 7547, 559, 485, 5582, 4739, 4562, 624, 7971, 9510, 186, 4763, 3270, 7202, 5598, 7621, 347, 2907, 7482, 1085, 2366, 9598, 342, 4467, 7276, 6507, 21, 3560, 4349, 9304, 8183, 8869, 3633, 5564, 8161, 3048, 3117, 495, 2209, 55, 2621, 1987, 2335, 9361, 3511, 803, 3429, 3710, 6505, 7123, 5523, 6265, 9042, 7895, 3959, 826, 7049, 5579, 9527, 8142, 1308, 7826, 3139, 3008, 7616, 1401, 6420, 9269, 6306, 702, 187, 3287, 5027, 1908, 8239, 8636, 3902, 1631, 7486, 8308, 8358, 4872, 4423, 3477, 7434, 8011, 7489, 6561, 5269, 5233, 4721, 3236, 2941, 7229, 49, 8653, 8912, 8015, 6471, 505, 1198, 1892, 2365, 4596, 4293, 4695, 8250, 8660, 7450, 2401, 4366, 7393, 6522, 2300, 7356, 6264, 626, 3417, 993, 4437, 3350, 6825, 1770, 7619, 2231, 7761, 4314, 1986, 8103, 5689, 6305, 8096, 1745, 9288, 8066, 5155, 8679, 4243, 6852, 671, 1660, 3137, 8361, 359, 6289, 3010, 3743, 2712, 8136, 1320, 4386, 7157, 6936, 3102, 5652, 4161, 5185, 1545, 4885, 9705, 8069, 1671, 9458, 1288, 1081, 703, 6118, 6610, 6752, 4845, 5881, 3714, 6989, 8209, 6298, 8514, 2646, 6971, 92, 8023, 2776, 5319, 6721, 8856, 5089, 6670, 7731, 6751, 4899, 4967, 1635, 5391, 4555, 8206, 7779, 3245, 919, 2882, 1031, 6501, 4107, 3055, 173, 4807, 7842, 574, 773, 5505, 8714, 871, 2096, 542, 8832, 8366, 9673, 5128, 9202, 9011, 5911, 8223, 8545, 1457, 4235, 6896, 6560, 3893, 8682, 4245, 3886, 4233, 510, 2139, 4496, 3913, 5160, 7435, 6286, 7765, 8303, 3434, 595, 4831, 9691, 2578, 2911, 845, 2952, 309, 730, 8296, 5700, 2709, 311, 9294, 2750, 6725, 5735, 2186, 6977, 5247, 384, 1371, 6346, 5264, 2938, 4297, 2842, 4578, 7606, 3825, 8132, 2487, 1501, 5220, 3648, 4118, 6887, 5429, 1943, 6077, 539, 2082, 8610, 2316, 3602, 7789, 9467, 5596, 3106, 1900, 1063, 1719, 5967, 3814, 8805, 2804, 9568, 3812, 7220, 98, 1498, 473, 8447, 2532, 6543, 1152, 7062, 2038, 9229, 2674, 2085, 9557, 1819, 6028, 3578, 5640, 9349, 8556, 964, 4670, 8841, 1585, 2474, 8646, 3922, 6708, 2507, 6096, 2821, 7984, 6748, 775, 2923, 5428, 2756, 590, 4552, 7143, 8789, 8995, 249, 1087, 1691, 3260, 3041, 2375, 7106, 8312, 4730, 3912, 7717, 7146, 8965, 6512, 4661, 3623, 5853, 5836, 377, 1789, 6840, 698, 2299, 8658, 5295, 224, 489, 8630, 2470, 2934, 8359, 3489, 8123, 258, 2327, 1072, 8431, 1546, 7339, 6829, 1683, 6299, 8035, 2781, 7511, 5280, 768, 1372, 6844, 7331, 3274, 6570, 6004, 3553, 3986, 9207, 7208, 2134, 5071, 3412, 8204, 3996, 5710, 4691, 466, 2369, 7829, 8848, 5805, 1966, 5894, 8566, 2152, 9489, 1673, 7395, 8945, 2803, 4531, 8384, 4264, 6068, 7835, 9703, 6661, 2112, 1699, 2342, 6693, 8060, 2492, 5260, 8599, 6986, 6476, 6036, 7796, 6489, 2144, 8021, 5435, 8572, 9153, 8493, 8500, 7322, 3100, 1200, 2515, 9720, 4530, 2016, 1118, 6009, 1224, 1157, 2505, 165, 8255, 2306, 6129, 3587, 3148, 4801, 1863, 9392, 1795, 1686, 334, 428, 8053, 6961, 5046, 8777, 54, 942, 3754, 2495, 3176, 7499, 9170, 2381, 9523, 2021, 3006, 657, 6444, 343, 3690, 7972, 2331, 4395, 7942, 6454, 3483, 8470, 6088, 6818, 8321, 6787, 4884, 984, 2863, 8813, 4387, 7676, 6167, 7369, 7632, 4922, 5400, 6062, 7055, 8990, 7492, 2377, 866, 4265, 7587, 4860, 1578, 2127, 5249, 5629, 8388, 4105, 3531, 6958, 5537, 2249, 6136, 7258, 2180, 484, 7211, 2816, 5265, 9187, 4525, 1471, 4960, 7604, 9583, 7467, 8267, 8003, 6680, 5045, 2593, 7839, 6538, 3387, 2442, 8681, 2555, 2517, 5134, 842, 171, 3715, 8457, 8129, 6918, 6045, 3855, 8341, 1781, 2124, 3441, 2698, 7117, 3507, 8701, 8473, 678, 6466, 4940, 8155, 5739, 1477, 7019, 4651, 8509, 9380, 2240, 9108, 4615, 2006, 708, 95, 693, 7368, 5060, 4758, 4570, 2596, 322, 1808, 4711, 3215, 3025, 7070, 1478, 7722, 956, 1597, 8739, 6509, 5591, 4442, 7734, 6024, 2123, 9633, 2071, 3933, 3869, 9601, 7043, 5584, 4706, 2061, 8557, 2201, 1801, 7608, 8783, 799, 573, 3294, 4832, 599, 5501, 4177, 8759, 2409, 9707, 2330, 7885, 3866, 30, 767, 3749, 4339, 3969, 3451, 7733, 5055, 5880, 1774, 578, 4664, 7481, 1071, 5208, 694, 4497, 8390, 3075, 3456, 6774, 6268, 3418, 6155, 3171, 1879, 2030, 2103, 2324, 7963, 5488, 7374, 8765, 276, 512, 691, 411, 1735, 6706, 1718, 8761, 7951, 7155, 3672, 598, 6946, 257, 6529, 1108, 3321, 1593, 6757, 6696, 5223, 5377, 1161, 324, 4139, 273, 5734, 239, 1417, 959, 9490, 912, 4696, 4648, 2417, 312, 7843, 5995, 1996, 34, 255, 4344, 7770, 9683, 1358, 365, 6838, 2802, 9613, 8177, 4655, 8249, 7500, 9122, 6229, 8403, 1656, 230, 5516, 6637, 9276, 815, 5302, 2089, 3541, 6311, 8333, 3693, 7755, 251, 1665, 2618, 8453, 7144, 216, 6320, 3128, 3056, 9699, 7774, 4993, 1074, 7127, 3202, 8541, 1945, 7918, 2075, 2788, 3964, 7967, 1055, 3858, 6707, 4543, 1230, 3516, 2754, 2281, 3860, 4284, 415, 550, 9198, 4471, 3050, 6658, 4259, 2658, 5381, 925, 9411, 4606, 1414, 8817, 4723, 5924, 785, 625, 1344, 1095, 8943, 6855, 5139, 5231, 3520, 8955, 5756, 4526, 7346, 3756, 4225, 3997, 18, 5840, 7864, 7541, 3120, 3069, 6647, 738, 1833, 1026, 4777, 2683, 2932, 467, 8286, 6578, 6391, 756, 4806, 9369, 7618, 4731, 3497, 6864, 3898, 4542, 8518, 7028, 9157, 8560, 7909, 1517, 2961, 8845, 6353, 7139, 5321, 4725, 7261, 4976, 2935, 2291, 3595, 8266, 4668, 7860, 3808, 4629, 8502, 3776, 358, 5948, 3479, 548, 7964, 784, 8324, 6592, 2628, 5183, 5004, 3880, 4550, 5110, 3608, 7172, 182, 4611, 4741, 8503, 1733, 281, 2019, 1654, 7508, 2553, 1378, 7931, 9528, 3848, 2426, 4330, 9217, 3023, 613, 5296, 7066, 742, 4738, 7147, 7902, 3299, 5872, 75, 3773, 8078, 8258, 5816, 5053, 8234, 5383, 643, 328, 3505, 1963, 4557, 4376, 792, 4534, 3383, 5832, 5380, 7425, 8389, 8382, 4824, 9312, 1605, 6687, 2906, 1017, 6039, 695, 6820, 2163, 2200, 1526, 8236, 8875, 5005, 5943, 1552, 6145, 5724, 1010, 9052, 6424, 2913, 153, 6933, 6980, 1459, 5486, 4684, 3060, 6771, 59, 8852, 1408, 8954, 1846, 3834, 5972, 5955, 8336, 2073, 9075, 8291, 8529, 1202, 6608, 7040, 3936, 7335, 6974, 2122, 3353, 9002, 6457, 2102, 740, 4994, 7116, 7388, 877, 1422, 8690, 295, 8168, 40, 8417, 2091, 3670, 2520, 3840, 4480, 2716, 1206, 7975, 3225, 371, 8935, 7725, 5066, 9180, 8931, 4938, 2395, 6828, 3444, 1663, 9302, 4022, 2542, 2946, 1584, 9395, 1623, 8436, 4704, 3798, 5928, 3259, 4475, 8751, 6226, 409, 5709, 5123, 6889, 4370, 3868, 6981, 3026, 5181, 5023, 3163, 475, 9665, 16, 6071, 9372, 7089, 8858, 7099, 4329, 8318, 8432, 644, 3644, 9449, 6185, 8564, 5087, 4722, 4646, 8395, 9378, 3018, 2192, 6799, 1261, 5926, 3224, 9718, 3699, 4732, 8156, 3877, 4439, 2535, 8481, 2065, 9735, 9273, 2222, 3333, 4680, 2464, 5200, 7197, 1899, 4523, 339, 6134, 7436, 2734, 3239, 9377, 5565, 4160, 1763, 3063, 1569, 1485, 481, 5879, 2666, 5191, 3357, 4782, 9125, 7520, 641, 6438, 6789, 531, 8224, 4787, 985, 7440, 1881, 5531, 1519, 8486, 6376, 1404, 4603, 3787, 5636, 8632, 6418, 7966, 6106, 6514, 709, 1473, 5851, 8574, 3324, 4154, 7916, 5945, 4398, 7504, 2654, 2794, 6140, 1324, 7723, 4282, 7455, 5099, 9118, 5112, 715, 6567, 8191, 5821, 1780, 8061, 3829, 5476, 2807, 7248, 4882, 1112, 654, 4665, 4569, 8716, 6220, 880, 952, 9632, 3468, 6263, 4746, 3104, 7784, 7583, 2939, 5201, 2977, 1351, 3838, 5574, 4252, 2169, 7479, 9538, 9081, 6108, 2867, 6301, 159, 4748, 1630, 1997, 3937, 133, 1413, 7045, 1864, 3590, 1853, 202, 2841, 7245, 8483, 8763, 8592, 193, 2655, 8252, 
Train Sentence: 499
Number of candidates => 1559
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 1559
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:43:03 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:43:03 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:10 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 2152
...+...+...+...+...500
            ...+...+...+...+...1000
            ...+...+...+...+...1500
            ..
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2152 distinct candidates (about 1 per sentence):
newCandidatesAdded[it=1] = 2152 (about 1 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:43:10 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.8175835532538354)

Final lambda[j=1]: {10.591005065658518, 5.6182150357335925E14, 1.1817157487265552E14}
(Final BLEU[j=1]: 0.917784512024492)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:43:10 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.8652989048548032)

Final lambda[j=2]: {-0.017230818545013626, 1.6189630004374225E14, 3.2316213584048047E13}
(Final BLEU[j=2]: 0.9171139996873481)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:43:10 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7915860007944014)

Final lambda[j=3]: {69.45543693640778, 3.720090624058514E15, 7.741306870413572E14}
(Final BLEU[j=3]: 0.917784512024492)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:43:10 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7806761238144728)

Final lambda[j=4]: {92.39779793858236, 4.947503308584682E15, 1.029548565241123E15}
(Final BLEU[j=4]: 0.917784512024492)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:43:10 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.8159776117918609)

Final lambda[j=5]: {81.97431452488448, 4.347350931096166E15, 9.1440645428124E14}
(Final BLEU[j=5]: 0.917784512024492)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:43:10 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.900485032111919)

Final lambda[j=6]: {3.2507817126125354E-15, 0.019144858619011232, 0.6539535776649275}
(Final BLEU[j=6]: 0.9176594477016291)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:43:10 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.8131216615861605)

Final lambda[j=7]: {0.32669861662639876, 1.1669806793063143E13, 3.860648378532335E14}
(Final BLEU[j=7]: 0.9177219862543282)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:43:10 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.900485032111919)

Final lambda[j=8]: {1.8357314168906524E-15, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.9176594477016291)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:43:10 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7995515604825638)

Final lambda[j=9]: {0.7649934440000198, 4.060341122012522E13, 8.540361471105835E12}
(Final BLEU[j=9]: 0.917784512024492)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:43:10 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.8041155598364106)

Final lambda[j=10]: {53.656167887063525, 2.8393175866246175E15, 5.972120002740069E14}
(Final BLEU[j=10]: 0.917784512024492)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:43:10 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.8030153539967888)

Final lambda[j=11]: {84.50164113707345, 4.4834629176348645E15, 9.430356925023218E14}
(Final BLEU[j=11]: 0.917784512024492)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:43:10 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8907425760902816)

Final lambda[j=12]: {0.594593012768021, 2.0950544376244414E13, 7.026399014576682E14}
(Final BLEU[j=12]: 0.9177219862543282)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:43:10 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7915574497435856)

Final lambda[j=13]: {27.978547555528895, 1.4891291157336155E15, 3.1321750447614906E14}
(Final BLEU[j=13]: 0.917784512024492)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:43:10 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.900485032111919)

Final lambda[j=14]: {1.6420626705051878E-16, 0.0010597915722171634, 0.03620028521108054}
(Final BLEU[j=14]: 0.9176594477016291)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:43:10 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.900485032111919)

Final lambda[j=15]: {5.253533502572047E-16, 0.018099402167483188, 0.593645578752771}
(Final BLEU[j=15]: 0.9177219862543282)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:43:11 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.8147902583129618)

Final lambda[j=16]: {69.6887917709289, 3.694598284622095E15, 7.771086897632512E14}
(Final BLEU[j=16]: 0.917784512024492)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:43:11 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7809038269253386)

Final lambda[j=17]: {116.25271193986217, 6.166306099359616E15, 1.2969962831570695E15}
(Final BLEU[j=17]: 0.917784512024492)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:43:11 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7809634857230483)

Final lambda[j=18]: {61.968190122914606, 3.274358301731647E15, 6.887155051975758E14}
(Final BLEU[j=18]: 0.917784512024492)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:43:11 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7923771134032078)

Final lambda[j=19]: {118.50279492915637, 6.31618753555445E15, 1.3143643270901555E15}
(Final BLEU[j=19]: 0.917784512024492)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:43:11 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7806761238144728)

Final lambda[j=20]: {38.909151703479466, 2.0632858985389988E15, 4.339833218988662E14}
(Final BLEU[j=20]: 0.917784512024492)

Best final lambda is lambda[j=1] (BLEU: 0.9178).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:43:11 JST 2015  ---

Next iteration will decode with lambda: {10.591005065658518, 5.6182150357335925E14, 1.1817157487265552E14}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:43:11 JST 2015 ---
Redecoding using weight vector {10.591005065658518, 5.6182150357335925E14, 1.1817157487265552E14}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:11 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+...+...+...+...500
            ...+...+...+...+...1000
            ...+...+...+...+...1500
            ..
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2152 distinct candidates (about 1 per sentence):
newCandidatesAdded[it=1] = 2152 (about 1 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:43:11 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:43:11 JST 2015
----------------------------------------------------

FINAL lambda: {10.591005065658518, 5.6182150357335925E14, 1.1817157487265552E14} (BLEU: 0.917784512024492)

(OP Lamda) : [10.591005065658518,5.6182150357335925E14,1.1817157487265552E14]
Number of candidates => 908
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 908
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:43:13 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:43:13 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:14 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 9061
...+...+...+...+...500
            ...+...+...+...+
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 9061 distinct candidates (about 9 per sentence):
newCandidatesAdded[it=1] = 9061 (about 9 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.7929001202290609)

Final lambda[j=1]: {0.03917462304144103, 0.2, 4.0174684467701195E15}
(Final BLEU[j=1]: 0.8521001614146391)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7609079171540221)

Final lambda[j=2]: {8.413797340751366E-16, -0.6577977590753961, 34.5}
(Final BLEU[j=2]: 0.851894112626097)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7302903353336566)

Final lambda[j=3]: {0.9758505089945568, 0.09518077619965482, 2.9985469623835288E16}
(Final BLEU[j=3]: 0.851894112626097)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7267795517910447)

Final lambda[j=4]: {0.9758505089945568, -0.8743327758402839, 3.9878917843387576E16}
(Final BLEU[j=4]: 0.851894112626097)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.7927351642077628)

Final lambda[j=5]: {0.7737957524668755, 0.6493837878144797, 3.108700019780414E16}
(Final BLEU[j=5]: 0.851894112626097)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8293357179020798)

Final lambda[j=6]: {2.1746284240809505E-18, 0.15636862697411513, 0.06608646261082733}
(Final BLEU[j=6]: 0.8525121968016368)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.7921619116079617)

Final lambda[j=7]: {0.32669861662639876, 0.009245077311689887, 1.3125013839000048E16}
(Final BLEU[j=7]: 0.851894112626097)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8297975976813737)

Final lambda[j=8]: {2.7085292591948385E-17, 0.08804197789590318, 0.03872667200831772}
(Final BLEU[j=8]: 0.8521416947949425)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7279767487078588)

Final lambda[j=9]: {0.4879252544972784, 0.8593774528720801, 1.9028927400307436E16}
(Final BLEU[j=9]: 0.8521001614146391)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.7925218159301731)

Final lambda[j=10]: {0.5053771649119221, -0.5713556223455967, 2.0303368137517856E16}
(Final BLEU[j=10]: 0.851894112626097)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.7925218159301731)

Final lambda[j=11]: {0.31339698433152824, -0.399205792984231, 3.2060308637706256E16}
(Final BLEU[j=11]: 0.8521001614146391)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8275799354996982)

Final lambda[j=12]: {0.594593012768021, -0.2974935383022126, 2.3887586674655068E16}
(Final BLEU[j=12]: 0.851894112626097)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7280921376517732)

Final lambda[j=13]: {0.4879252544972784, 0.981831267676273, 1.2132282287652288E16}
(Final BLEU[j=13]: 0.851894112626097)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8289673248621926)

Final lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 1.5080678945376875E15}
(Final BLEU[j=14]: 0.851894112626097)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8295889429164341)

Final lambda[j=15]: {1.3542646295974193E-17, 0.02541025170060851, 0.01191107887341241}
(Final BLEU[j=15]: 0.8521416947949425)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.7929001202290609)

Final lambda[j=16]: {0.6576106932759427, 0.17534745529356877, 2.6419302104947224E16}
(Final BLEU[j=16]: 0.851894112626097)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7290275433749936)

Final lambda[j=17]: {92.02954151913944, -0.8495287747458757, 3.697257792238633E18}
(Final BLEU[j=17]: 0.851894112626097)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7283150153527672)

Final lambda[j=18]: {49.668027897108786, -0.10291979788243277, 1.99539734889932595E18}
(Final BLEU[j=18]: 0.851894112626097)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7302903353336566)

Final lambda[j=19]: {95.68279865667851, 0.3073471861931083, 3.8440262450186429E18}
(Final BLEU[j=19]: 0.851894112626097)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:43:15 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7267795517910447)

Final lambda[j=20]: {0.4879252544972784, -0.8022503514522665, 1.6810071535587696E16}
(Final BLEU[j=20]: 0.8521001614146391)

Best final lambda is lambda[j=6] (BLEU: 0.8525).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:43:15 JST 2015  ---

Next iteration will decode with lambda: {2.1746284240809505E-18, 0.15636862697411513, 0.06608646261082733}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:43:15 JST 2015 ---
Redecoding using weight vector {2.1746284240809505E-18, 0.15636862697411513, 0.06608646261082733}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:16 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+...+...+...+...500
            ...+...+...+...+
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 9061 distinct candidates (about 9 per sentence):
newCandidatesAdded[it=1] = 9061 (about 9 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:43:17 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:43:17 JST 2015
----------------------------------------------------

FINAL lambda: {2.1746284240809505E-18, 0.15636862697411513, 0.06608646261082733} (BLEU: 0.8525121968016368)

(OP Lamda) : [2.1746284240809505E-18,0.15636862697411513,0.06608646261082733]
Number of candidates => 608
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 608
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:43:17 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:43:17 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:18 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 3143
...+...+...+...+...500
            ...+
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 3143 distinct candidates (about 5 per sentence):
newCandidatesAdded[it=1] = 3143 (about 5 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.7898749141075628)

Final lambda[j=1]: {0.1, 0.2, 4.986415633856591E14}
(Final BLEU[j=1]: 0.8305465080104708)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7522798441851402)

Final lambda[j=2]: {4.557837612444475E-17, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.8305465080104708)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7387774174656553)

Final lambda[j=3]: {2.6274144989465817, 0.09518077619965482, 2.6400554375216304E16}
(Final BLEU[j=3]: 0.8305465080104708)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7387774174656553)

Final lambda[j=4]: {3.61269493605155, -0.8743327758402839, 3.5111190591867604E16}
(Final BLEU[j=4]: 0.8305465080104708)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.788643876211727)

Final lambda[j=5]: {0.2873734608222824, 0.6493837878144797, 3.037883327527041E15}
(Final BLEU[j=5]: 0.8305465080104708)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.812393866451348)

Final lambda[j=6]: {6.380972657422265E-17, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.8305465080104708)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.788643876211727)

Final lambda[j=7]: {0.1334233925246311, 0.009245077311689887, 1.2826023888234912E15}
(Final BLEU[j=7]: 0.8305465080104708)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.812393866451348)

Final lambda[j=8]: {1.0027242747377846E-16, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.8305465080104708)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7385620898084896)

Final lambda[j=9]: {0.02309251024464769, 0.8593774528720801, 2.31049015488678E14}
(Final BLEU[j=9]: 0.8305465080104708)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.788643876211727)

Final lambda[j=10]: {0.5053771649119221, -0.5713556223455967, 1.9840854107874335E15}
(Final BLEU[j=10]: 0.8303820901918246)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.788643876211727)

Final lambda[j=11]: {0.3284268123683227, -0.399205792984231, 3.132996959054887E15}
(Final BLEU[j=11]: 0.8305465080104708)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.812393866451348)

Final lambda[j=12]: {3.1904863287111327E-17, -0.2974935383022126, 0.2090909090909091}
(Final BLEU[j=12]: 0.8305465080104708)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7387774174656553)

Final lambda[j=13]: {1.0673871401970487, 0.981831267676273, 1.0681806296474808E16}
(Final BLEU[j=13]: 0.8305465080104708)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8120365599704426)

Final lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 1.8717890198454638E14}
(Final BLEU[j=14]: 0.8305465080104708)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8136057727338699)

Final lambda[j=15]: {5.925188896177817E-17, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.8305465080104708)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.788643876211727)

Final lambda[j=16]: {0.2668467850492622, 0.17534745529356877, 2.5817466104429205E15}
(Final BLEU[j=16]: 0.8305465080104708)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7387774174656553)

Final lambda[j=17]: {4.269548560788195, -0.8495287747458757, 4.4232086138269376E16}
(Final BLEU[j=17]: 0.8305465080104708)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7387774174656553)

Final lambda[j=18]: {2.6274144989465817, -0.10291979788243277, 2.3487594698678576E16}
(Final BLEU[j=18]: 0.8305465080104708)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7387774174656553)

Final lambda[j=19]: {4.597975373156518, 0.3073471861931083, 4.4824395385274928E16}
(Final BLEU[j=19]: 0.8305465080104708)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:43:18 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7387774174656553)

Final lambda[j=20]: {1.4779206556574522, -0.8022503514522665, 1.4800342072141116E16}
(Final BLEU[j=20]: 0.8305465080104708)

Best final lambda is lambda[j=1] (BLEU: 0.8305).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:43:18 JST 2015  ---

Next iteration will decode with lambda: {0.1, 0.2, 4.986415633856591E14}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:43:18 JST 2015 ---
Redecoding using weight vector {0.1, 0.2, 4.986415633856591E14}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:19 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+...+...+...+...500
            ...+
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 3143 distinct candidates (about 5 per sentence):
newCandidatesAdded[it=1] = 3143 (about 5 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:43:19 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:43:19 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, 0.2, 4.986415633856591E14} (BLEU: 0.8305465080104708)

(OP Lamda) : [0.1,0.2,4.986415633856591E14]
Number of candidates => 417
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 417
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:43:20 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:43:20 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:21 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 3460
...+...+...+...+
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 3460 distinct candidates (about 8 per sentence):
newCandidatesAdded[it=1] = 3460 (about 8 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:43:21 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.7945722532921508)

Final lambda[j=1]: {0.04174359681011487, 0.2, 1.68240344160828E15}
(Final BLEU[j=1]: 0.8216547568467055)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:43:21 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7582229035613055)

Final lambda[j=2]: {9.268940461851754E-18, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.8216547568467055)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:43:21 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7453646207104485)

Final lambda[j=3]: {0.333948774480919, 0.09518077619965482, 2.2181987806957656E16}
(Final BLEU[j=3]: 0.8216547568467055)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:43:21 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7457008357116687)

Final lambda[j=4]: {0.667897548961838, -0.8743327758402839, 2.9500744208906124E16}
(Final BLEU[j=4]: 0.8216547568467055)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:43:21 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.7945722532921508)

Final lambda[j=5]: {0.7737957524668755, 0.6493837878144797, 1.3018366370521396E16}
(Final BLEU[j=5]: 0.8210379973383526)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:43:21 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8101783791743696)

Final lambda[j=6]: {1.8537880923703508E-17, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.8216547568467055)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:43:21 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.7945722532921508)

Final lambda[j=7]: {0.08348719362022974, 0.009245077311689887, 5.496388769809172E15}
(Final BLEU[j=7]: 0.8216547568467055)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:43:21 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8101783791743696)

Final lambda[j=8]: {1.8537880923703508E-17, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.8216547568467055)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:43:22 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7456765776163979)

Final lambda[j=9]: {0.005217949601264359, 0.8593774528720801, 2.6249056528764506E14}
(Final BLEU[j=9]: 0.8216547568467055)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:43:22 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.7945722532921508)

Final lambda[j=10]: {0.1669743872404595, -0.5713556223455967, 8.502482815580526E15}
(Final BLEU[j=10]: 0.8216547568467055)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:43:22 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.7945722532921508)

Final lambda[j=11]: {0.333948774480919, -0.399205792984231, 1.3425960727697792E16}
(Final BLEU[j=11]: 0.8216547568467055)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:43:22 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8101783791743696)

Final lambda[j=12]: {0.1669743872404595, -0.2974935383022126, 1.0003453310371542E16}
(Final BLEU[j=12]: 0.8216547568467055)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:43:22 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7453646207104485)

Final lambda[j=13]: {0.1669743872404595, 0.981831267676273, 8.974951573256364E15}
(Final BLEU[j=13]: 0.8216547568467055)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:43:22 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8101783791743696)

Final lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 1.0124610763826858E15}
(Final BLEU[j=14]: 0.8216547568467055)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:43:22 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8104796618813049)

Final lambda[j=15]: {9.268940461851754E-18, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.8216547568467055)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:43:22 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.7945722532921508)

Final lambda[j=16]: {0.1669743872404595, 0.17534745529356877, 1.1063664936058524E16}
(Final BLEU[j=16]: 0.8216547568467055)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:43:22 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7453646207104485)

Final lambda[j=17]: {0.667897548961838, -0.8495287747458757, 3.7164204260667288E16}
(Final BLEU[j=17]: 0.8216547568467055)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:43:22 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7453646207104485)

Final lambda[j=18]: {0.333948774480919, -0.10291979788243277, 1.9734492382854856E16}
(Final BLEU[j=18]: 0.8216547568467055)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:43:22 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7453646207104485)

Final lambda[j=19]: {0.667897548961838, 0.3073471861931083, 3.7661867919857696E16}
(Final BLEU[j=19]: 0.8216547568467055)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:43:22 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7453646207104485)

Final lambda[j=20]: {0.333948774480919, -0.8022503514522665, 1.243538308768362E16}
(Final BLEU[j=20]: 0.8216547568467055)

Best final lambda is lambda[j=1] (BLEU: 0.8217).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:43:22 JST 2015  ---

Next iteration will decode with lambda: {0.04174359681011487, 0.2, 1.68240344160828E15}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:43:22 JST 2015 ---
Redecoding using weight vector {0.04174359681011487, 0.2, 1.68240344160828E15}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:22 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+...+...+...+
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 3460 distinct candidates (about 8 per sentence):
newCandidatesAdded[it=1] = 3460 (about 8 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:43:23 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:43:23 JST 2015
----------------------------------------------------

FINAL lambda: {0.04174359681011487, 0.2, 1.68240344160828E15} (BLEU: 0.8216547568467055)

(OP Lamda) : [0.04174359681011487,0.2,1.68240344160828E15]
Number of candidates => 290
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 290
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:43:25 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:43:25 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:26 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 7763
...+...+...
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 7763 distinct candidates (about 26 per sentence):
newCandidatesAdded[it=1] = 7763 (about 26 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.7967324500316569)

Final lambda[j=1]: {0.1, 0.2, 3.1928086279947725E15}
(Final BLEU[j=1]: 0.8155810712209217)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7650742780127183)

Final lambda[j=2]: {4.351519665575881E-18, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.8155810712209217)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7582660239360252)

Final lambda[j=3]: {0.1567800187550634, 0.09518077619965482, 2.1492543892804584E16}
(Final BLEU[j=3]: 0.8155810712209217)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7583170323802905)

Final lambda[j=4]: {2.1875480858237246, -0.8743327758402839, 2.8583824195478056E16}
(Final BLEU[j=4]: 0.8155810712209217)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.7967324500316569)

Final lambda[j=5]: {0.7737957524668755, 0.6493837878144797, 2.4705817547819468E16}
(Final BLEU[j=5]: 0.8155810712209217)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8088649162200141)

Final lambda[j=6]: {8.703039331151763E-18, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.8155810712209217)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.7967324500316569)

Final lambda[j=7]: {0.32669861662639876, 0.009245077311689887, 1.0430861619187222E16}
(Final BLEU[j=7]: 0.8155810712209217)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8090162412104719)

Final lambda[j=8]: {8.703039331151763E-18, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.8155810712209217)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7578416791247258)

Final lambda[j=9]: {0.0024496877930478655, 0.8593774528720801, 1.880957133021955E14}
(Final BLEU[j=9]: 0.8155810712209217)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.7967324500316569)

Final lambda[j=10]: {0.5053771649119221, -0.5713556223455967, 1.6135725725223216E16}
(Final BLEU[j=10]: 0.8155810712209217)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.7967324500316569)

Final lambda[j=11]: {0.7980226618452084, -0.399205792984231, 2.547933640074736E16}
(Final BLEU[j=11]: 0.8155810712209217)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8088649162200141)

Final lambda[j=12]: {2.6021424132387722E-17, -0.2974935383022126, 0.3275255626321376}
(Final BLEU[j=12]: 0.8155810712209217)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7581106819549914)

Final lambda[j=13]: {0.0783900093775317, 0.981831267676273, 8.695998857392942E15}
(Final BLEU[j=13]: 0.8155810712209217)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8088649162200141)

Final lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 1.1985090235501115E15}
(Final BLEU[j=14]: 0.8155810712209217)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8090162412104719)

Final lambda[j=15]: {4.351519665575881E-18, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.8155810712209217)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.7967324500316569)

Final lambda[j=16]: {0.6576106932759427, 0.17534745529356877, 2.0996250953530532E16}
(Final BLEU[j=16]: 0.8155810712209217)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7582660239360252)

Final lambda[j=17]: {0.3135600375101268, -0.8495287747458757, 3.6009094327561032E16}
(Final BLEU[j=17]: 0.8155810712209217)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7581106819549914)

Final lambda[j=18]: {0.1567800187550634, -0.10291979788243277, 1.912111968647316E16}
(Final BLEU[j=18]: 0.8155810712209217)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7582660239360252)

Final lambda[j=19]: {0.3135600375101268, 0.3073471861931083, 3.6491289978017952E16}
(Final BLEU[j=19]: 0.8155810712209217)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:43:27 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7583077384747023)

Final lambda[j=20]: {0.9375206082101678, -0.8022503514522665, 1.2048875833934416E16}
(Final BLEU[j=20]: 0.8155810712209217)

Best final lambda is lambda[j=1] (BLEU: 0.8156).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:43:27 JST 2015  ---

Next iteration will decode with lambda: {0.1, 0.2, 3.1928086279947725E15}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:43:27 JST 2015 ---
Redecoding using weight vector {0.1, 0.2, 3.1928086279947725E15}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:28 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+...+...
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 7763 distinct candidates (about 26 per sentence):
newCandidatesAdded[it=1] = 7763 (about 26 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:43:29 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:43:29 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, 0.2, 3.1928086279947725E15} (BLEU: 0.8155810712209217)

(OP Lamda) : [0.1,0.2,3.1928086279947725E15]
Number of candidates => 200
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 200
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:43:30 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:43:30 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:30 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 2425
...+...+
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2425 distinct candidates (about 12 per sentence):
newCandidatesAdded[it=1] = 2425 (about 12 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.8090999742824304)

Final lambda[j=1]: {0.029953311782496144, 0.2, 2.645807784429807E15}
(Final BLEU[j=1]: 0.8222426467553179)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7804203737337145)

Final lambda[j=2]: {6.650971280940642E-18, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.8222426467553179)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.775712194357685)

Final lambda[j=3]: {0.11981324712998458, 0.09518077619965482, 1.0216903759625672E16}
(Final BLEU[j=3]: 0.8222426467553179)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.775712194357685)

Final lambda[j=4]: {0.23962649425996915, -0.8743327758402839, 1.358788342337779E16}
(Final BLEU[j=4]: 0.8222426467553179)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.8090999742824304)

Final lambda[j=5]: {0.23962649425996915, 0.6493837878144797, 2.0473148254355792E16}
(Final BLEU[j=5]: 0.8222426467553179)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8161422086335495)

Final lambda[j=6]: {1.3301942561881283E-17, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.8222426467553179)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.8087323999028982)

Final lambda[j=7]: {0.11981324712998458, 0.009245077311689887, 8.64381743032575E15}
(Final BLEU[j=7]: 0.8222426467553179)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8161422086335495)

Final lambda[j=8]: {1.3301942561881283E-17, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.8222426467553179)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7754552210455176)

Final lambda[j=9]: {0.003060642894670401, 0.8593774528720801, 1.1504495980313205E14}
(Final BLEU[j=9]: 0.8222426467553179)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.8090999742824304)

Final lambda[j=10]: {0.5053771649119221, -0.5713556223455967, 1.3371308369970296E16}
(Final BLEU[j=10]: 0.8215146662590564)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.8090999742824304)

Final lambda[j=11]: {0.4792529885199383, -0.399205792984231, 2.111414570861448E16}
(Final BLEU[j=11]: 0.8222426467553179)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8161422086335495)

Final lambda[j=12]: {1.0873587877819094E-17, -0.2974935383022126, 0.55}
(Final BLEU[j=12]: 0.8222426467553179)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.775712194357685)

Final lambda[j=13]: {0.05990662356499229, 0.981831267676273, 4.133814213102201E15}
(Final BLEU[j=13]: 0.8222426467553179)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8161422086335495)

Final lambda[j=14]: {1.3591984847273867E-18, 0.00790321507749736, 0.096875}
(Final BLEU[j=14]: 0.8222426467553179)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8161422086335495)

Final lambda[j=15]: {6.650971280940642E-18, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.8222426467553179)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.8090999742824304)

Final lambda[j=16]: {0.23962649425996915, 0.17534745529356877, 1.739911491393771E16}
(Final BLEU[j=16]: 0.8222426467553179)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.775712194357685)

Final lambda[j=17]: {0.23962649425996915, -0.8495287747458757, 1.7117631726188644E16}
(Final BLEU[j=17]: 0.8222426467553179)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.775712194357685)

Final lambda[j=18]: {0.11981324712998458, -0.10291979788243277, 9.089600588340944E15}
(Final BLEU[j=18]: 0.8222426467553179)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.775712194357685)

Final lambda[j=19]: {0.23962649425996915, 0.3073471861931083, 1.7346852919296354E16}
(Final BLEU[j=19]: 0.8222426467553179)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:43:31 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.775712194357685)

Final lambda[j=20]: {0.11981324712998458, -0.8022503514522665, 5.72767027583926E15}
(Final BLEU[j=20]: 0.8222426467553179)

Best final lambda is lambda[j=1] (BLEU: 0.8222).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:43:31 JST 2015  ---

Next iteration will decode with lambda: {0.029953311782496144, 0.2, 2.645807784429807E15}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:43:31 JST 2015 ---
Redecoding using weight vector {0.029953311782496144, 0.2, 2.645807784429807E15}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:32 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+...+
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2425 distinct candidates (about 12 per sentence):
newCandidatesAdded[it=1] = 2425 (about 12 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:43:32 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:43:32 JST 2015
----------------------------------------------------

FINAL lambda: {0.029953311782496144, 0.2, 2.645807784429807E15} (BLEU: 0.8222426467553179)

(OP Lamda) : [0.029953311782496144,0.2,2.645807784429807E15]
Number of candidates => 118
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 118
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:43:33 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:43:33 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:33 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 2526
...+
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2526 distinct candidates (about 21 per sentence):
newCandidatesAdded[it=1] = 2526 (about 21 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.8062242211479125)

Final lambda[j=1]: {0.1, 0.2, 2.465525670828773E14}
(Final BLEU[j=1]: 0.8218198832989962)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7869472175294724)

Final lambda[j=2]: {1.0139825472430143E-16, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.8218198832989962)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7840590805771568)

Final lambda[j=3]: {0.9133142843847651, 0.09518077619965482, 5.906658083592908E15}
(Final BLEU[j=3]: 0.8218198832989962)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7840590805771568)

Final lambda[j=4]: {1.3699714265771477, -0.8743327758402839, 7.855509198273298E15}
(Final BLEU[j=4]: 0.8218198832989962)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.8062242211479125)

Final lambda[j=5]: {0.3424928566442869, 0.6493837878144797, 1.719297939268739E15}
(Final BLEU[j=5]: 0.8218198832989962)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8119665608260567)

Final lambda[j=6]: {1.5209738208645214E-16, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.8218198832989962)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.8062242211479125)

Final lambda[j=7]: {0.32669861662639876, 0.009245077311689887, 8.05483825916634E14}
(Final BLEU[j=7]: 0.8218198832989962)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8119665608260567)

Final lambda[j=8]: {2.0279650944860287E-16, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.8218198832989962)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7840590805771568)

Final lambda[j=9]: {0.010702901770133966, 0.8593774528720801, 5.439811671575305E13}
(Final BLEU[j=9]: 0.8218198832989962)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.8062242211479125)

Final lambda[j=10]: {0.22832857109619129, -0.5713556223455967, 1.122898278281439E15}
(Final BLEU[j=10]: 0.8218198832989962)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.8062242211479125)

Final lambda[j=11]: {0.3424928566442869, -0.399205792984231, 1.7731277454368745E15}
(Final BLEU[j=11]: 0.8218198832989962)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8119665608260567)

Final lambda[j=12]: {5.0699127362150717E-17, -0.2974935383022126, 0.3275255626321376}
(Final BLEU[j=12]: 0.8218198832989962)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7840590805771568)

Final lambda[j=13]: {0.45665714219238257, 0.981831267676273, 2.389865629779208E15}
(Final BLEU[j=13]: 0.8218198832989962)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8119665608260567)

Final lambda[j=14]: {6.3373909202688396E-18, 0.00790321507749736, 0.03620028521108054}
(Final BLEU[j=14]: 0.8218198832989962)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8119665608260567)

Final lambda[j=15]: {1.267478184053768E-16, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.8218198832989962)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.8062242211479125)

Final lambda[j=16]: {0.3424928566442869, 0.17534745529356877, 1.461146182550045E15}
(Final BLEU[j=16]: 0.8218198832989962)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7840590805771568)

Final lambda[j=17]: {1.8266285687695303, -0.8495287747458757, 9.896148597093468E15}
(Final BLEU[j=17]: 0.8218198832989962)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7840590805771568)

Final lambda[j=18]: {1.1416428554809566, -0.10291979788243277, 5.254934768390336E15}
(Final BLEU[j=18]: 0.8218198832989962)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7840590805771568)

Final lambda[j=19]: {2.283285710961913, 0.3073471861931083, 1.0028667337120246E16}
(Final BLEU[j=19]: 0.8218198832989962)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:43:34 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7840590805771568)

Final lambda[j=20]: {0.6849857132885738, -0.8022503514522665, 3.3113153192881115E15}
(Final BLEU[j=20]: 0.8218198832989962)

Best final lambda is lambda[j=1] (BLEU: 0.8218).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:43:34 JST 2015  ---

Next iteration will decode with lambda: {0.1, 0.2, 2.465525670828773E14}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:43:34 JST 2015 ---
Redecoding using weight vector {0.1, 0.2, 2.465525670828773E14}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:35 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2526 distinct candidates (about 21 per sentence):
newCandidatesAdded[it=1] = 2526 (about 21 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:43:35 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:43:35 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, 0.2, 2.465525670828773E14} (BLEU: 0.8218198832989962)

(OP Lamda) : [0.1,0.2,2.465525670828773E14]
Number of candidates => 67
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 67
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:43:35 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:43:35 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:36 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 841
..
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 841 distinct candidates (about 12 per sentence):
newCandidatesAdded[it=1] = 841 (about 12 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.7966209299271624)

Final lambda[j=1]: {0.1, 0.2, 1.05939152478795E15}
(Final BLEU[j=1]: 0.8086924321140149)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7752717607089675)

Final lambda[j=2]: {2.3598451955714916E-17, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.8086924321140149)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.772598437712651)

Final lambda[j=3]: {0.05313898971713912, 0.09518077619965482, 1.0649446499130132E15}
(Final BLEU[j=3]: 0.8086924321140149)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.772598437712651)

Final lambda[j=4]: {0.05313898971713912, -0.8743327758402839, 1.4163139925571805E15}
(Final BLEU[j=4]: 0.8086924321140149)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.7966209299271624)

Final lambda[j=5]: {0.2125559588685565, 0.6493837878144797, 4.1025799761120825E15}
(Final BLEU[j=5]: 0.8086924321140149)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8015142236986205)

Final lambda[j=6]: {3.5397677933572374E-17, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.8086924321140149)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.7966209299271624)

Final lambda[j=7]: {0.07970848457570869, 0.009245077311689887, 1.73212013444382E15}
(Final BLEU[j=7]: 0.8086924321140149)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8015142236986205)

Final lambda[j=8]: {4.719690391142983E-17, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.8086924321140149)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.772598437712651)

Final lambda[j=9]: {8.302967143302988E-4, 0.8593774528720801, 1.738410759172555E13}
(Final BLEU[j=9]: 0.8086924321140149)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.7966209299271624)

Final lambda[j=10]: {0.10627797943427825, -0.5713556223455967, 2.6794541460612385E15}
(Final BLEU[j=10]: 0.8086924321140149)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.7966209299271624)

Final lambda[j=11]: {0.2125559588685565, -0.399205792984231, 4.231028385116351E15}
(Final BLEU[j=11]: 0.8086924321140149)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8015142236986205)

Final lambda[j=12]: {1.1799225977857458E-17, -0.2974935383022126, 0.3275255626321376}
(Final BLEU[j=12]: 0.8086924321140149)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.772598437712651)

Final lambda[j=13]: {0.03985424228785434, 0.981831267676273, 8.03698165686346E14}
(Final BLEU[j=13]: 0.8086924321140149)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8015142236986205)

Final lambda[j=14]: {1.4749032472321822E-18, 0.00790321507749736, 0.03620028521108054}
(Final BLEU[j=14]: 0.8086924321140149)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8015142236986205)

Final lambda[j=15]: {2.949806494464364E-17, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.8086924321140149)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.7966209299271624)

Final lambda[j=16]: {0.15941696915141737, 0.17534745529356877, 3.4865795705263425E15}
(Final BLEU[j=16]: 0.8086924321140149)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.772598437712651)

Final lambda[j=17]: {0.07970848457570869, -0.8495287747458757, 1.7842323618651508E15}
(Final BLEU[j=17]: 0.8086924321140149)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.772598437712651)

Final lambda[j=18]: {0.03985424228785434, -0.10291979788243277, 9.474417831605884E14}
(Final BLEU[j=18]: 0.8086924321140149)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.772598437712651)

Final lambda[j=19]: {0.07970848457570869, 0.3073471861931083, 1.8081249117990535E15}
(Final BLEU[j=19]: 0.8086924321140149)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:43:36 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.772598437712651)

Final lambda[j=20]: {0.05313898971713912, -0.8022503514522665, 1.113576434992632E15}
(Final BLEU[j=20]: 0.8086924321140149)

Best final lambda is lambda[j=1] (BLEU: 0.8087).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:43:36 JST 2015  ---

Next iteration will decode with lambda: {0.1, 0.2, 1.05939152478795E15}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:43:36 JST 2015 ---
Redecoding using weight vector {0.1, 0.2, 1.05939152478795E15}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:36 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ..
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 841 distinct candidates (about 12 per sentence):
newCandidatesAdded[it=1] = 841 (about 12 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:43:36 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:43:36 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, 0.2, 1.05939152478795E15} (BLEU: 0.8086924321140149)

(OP Lamda) : [0.1,0.2,1.05939152478795E15]
Number of candidates => 28
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 28
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:43:37 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:43:37 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:37 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 361
.
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 361 distinct candidates (about 12 per sentence):
newCandidatesAdded[it=1] = 361 (about 12 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.8263398240617337)

Final lambda[j=1]: {0.025569178670738386, 0.2, 1.100941062470148E15}
(Final BLEU[j=1]: 0.8325813079265812)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7993260668478418)

Final lambda[j=2]: {1.1354996352403284E-17, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.8325813079265812)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7985211695526536)

Final lambda[j=3]: {0.05000001138312707, 0.09518077619965482, 1.1008372400873685E15}
(Final BLEU[j=3]: 0.8325813079265812)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7985211695526536)

Final lambda[j=4]: {0.050000122372703945, -0.8743327758402839, 1.100839683716002E15}
(Final BLEU[j=4]: 0.8325813079265812)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.8263398240617337)

Final lambda[j=5]: {0.2045534293659071, 0.6493837878144797, 8.519035178557693E15}
(Final BLEU[j=5]: 0.8325813079265812)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8271343681860972)

Final lambda[j=6]: {2.270999270480657E-17, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.8325813079265812)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.8263398240617337)

Final lambda[j=7]: {0.05113835734147677, 0.009245077311689887, 3.596759220961949E15}
(Final BLEU[j=7]: 0.8325813079265812)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8271343681860972)

Final lambda[j=8]: {2.270999270480657E-17, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.8325813079265812)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7985211695526536)

Final lambda[j=9]: {0.05000000273178411, 0.8593774528720801, 1.1008370496130015E15}
(Final BLEU[j=9]: 0.8325813079265812)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.8263398240617337)

Final lambda[j=10]: {0.1534150720244303, -0.5713556223455967, 5.563904728862825E15}
(Final BLEU[j=10]: 0.8325813079265812)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.8263398240617337)

Final lambda[j=11]: {0.2045534293659071, -0.399205792984231, 8.785759172071194E15}
(Final BLEU[j=11]: 0.8325813079265812)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8271343681860972)

Final lambda[j=12]: {5.677498176201642E-18, -0.2974935383022126, 0.3275255626321376}
(Final BLEU[j=12]: 0.8325813079265812)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7985211695526536)

Final lambda[j=13]: {0.05000015508755608, 0.981831267676273, 1.1008404039903885E15}
(Final BLEU[j=13]: 0.8325813079265812)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8271343681860972)

Final lambda[j=14]: {7.096872720252053E-19, 0.00790321507749736, 0.03620028521108054}
(Final BLEU[j=14]: 0.8325813079265812)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8271343681860972)

Final lambda[j=15]: {1.1354996352403284E-17, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.8325813079265812)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.8263398240617337)

Final lambda[j=16]: {0.2045534293659071, 0.17534745529356877, 7.239906153469467E15}
(Final BLEU[j=16]: 0.8325813079265812)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7985211695526536)

Final lambda[j=17]: {0.05000007588167779, -0.8495287747458757, 1.1008386601351765E15}
(Final BLEU[j=17]: 0.8325813079265812)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7985211695526536)

Final lambda[j=18]: {0.05000016663833114, -0.10291979788243277, 1.1008406583007975E15}
(Final BLEU[j=18]: 0.8325813079265812)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7985211695526536)

Final lambda[j=19]: {0.05000001579990522, 0.3073471861931083, 1.1008373373304238E15}
(Final BLEU[j=19]: 0.8325813079265812)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:43:37 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7985211695526536)

Final lambda[j=20]: {0.05000006479397755, -0.8022503514522665, 1.1008384160201652E15}
(Final BLEU[j=20]: 0.8325813079265812)

Best final lambda is lambda[j=1] (BLEU: 0.8326).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:43:37 JST 2015  ---

Next iteration will decode with lambda: {0.025569178670738386, 0.2, 1.100941062470148E15}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:43:37 JST 2015 ---
Redecoding using weight vector {0.025569178670738386, 0.2, 1.100941062470148E15}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:37 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: .
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 361 distinct candidates (about 12 per sentence):
newCandidatesAdded[it=1] = 361 (about 12 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:43:37 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:43:37 JST 2015
----------------------------------------------------

FINAL lambda: {0.025569178670738386, 0.2, 1.100941062470148E15} (BLEU: 0.8325813079265812)

(OP Lamda) : [0.025569178670738386,0.2,1.100941062470148E15]
Number of candidates => 15
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 15
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:43:38 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:43:38 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:38 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 74

Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 74 distinct candidates (about 4 per sentence):
newCandidatesAdded[it=1] = 74 (about 4 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.8528199621360041)

Final lambda[j=1]: {0.1, 0.2, 0.05}
(Final BLEU[j=1]: 0.8596924462223581)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.8239585341898005)

Final lambda[j=2]: {0.05, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.8596924462223581)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.8224985050018094)

Final lambda[j=3]: {0.05005805449798609, 0.09518077619965482, 0.05}
(Final BLEU[j=3]: 0.8596924462223581)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.8224985050018094)

Final lambda[j=4]: {0.05062410670187971, -0.8743327758402839, 0.05}
(Final BLEU[j=4]: 0.8596924462223581)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.8528199621360041)

Final lambda[j=5]: {0.7737957524668755, 0.6493837878144797, 0.05}
(Final BLEU[j=5]: 0.8596924462223581)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8596924462223581)

Final lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.8596924462223581)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.8528199621360041)

Final lambda[j=7]: {0.32669861662639876, 0.009245077311689887, 0.05}
(Final BLEU[j=7]: 0.8596924462223581)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8596924462223581)

Final lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.8596924462223581)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.822070677286207)

Final lambda[j=9]: {0.05001393223093356, 0.8593774528720801, 0.05}
(Final BLEU[j=9]: 0.8596924462223581)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.8528199621360041)

Final lambda[j=10]: {0.5053771649119221, -0.5713556223455967, 0.05}
(Final BLEU[j=10]: 0.8596924462223581)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.8528199621360041)

Final lambda[j=11]: {0.7980226618452084, -0.399205792984231, 0.05}
(Final BLEU[j=11]: 0.8596924462223581)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8596924462223581)

Final lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Final BLEU[j=12]: 0.8596924462223581)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.8224985050018094)

Final lambda[j=13]: {0.05079095402821021, 0.981831267676273, 0.05}
(Final BLEU[j=13]: 0.8596924462223581)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8596924462223581)

Final lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Final BLEU[j=14]: 0.8596924462223581)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8596924462223581)

Final lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.8596924462223581)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.8528199621360041)

Final lambda[j=16]: {0.6576106932759427, 0.17534745529356877, 0.05}
(Final BLEU[j=16]: 0.8596924462223581)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.8224985050018094)

Final lambda[j=17]: {0.050387000222564524, -0.8495287747458757, 0.05}
(Final BLEU[j=17]: 0.8596924462223581)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.8224985050018094)

Final lambda[j=18]: {0.050849863539049744, -0.10291979788243277, 0.05}
(Final BLEU[j=18]: 0.8596924462223581)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.8224985050018094)

Final lambda[j=19]: {0.05008058027990493, 0.3073471861931083, 0.05}
(Final BLEU[j=19]: 0.8596924462223581)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.8224985050018094)

Final lambda[j=20]: {0.05033045241568346, -0.8022503514522665, 0.05}
(Final BLEU[j=20]: 0.8596924462223581)

Best final lambda is lambda[j=1] (BLEU: 0.8597).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:43:38 JST 2015  ---

Next iteration will decode with lambda: {0.1, 0.2, 0.05}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:43:38 JST 2015 ---
Redecoding using weight vector {0.1, 0.2, 0.05}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:38 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: 
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 74 distinct candidates (about 4 per sentence):
newCandidatesAdded[it=1] = 74 (about 4 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:43:38 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:43:38 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, 0.2, 0.05} (BLEU: 0.8596924462223581)

(OP Lamda) : [0.1,0.2,0.05]
Number of candidates => 8
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 8
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:43:38 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:43:38 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:38 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 52

Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 52 distinct candidates (about 6 per sentence):
newCandidatesAdded[it=1] = 52 (about 6 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.8700377656474493)

Final lambda[j=1]: {0.1, 0.2, -0.1}
(Final BLEU[j=1]: 0.8700377656474493)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.8280055776114343)

Final lambda[j=2]: {0.05, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.8700377656474493)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.8280055776114343)

Final lambda[j=3]: {0.050000000067827094, 0.09518077619965482, -0.059473491599185246}
(Final BLEU[j=3]: 0.8700377656474493)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.8280055776114343)

Final lambda[j=4]: {0.05000000072916558, -0.8743327758402839, -0.6393613928095458}
(Final BLEU[j=4]: 0.8700377656474493)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.8700377656474493)

Final lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Final BLEU[j=5]: 0.8700377656474493)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8700377656474493)

Final lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.8700377656474493)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.8700377656474493)

Final lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Final BLEU[j=7]: 0.8700377656474493)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8700377656474493)

Final lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.8700377656474493)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.8280055776114343)

Final lambda[j=9]: {0.05000000001627751, 0.8593774528720801, -0.01427276865927607}
(Final BLEU[j=9]: 0.8700377656474493)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.8700377656474493)

Final lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Final BLEU[j=10]: 0.8700377656474493)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.8700377656474493)

Final lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Final BLEU[j=11]: 0.8700377656474493)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8700377656474493)

Final lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Final BLEU[j=12]: 0.8700377656474493)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.8280055776114343)

Final lambda[j=13]: {0.05000000092409912, 0.981831267676273, -0.8102868749874037}
(Final BLEU[j=13]: 0.8700377656474493)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8700377656474493)

Final lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Final BLEU[j=14]: 0.8700377656474493)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8700377656474493)

Final lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.8700377656474493)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.8700377656474493)

Final lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Final BLEU[j=16]: 0.8700377656474493)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.8280055776114343)

Final lambda[j=17]: {0.05000000045214581, -0.8495287747458757, -0.3964594524802063}
(Final BLEU[j=17]: 0.8700377656474493)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.8280055776114343)

Final lambda[j=18]: {0.050000000992925134, -0.10291979788243277, -0.8706362780408492}
(Final BLEU[j=18]: 0.8700377656474493)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.8280055776114343)

Final lambda[j=19]: {0.05000000009414475, 0.3073471861931083, -0.08254985860243691}
(Final BLEU[j=19]: 0.8700377656474493)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.8280055776114343)

Final lambda[j=20]: {0.05000000038607903, -0.8022503514522665, -0.33852947919372234}
(Final BLEU[j=20]: 0.8700377656474493)

Best final lambda is lambda[j=1] (BLEU: 0.8700).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:43:38 JST 2015  ---

No parameter value changed in this iteration; exiting Z-MERT.


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:43:38 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, 0.2, -0.1} (BLEU: 0.8700377656474493)

(OP Lamda) : [0.1,0.2,-0.1]
Number of candidates => 3
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 3
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:43:38 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:43:38 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:38 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 4

Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 4 distinct candidates (about 1 per sentence):
newCandidatesAdded[it=1] = 4 (about 1 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.9037664163933351)

Final lambda[j=1]: {0.1, 0.2, -0.1}
(Final BLEU[j=1]: 0.9037664163933351)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.8830710861889355)

Final lambda[j=2]: {0.04999999940497489, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.9037664163933351)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.8830710861889355)

Final lambda[j=3]: {0.05000000007690407, 0.09518077619965482, -0.059473491599185246}
(Final BLEU[j=3]: 0.9037664163933351)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.8830710861889355)

Final lambda[j=4]: {0.05000000082674634, -0.8743327758402839, -0.6393613928095458}
(Final BLEU[j=4]: 0.9037664163933351)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.9037664163933351)

Final lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Final BLEU[j=5]: 0.9037664163933351)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.9037664163933351)

Final lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.9037664163933351)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.9037664163933351)

Final lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Final BLEU[j=7]: 0.9037664163933351)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.9037664163933351)

Final lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.9037664163933351)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.8830710861889355)

Final lambda[j=9]: {0.05000000001845585, 0.8593774528720801, -0.01427276865927607}
(Final BLEU[j=9]: 0.9037664163933351)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.9037664163933351)

Final lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Final BLEU[j=10]: 0.9037664163933351)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.9037664163933351)

Final lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Final BLEU[j=11]: 0.9037664163933351)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.9037664163933351)

Final lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Final BLEU[j=12]: 0.9037664163933351)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.8830710861889355)

Final lambda[j=13]: {0.050000001047766876, 0.981831267676273, -0.8102868749874037}
(Final BLEU[j=13]: 0.9037664163933351)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.9037664163933351)

Final lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Final BLEU[j=14]: 0.9037664163933351)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.9037664163933351)

Final lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.9037664163933351)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.9037664163933351)

Final lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Final BLEU[j=16]: 0.9037664163933351)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.8830710861889355)

Final lambda[j=17]: {0.05000000051265434, -0.8495287747458757, -0.3964594524802063}
(Final BLEU[j=17]: 0.9037664163933351)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.8830710861889355)

Final lambda[j=18]: {0.05000000112580355, -0.10291979788243277, -0.8706362780408492}
(Final BLEU[j=18]: 0.9037664163933351)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.8830710861889355)

Final lambda[j=19]: {0.050000000106743686, 0.3073471861931083, -0.08254985860243691}
(Final BLEU[j=19]: 0.9037664163933351)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:43:38 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.8830710861889355)

Final lambda[j=20]: {0.05000000043774616, -0.8022503514522665, -0.33852947919372234}
(Final BLEU[j=20]: 0.9037664163933351)

Best final lambda is lambda[j=1] (BLEU: 0.9038).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:43:38 JST 2015  ---

No parameter value changed in this iteration; exiting Z-MERT.


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:43:38 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, 0.2, -0.1} (BLEU: 0.9037664163933351)

(OP Lamda) : [0.1,0.2,-0.1]
Number of candidates => 1
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 1
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:43:38 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:43:38 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:43:39 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 2

Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2 distinct candidates (about 2 per sentence):
newCandidatesAdded[it=1] = 2 (about 2 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.9426092134060646)

Final lambda[j=1]: {0.1, 0.2, -0.1}
(Final BLEU[j=1]: 0.9426092134060646)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.8916809066702428)

Final lambda[j=2]: {0.04999997247791982, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.9426092134060646)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.8916809066702428)

Final lambda[j=3]: {0.05000000355709325, 0.09518077619965482, -0.059473491599185246}
(Final BLEU[j=3]: 0.9426092134060646)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.8916809066702428)

Final lambda[j=4]: {0.0500000382400298, -0.8743327758402839, -0.6393613928095458}
(Final BLEU[j=4]: 0.9426092134060646)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.9426092134060646)

Final lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Final BLEU[j=5]: 0.9426092134060646)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.9426092134060646)

Final lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.9426092134060646)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.9426092134060646)

Final lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Final BLEU[j=7]: 0.9426092134060646)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.9426092134060646)

Final lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.9426092134060646)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.8916809066702428)

Final lambda[j=9]: {0.050000000853650375, 0.8593774528720801, -0.01427276865927607}
(Final BLEU[j=9]: 0.9426092134060646)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.9426092134060646)

Final lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Final BLEU[j=10]: 0.9426092134060646)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.9426092134060646)

Final lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Final BLEU[j=11]: 0.9426092134060646)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.9426092134060646)

Final lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Final BLEU[j=12]: 0.9426092134060646)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.8916809066702428)

Final lambda[j=13]: {0.05000004846303607, 0.981831267676273, -0.8102868749874037}
(Final BLEU[j=13]: 0.9426092134060646)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.9426092134060646)

Final lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Final BLEU[j=14]: 0.9426092134060646)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.9426092134060646)

Final lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.9426092134060646)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.9426092134060646)

Final lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Final BLEU[j=16]: 0.9426092134060646)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.8916809066702428)

Final lambda[j=17]: {0.050000023712131275, -0.8495287747458757, -0.3964594524802063}
(Final BLEU[j=17]: 0.9426092134060646)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.8916809066702428)

Final lambda[j=18]: {0.05000005207251733, -0.10291979788243277, -0.8706362780408492}
(Final BLEU[j=18]: 0.9426092134060646)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.8916809066702428)

Final lambda[j=19]: {0.05000000493728443, 0.3073471861931083, -0.08254985860243691}
(Final BLEU[j=19]: 0.9426092134060646)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:43:39 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.8916809066702428)

Final lambda[j=20]: {0.050000020247355434, -0.8022503514522665, -0.33852947919372234}
(Final BLEU[j=20]: 0.9426092134060646)

Best final lambda is lambda[j=1] (BLEU: 0.9426).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:43:39 JST 2015  ---

No parameter value changed in this iteration; exiting Z-MERT.


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:43:39 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, 0.2, -0.1} (BLEU: 0.9426092134060646)

(OP Lamda) : [0.1,0.2,-0.1]
Number of candidates => 0
Processing 526 sentences...

Corpus level score:
 See data/results/bleu_results.txt
Test Cumulative: 113809
Making Dependency Statistics ....
730818
0
168292
corpara: 0
next => 
1446, 2274, 6645, 5375, 6395, 9606, 191, 6358, 3123, 6822, 9142, 6235, 2582, 2441, 9572, 9219, 874, 4841, 5372, 3909, 3267, 7297, 240, 9571, 9212, 4126, 6340, 9368, 9710, 3373, 3347, 3873, 4237, 5611, 5547, 344, 1343, 4412, 5069, 3316, 7080, 7875, 3389, 4361, 7682, 4833, 4063, 947, 2866, 5772, 6442, 7768, 2344, 4928, 2493, 117, 4674, 66, 5119, 5409, 5447, 5695, 7577, 9416, 6446, 5229, 6593, 454, 5982, 3906, 1337, 76, 7600, 8260, 7403, 4751, 7341, 6210, 9027, 908, 5030, 7559, 156, 4821, 3816, 4867, 3615, 8450, 1995, 7192, 9631, 9412, 5550, 397, 5613, 5575, 6302, 6621, 968, 5048, 7084, 6146, 6322, 3667, 1171, 6003, 8001, 4049, 8882, 2317, 1119, 1566, 308, 651, 3928, 2488, 2279, 284, 6143, 5529, 1494, 5012, 5844, 9008, 1988, 5242, 6623, 3534, 3528, 7086, 4585, 374, 5479, 6084, 1, 504, 3676, 2499, 860, 2430, 8617, 4981, 192, 9659, 7738, 8171, 7083, 8461, 5244, 4449, 8903, 2502, 1277, 3235, 2825, 4289, 1616, 3073, 2780, 234, 2778, 576, 8798, 1714, 7268, 7713, 64, 2985, 5768, 183, 1210, 1679, 6730, 6103, 7891, 586, 7557, 4858, 650, 4510, 5020, 4123, 6681, 7973, 926, 1377, 1067, 653, 237, 1151, 2833, 819, 3436, 4720, 4848, 7754, 696, 7678, 9244, 2288, 8639, 7195, 9539, 3717, 9208, 6678, 3159, 737, 8725, 2544, 8302, 9533, 1245, 5009, 8214, 4281, 2531, 2311, 2459, 4268, 8115, 3098, 5760, 5521, 9232, 507, 2319, 9394, 3363, 6875, 203, 9201, 4786, 5993, 7692, 1604, 3127, 4378, 6506, 3277, 6141, 890, 2119, 3311, 1766, 2808, 6183, 5560, 8799, 2135, 6006, 6253, 8045, 3513, 7685, 5698, 7342, 743, 6474, 120, 1744, 2994, 8792, 2283, 7035, 2638, 1950, 9239, 6804, 1782, 6606, 5814, 2173, 540, 6614, 4958, 8216, 6070, 982, 2519, 846, 2873, 2293, 5870, 5974, 5326, 7198, 2179, 3446, 3016, 6846, 430, 6854, 8650, 1091, 5443, 8596, 4210, 2551, 296, 7477, 6632, 6204, 2147, 2640, 185, 751, 4909, 7490, 6954, 971, 9041, 1293, 3586, 2410, 1544, 6477, 5604, 8601, 5681, 5144, 4584, 3821, 1364, 843, 1027, 3888, 9679, 2740, 8755, 4083, 9513, 7176, 9165, 4261, 9096, 1439, 8843, 1349, 4114, 5819, 4600, 2818, 6012, 9064, 5461, 3910, 8614, 4470, 4798, 9409, 8451, 1465, 2795, 6027, 7772, 3567, 9044, 7628, 8203, 7757, 7701, 2303, 2087, 776, 2637, 3474, 4216, 3372, 7087, 9099, 3823, 8419, 4522, 8356, 5221, 2991, 1534, 4189, 4697, 1154, 4620, 420, 7133, 1562, 4317, 7911, 8663, 8387, 5941, 2697, 6007, 6736, 5485, 806, 2533, 1693, 4030, 7806, 2202, 7457, 1229, 2131, 6930, 5115, 9308, 1849, 3064, 6577, 4492, 1462, 7502, 7430, 213, 5782, 1445, 5498, 1815, 4353, 2063, 3629, 2205, 6950, 4762, 5487, 7352, 3581, 3397, 468, 9126, 6162, 1595, 1725, 63, 1514, 6123, 1082, 3181, 6921, 7752, 1836, 7580, 5593, 7474, 2229, 1466, 8683, 4389, 1444, 2647, 7337, 3058, 9139, 4499, 6932, 5526, 8824, 5101, 5395, 5253, 3487, 6310, 5867, 7721, 8626, 1739, 2539, 4419, 1432, 1086, 9713, 7469, 1034, 976, 3650, 8002, 2057, 2759, 8283, 9037, 3308, 3370, 3248, 1919, 8513, 3991, 513, 7749, 4919, 3194, 6649, 141, 4290, 4278, 4662, 2049, 2297, 9354, 4142, 1777, 6960, 300, 6983, 8999, 8928, 7712, 4708, 7304, 8165, 5175, 623, 9645, 6602, 9383, 861, 7108, 1199, 9446, 7410, 5628, 8674, 7793, 348, 4509, 8038, 3034, 366, 7463, 3499, 994, 9085, 2183, 3559, 8920, 4913, 3005, 3892, 5846, 9390, 45, 4903, 8550, 3769, 4443, 5580, 121, 8071, 7306, 6193, 9357, 5751, 336, 577, 8521, 5657, 7674, 4220, 5678, 5536, 4060, 8794, 7285, 7848, 1335, 1341, 3532, 4478, 8246, 7270, 8618, 1387, 8652, 5075, 7948, 7889, 7512, 2851, 852, 2245, 1406, 7301, 987, 2284, 4729, 5451, 4249, 1572, 1472, 7305, 174, 7586, 1762, 8130, 2936, 4444, 9433, 353, 759, 1924, 4682, 2620, 1856, 5389, 6025, 7739, 7812, 5460, 2108, 4040, 6790, 8148, 2854, 4551, 2751, 544, 1670, 4609, 8516, 84, 8742, 2371, 4887, 7380, 608, 3068, 2120, 2775, 4316, 7900, 1998, 5672, 7340, 3701, 354, 4653, 7438, 2586, 2017, 7027, 457, 3687, 1949, 2762, 4271, 1615, 5494, 8808, 8385, 2314, 5332, 616, 6807, 1242, 3238, 7867, 9301, 3182, 1775, 3750, 6972, 2398, 2133, 4495, 3673, 7466, 4511, 378, 8265, 5873, 2436, 5706, 1390, 5923, 9343, 4983, 3683, 3346, 745, 4002, 4846, 8095, 9292, 3450, 1369, 1555, 795, 4688, 3464, 7881, 3072, 7185, 4012, 8661, 5073, 6612, 6723, 2241, 9020, 9168, 5866, 1927, 6408, 3431, 5152, 2684, 1023, 5135, 3356, 6334, 6722, 4678, 2453, 6802, 9567, 4070, 8089, 478, 8769, 3753, 3157, 1265, 2322, 3552, 8624, 8409, 9164, 3124, 1658, 1951, 6246, 8561, 4148, 2567, 8000, 7649, 4080, 5770, 61, 3327, 201, 1876, 4416, 2614, 2529, 4498, 4345, 2357, 9733, 1411, 4773, 7890, 8344, 5627, 401, 1046, 1060, 1507, 2540, 9470, 4067, 9176, 4451, 2929, 6927, 6880, 5922, 4202, 1279, 4068, 9460, 1237, 5324, 313, 536, 8065, 7834, 1323, 5214, 6783, 6533, 2964, 8454, 8487, 2374, 8242, 7464, 5912, 7961, 5011, 7552, 6242, 6047, 1518, 9650, 7598, 260, 1374, 6964, 2385, 9634, 5083, 2250, 3057, 7254, 8782, 8811, 3916, 3319, 4488, 8948, 8562, 7170, 6845, 8101, 4631, 5481, 6740, 486, 4559, 5364, 7656, 8870, 3558, 4888, 5272, 1272, 9667, 837, 8559, 8647, 4354, 2219, 4149, 7962, 4115, 7379, 6724, 5213, 9355, 7943, 7625, 1723, 4957, 346, 7300, 5577, 5944, 48, 7234, 8966, 4855, 563, 3940, 3711, 7082, 7956, 9674, 9247, 6078, 1952, 7237, 5146, 4715, 5017, 2865, 6318, 7707, 8141, 6015, 2084, 6677, 7515, 1340, 6940, 7321, 4783, 413, 8057, 4009, 6516, 2980, 2416, 4908, 1318, 5474, 1267, 4875, 3205, 8217, 4484, 8149, 5114, 5929, 4576, 5256, 3843, 1694, 7894, 1928, 5802, 3460, 5202, 6369, 4586, 7671, 2478, 8758, 3514, 1624, 4548, 1811, 7856, 4822, 4966, 7399, 521, 1008, 9098, 8790, 3089, 215, 1754, 6530, 3941, 1541, 1116, 3481, 1527, 7427, 3688, 3332, 7, 246, 4977, 425, 8987, 7906, 24, 7992, 6901, 1491, 1583, 1904, 7088, 4780, 992, 7232, 5401, 1274, 3031, 2843, 1032, 7387, 8146, 447, 791, 9194, 8704, 5386, 983, 2997, 7613, 5504, 8620, 4325, 2188, 4992, 632, 4055, 2086, 763, 7590, 9045, 6957, 9258, 5281, 1672, 2329, 3078, 9661, 6988, 9663, 4619, 5931, 1051, 4050, 3152, 83, 6367, 7191, 3345, 673, 9688, 9111, 670, 5815, 2296, 7634, 6352, 1490, 2054, 491, 2469, 753, 4844, 7130, 1962, 5441, 3655, 3376, 8651, 5765, 3184, 8576, 2175, 2711, 1383, 441, 3300, 3729, 3939, 6673, 9504, 7475, 7153, 4912, 4276, 4591, 8939, 6475, 4861, 7067, 1632, 658, 2151, 302, 7264, 8353, 2150, 4911, 5792, 802, 6597, 4635, 4482, 5149, 9032, 1786, 6450, 2916, 8094, 5057, 5806, 1619, 9303, 5798, 4288, 9265, 6066, 4866, 2710, 9406, 3000, 3612, 816, 4018, 3094, 3093, 7710, 9681, 4279, 2472, 939, 1489, 5082, 3506, 7124, 840, 3656, 6432, 4673, 5553, 4459, 2537, 8719, 2207, 1809, 5994, 588, 7882, 9074, 7830, 9197, 7764, 3904, 4503, 6065, 5164, 488, 8028, 8958, 293, 7350, 8673, 4641, 2255, 2148, 7645, 592, 8020, 911, 3955, 7370, 5036, 4079, 7244, 4565, 7271, 131, 6935, 6882, 2341, 781, 3496, 3887, 6924, 1961, 7476, 6911, 1484, 9445, 6776, 6429, 7047, 907, 4886, 4104, 3393, 3561, 2836, 3253, 8273, 848, 9289, 6351, 3030, 1602, 5418, 6553, 6171, 90, 6562, 6716, 7266, 9040, 3908, 8466, 132, 1872, 683, 2566, 5312, 8705, 3737, 3315, 4234, 3021, 4255, 5947, 3126, 878, 4538, 1930, 7667, 855, 2585, 6309, 3262, 8402, 4486, 424, 5255, 1862, 5081, 6720, 1025, 3454, 6192, 979, 5884, 4239, 5874, 6733, 3907, 1603, 8219, 7633, 6089, 8309, 1834, 69, 5670, 6773, 6316, 8573, 1248, 2232, 9424, 4929, 70, 3439, 8795, 7316, 4253, 4987, 3364, 5876, 2768, 3144, 6488, 4618, 8872, 7121, 4445, 4852, 1825, 1402, 6622, 6903, 2216, 1228, 1131, 944, 6841, 5366, 5379, 7527, 9622, 9053, 1537, 8885, 4594, 6161, 1101, 7227, 3846, 9088, 7008, 8643, 6371, 9115, 7452, 687, 4231, 6792, 765, 1325, 1328, 5354, 8878, 7478, 4997, 8675, 3111, 5915, 4627, 9338, 1621, 1662, 1746, 3269, 5483, 4953, 2634, 9325, 5396, 7302, 2904, 9417, 8729, 2704, 2930, 5352, 5072, 8280, 1223, 9441, 379, 3508, 9671, 2888, 6472, 6456, 9223, 8058, 5810, 3802, 2337, 5600, 6216, 6069, 6713, 7178, 331, 8086, 2561, 8881, 6063, 5459, 4385, 8880, 525, 8591, 3643, 1234, 8444, 8032, 9552, 7498, 4504, 6257, 1800, 5891, 8173, 5992, 6019, 3369, 2391, 7101, 928, 3212, 2686, 4362, 8933, 290, 6667, 9640, 2390, 8537, 1667, 5522, 2373, 5559, 1376, 6810, 28, 8963, 8603, 2302, 2861, 1852, 6151, 6798, 9413, 8127, 9577, 4714, 7795, 5068, 1469, 3110, 8357, 3386, 1964, 4532, 1550, 547, 5650, 603, 3716, 3599, 7941, 4535, 2744, 6005, 5824, 7960, 7847, 4377, 7037, 9314, 7844, 8698, 2662, 6262, 9275, 8034, 8426, 3556, 5127, 8, 1410, 3024, 4350, 8937, 717, 5040, 9038, 1075, 841, 4756, 3700, 6699, 3472, 8383, 4203, 2895, 1983, 6076, 5438, 9296, 7360, 8726, 11, 1375, 6625, 4964, 6326, 4258, 8072, 7787, 5946, 836, 3242, 4254, 600, 6919, 2078, 9210, 8730, 4052, 1142, 2943, 6920, 5939, 5299, 8415, 9315, 1205, 1758, 9237, 152, 3896, 1516, 851, 1195, 1304, 3164, 7351, 97, 797, 1294, 8684, 3774, 3165, 3368, 2224, 5374, 1788, 210, 4737, 9463, 5283, 6992, 105, 7929, 2363, 1682, 8167, 4547, 2901, 7562, 6153, 5274, 1397, 5917, 8888, 6830, 9334, 7372, 6086, 4972, 320, 1701, 9474, 5525, 5759, 6430, 8269, 4878, 930, 1436, 8506, 3905, 3805, 6717, 3207, 798, 6929, 642, 4579, 2298, 4910, 221, 8886, 6251, 434, 7939, 8623, 7488, 3427, 8163, 4101, 1916, 4561, 9566, 1959, 9089, 1405, 9477, 6308, 8715, 854, 1547, 7014, 1412, 4157, 8709, 1185, 8128, 4201, 5886, 6073, 9530, 9174, 6686, 2671, 8866, 8090, 8568, 8594, 6535, 3613, 3801, 6030, 4889, 1702, 2591, 3013, 7312, 8512, 1386, 8711, 2064, 5561, 5125, 2172, 4990, 4401, 1634, 906, 7980, 2892, 8365, 1338, 9558, 3685, 4937, 7662, 4649, 7150, 5510, 4563, 7280, 3946, 8604, 4206, 7278, 1947, 9324, 2454, 4134, 3365, 8530, 8600, 8331, 7359, 8313, 3971, 6317, 3674, 5589, 4742, 3609, 7493, 9442, 1458, 5938, 3915, 1588, 9182, 2616, 7295, 7578, 720, 6018, 8900, 5209, 739, 581, 4318, 2847, 5341, 1559, 2580, 1965, 1373, 3466, 8299, 4307, 4621, 3471, 8897, 1487, 3977, 2189, 9127, 725, 7760, 4524, 7816, 5720, 1419, 6906, 6755, 1586, 5390, 5848, 6595, 4305, 2422, 6871, 5445, 3276, 990, 8640, 3322, 6925, 9407, 2912, 8026, 2494, 6053, 8773, 9272, 8396, 391, 8274, 9059, 8364, 2875, 8823, 9218, 4915, 4031, 2235, 1078, 656, 5245, 8770, 1960, 8010, 2773, 3574, 6588, 7166, 5039, 4774, 9300, 9060, 5480, 3375, 523, 7332, 8891, 169, 8338, 3689, 764, 1921, 1394, 109, 3734, 1145, 9328, 636, 375, 2258, 2012, 9086, 8042, 1508, 5715, 6224, 5829, 6847, 5318, 1180, 3779, 3824, 388, 6876, 6956, 770, 6914, 9104, 8902, 707, 9178, 3680, 8957, 6554, 2927, 6548, 3082, 1753, 5572, 1363, 1302, 8221, 5430, 3681, 9680, 7523, 969, 937, 114, 5426, 2631, 2460, 9384, 427, 5899, 7782, 1931, 9356, 2512, 8833, 1647, 7241, 6020, 7866, 1984, 7480, 6492, 4199, 1360, 5509, 7118, 4399, 4411, 256, 1750, 7758, 6338, 5243, 6034, 3820, 4477, 8607, 2118, 8334, 8310, 8819, 8480, 6870, 2042, 3677, 2592, 9603, 3647, 549, 2755, 9250, 7168, 7446, 6411, 7058, 5789, 1429, 9545, 4608, 3746, 8836, 1269, 8780, 275, 5092, 1356, 9476, 5916, 2137, 6900, 8774, 6785, 6296, 6569, 6110, 9599, 9696, 4481, 1594, 6478, 1826, 3282, 7579, 8921, 6939, 9200, 957, 3318, 9554, 7217, 1629, 2242, 9472, 4312, 6655, 3168, 8772, 4021, 7612, 167, 1606, 6254, 1122, 6987, 5300, 5534, 4690, 8835, 1298, 6111, 2877, 5588, 7048, 3857, 8496, 6099, 7987, 7615, 2606, 8629, 8736, 6585, 5237, 2005, 3703, 7627, 1955, 1688, 4223, 4137, 3234, 3610, 9499, 2589, 93, 7719, 6168, 8976, 8507, 2681, 5178, 677, 9593, 4432, 2272, 4487, 7673, 7614, 2557, 5392, 4745, 4175, 2641, 7174, 8412, 4518, 2278, 2708, 796, 7635, 8967, 3954, 5143, 7029, 4061, 8828, 6339, 8261, 8133, 6422, 5167, 3535, 8728, 8689, 1837, 8727, 9514, 3782, 8172, 8200, 1897, 1790, 9144, 995, 3874, 7766, 9618, 2434, 6970, 9401, 5838, 2741, 2600, 4457, 129, 601, 3344, 2601, 2918, 6865, 4768, 8998, 1989, 5749, 6600, 145, 2562, 6750, 5813, 2280, 7657, 7151, 2394, 1480, 9503, 675, 8895, 8717, 5649, 7343, 5172, 2212, 4460, 4765, 1581, 4800, 4430, 8654, 4435, 142, 3878, 8190, 5216, 4003, 1509, 7852, 9133, 5417, 2909, 2310, 1637, 3607, 6082, 1530, 8367, 8441, 6342, 6743, 9662, 2992, 4830, 1068, 3879, 1792, 4820, 4461, 4227, 2810, 3786, 9092, 3170, 3218, 6121, 2425, 2672, 5858, 422, 8587, 5236, 8225, 6469, 9454, 4062, 1895, 3962, 5518, 1557, 524, 5410, 7668, 3845, 2835, 7565, 7243, 8757, 81, 2786, 1681, 9047, 2277, 2774, 3206, 4520, 1238, 1724, 2506, 5766, 2577, 4368, 6470, 7832, 6480, 8319, 5016, 1747, 5131, 8830, 1511, 5433, 9279, 6781, 9169, 5203, 3518, 2669, 674, 3727, 1233, 3158, 7798, 3053, 8063, 3984, 1005, 9437, 5704, 5215, 2766, 222, 2885, 1194, 2223, 8228, 3881, 490, 5199, 3569, 8867, 4418, 6283, 2673, 6537, 462, 9114, 4694, 8578, 808, 6347, 7876, 1347, 3113, 9110, 176, 6055, 1177, 2368, 2705, 8797, 664, 102, 1130, 8860, 692, 7940, 4984, 6605, 5308, 7291, 1764, 3443, 6517, 2136, 7831, 9172, 6737, 1022, 2203, 741, 5100, 5701, 648, 5508, 1286, 3394, 8458, 5666, 1756, 3362, 4761, 5113, 9451, 7222, 4280, 3632, 3409, 4946, 4311, 7730, 7005, 1280, 5953, 7778, 6884, 8899, 483, 1871, 1315, 6337, 9050, 3745, 7448, 6952, 5909, 9623, 6631, 5578, 6330, 5970, 562, 9087, 9657, 6104, 2779, 1385, 825, 6398, 2706, 9120, 2545, 4195, 786, 7630, 5883, 8013, 1810, 9443, 1468, 4324, 6043, 8051, 5732, 1521, 6650, 3813, 4607, 1643, 3841, 6345, 3227, 9714, 1263, 6464, 2215, 582, 2530, 8810, 3576, 4749, 5961, 4982, 9149, 8052, 5691, 6198, 65, 3999, 3762, 3781, 8918, 4501, 3550, 8278, 9160, 5887, 6742, 5981, 6273, 8144, 6656, 630, 3419, 1890, 8391, 1188, 1190, 5788, 1847, 4136, 1536, 2275, 5863, 6354, 2090, 6218, 6626, 4755, 1495, 5420, 2569, 7672, 5804, 2138, 5546, 3597, 5388, 682, 6653, 7085, 1830, 1139, 823, 2670, 9693, 3229, 3921, 5614, 8022, 6668, 7104, 5616, 456, 2872, 3777, 5524, 4164, 4630, 2132, 9, 5329, 4716, 9257, 9145, 873, 4962, 3547, 9682, 8750, 1628, 757, 5412, 9039, 8074, 6313, 2076, 9414, 7203, 1316, 7560, 403, 5182, 4767, 5140, 9345, 9248, 2613, 8472, 5539, 9694, 139, 5228, 3842, 4262, 3201, 5517, 1903, 4636, 2450, 8645, 3374, 5356, 3757, 6426, 2597, 9337, 5468, 3313, 3275, 7376, 6038, 1333, 9057, 3467, 5226, 5044, 1442, 5063, 2165, 5786, 7404, 1099, 3051, 2062, 661, 1738, 6049, 6410, 4178, 8738, 6461, 7644, 7444, 6365, 1717, 4273, 4834, 1617, 211, 1869, 146, 4947, 3865, 6029, 2931, 1016, 8801, 9137, 1454, 7068, 3577, 3043, 2513, 6148, 8528, 4005, 637, 8138, 8490, 4490, 6834, 1939, 8767, 6159, 5118, 7695, 3295, 4638, 3657, 5054, 7631, 7589, 7235, 1711, 6557, 5667, 8439, 6575, 602, 3020, 2806, 7535, 6379, 4015, 9147, 6184, 629, 2785, 5800, 4537, 9507, 2963, 2208, 4064, 787, 6965, 3800, 931, 6549, 289, 6651, 1599, 1824, 7957, 5462, 5424, 3090, 1057, 8621, 3033, 7597, 7391, 8686, 9261, 116, 259, 8030, 2691, 9404, 2830, 5091, 8462, 6628, 5382, 8669, 2080, 6599, 6102, 6797, 6784, 7927, 3554, 8776, 2177, 2650, 1486, 115, 5108, 5475, 1059, 1652, 6494, 8911, 3379, 6000, 5393, 2351, 4270, 2944, 2630, 6473, 68, 5548, 5793, 3658, 4122, 7371, 8372, 7647, 2004, 3461, 575, 9148, 7532, 101, 3077, 945, 3772, 8401, 1115, 138, 8535, 811, 1089, 9385, 4622, 1796, 7858, 6010, 9642, 8084, 3890, 8325, 7283, 4132, 323, 5979, 5826, 4764, 4382, 5313, 1791, 9277, 3150, 7286, 8991, 3473, 8196, 6031, 2692, 2905, 204, 5808, 3455, 8732, 4343, 8118, 7650, 1828, 6596, 7822, 7228, 8040, 1443, 9639, 2554, 4775, 9466, 2213, 2942, 2729, 9701, 1158, 5440, 3616, 5514, 9313, 3382, 8501, 3062, 1434, 6995, 3143, 2285, 4096, 3665, 800, 3883, 6406, 267, 6526, 1281, 4420, 9715, 4717, 6343, 8151, 1549, 7194, 7926, 7530, 0, 2185, 282, 3017, 6786, 8982, 4727, 6277, 9021, 4393, 5378, 4436, 294, 6874, 9452, 4659, 8857, 4733, 8259, 9425, 4205, 2104, 7945, 7706, 9374, 3422, 951, 3583, 918, 6899, 2110, 5617, 1079, 6332, 7432, 8821, 4193, 2632, 8523, 3675, 910, 4917, 9305, 3367, 1148, 8145, 638, 7575, 1284, 6558, 4274, 2856, 8081, 3799, 6187, 3161, 2733, 4246, 6640, 6247, 3343, 7367, 3718, 9297, 3335, 9203, 850, 4082, 4016, 2573, 6948, 726, 5662, 2974, 2819, 2900, 4106, 3233, 2588, 1685, 2458, 8949, 4750, 6412, 584, 1125, 1243, 4138, 5010, 1882, 2508, 1030, 839, 6243, 9195, 7396, 4211, 2509, 8846, 9158, 2886, 7546, 3604, 3850, 2715, 3770, 3970, 2605, 3219, 5018, 459, 2276, 2629, 5338, 195, 6890, 4879, 933, 5820, 9400, 7596, 7805, 3666, 2490, 3882, 6067, 6769, 8482, 4165, 200, 3827, 5910, 7607, 4546, 2198, 8944, 9246, 7044, 8853, 8004, 1433, 6214, 528, 9492, 5535, 9188, 404, 7324, 6325, 6500, 7031, 269, 3348, 7549, 4568, 4924, 4826, 3503, 1666, 3758, 2510, 6295, 8465, 7792, 245, 5363, 1797, 9256, 4404, 7402, 4718, 7877, 2871, 4516, 900, 5224, 4515, 6258, 2957, 2126, 479, 6017, 2032, 6336, 2462, 9619, 7386, 1114, 5932, 611, 1449, 5179, 2894, 5998, 9082, 6234, 6779, 8756, 5968, 1464, 5456, 9284, 470, 7382, 659, 1612, 7781, 5194, 4760, 5755, 6091, 7303, 2253, 6646, 1861, 4932, 7727, 338, 4291, 9651, 8492, 6463, 7937, 2694, 1278, 7708, 6866, 5603, 940, 3169, 5106, 1636, 8317, 393, 3241, 5605, 5013, 8323, 2009, 7990, 9498, 6848, 5286, 2051, 4590, 3490, 1305, 7715, 2622, 7112, 6115, 3230, 2959, 7282, 1273, 5730, 14, 8608, 9022, 4359, 8980, 3028, 5184, 7969, 3544, 6860, 25, 6635, 1207, 5195, 6189, 2294, 7177, 2608, 2184, 8484, 4222, 4260, 974, 8339, 7510, 4428, 610, 335, 3641, 6482, 8892, 179, 4950, 1039, 5422, 7859, 3052, 640, 5262, 8555, 364, 1463, 2984, 3125, 3722, 6571, 8368, 1232, 8231, 8533, 5141, 5976, 9000, 9028, 6976, 9329, 1641, 6324, 9486, 9295, 2718, 5470, 4969, 9381, 7978, 6636, 4294, 5287, 3027, 2243, 2162, 1107, 9393, 1689, 5668, 8413, 8174, 4883, 1565, 5282, 4936, 1977, 7002, 8041, 2945, 198, 2140, 3796, 6144, 5601, 2286, 3501, 8665, 9724, 2536, 8012, 6837, 991, 9280, 2197, 7813, 8293, 4069, 9648, 3, 8855, 2998, 6966, 724, 5942, 3036, 731, 2982, 7720, 6211, 209, 8237, 7113, 6684, 4342, 1582, 2348, 3459, 8033, 7791, 909, 2181, 1045, 3252, 4890, 51, 7096, 6806, 8659, 1121, 3815, 7165, 4196, 4527, 6238, 1482, 828, 7065, 1221, 3831, 9660, 9033, 8416, 820, 8112, 5791, 3631, 1070, 1096, 4087, 439, 2456, 3981, 824, 3049, 6384, 6762, 446, 6998, 144, 280, 4119, 2355, 829, 1475, 7308, 9518, 5122, 9455, 357, 9586, 8125, 8825, 6236, 5065, 1181, 8212, 4045, 9500, 4320, 232, 5664, 5752, 8471, 1535, 4666, 2996, 5097, 3220, 2893, 9068, 6756, 7503, 9722, 1319, 5176, 6056, 6942, 7566, 9485, 3097, 6124, 6805, 7658, 5348, 4034, 7443, 2602, 4375, 8478, 1772, 6416, 7988, 4383, 9333, 7437, 4567, 4380, 8842, 2731, 5047, 2270, 5132, 184, 1794, 7216, 5919, 5289, 1622, 1982, 2100, 3360, 7344, 5585, 6644, 4007, 6534, 7603, 3255, 5683, 4172, 2356, 7348, 1368, 3626, 6504, 9723, 5654, 5937, 3433, 821, 8688, 9654, 4315, 5196, 1476, 7423, 3221, 6536, 1798, 1146, 5809, 2800, 2717, 4549, 7769, 4686, 5949, 8400, 17, 7654, 3391, 9260, 5984, 1186, 4292, 8906, 414, 1019, 3851, 8558, 4840, 3310, 3342, 7102, 8768, 440, 5646, 1391, 8930, 1128, 2408, 9562, 7563, 3047, 7901, 3312, 2433, 8369, 3107, 1677, 7924, 570, 5339, 2447, 1757, 621, 3797, 4921, 7783, 5448, 2657, 5581, 1506, 5619, 9439, 8657, 6662, 7349, 7747, 9524, 7921, 9716, 9365, 5343, 7164, 4414, 1726, 170, 2045, 2958, 870, 7319, 175, 6449, 6328, 5659, 1992, 9320, 2552, 5210, 1721, 2677, 9095, 904, 3263, 9512, 3032, 9573, 3187, 4476, 3987, 9484, 7260, 8505, 7366, 4285, 6524, 4528, 5059, 8934, 1502, 5936, 7950, 9450, 2976, 223, 3448, 2727, 7181, 975, 555, 8106, 8327, 2962, 4536, 6811, 1651, 6394, 5041, 2190, 6676, 4587, 719, 1812, 3817, 8476, 5171, 3405, 9616, 1225, 9553, 7326, 6660, 7218, 4372, 884, 4275, 9415, 1675, 3709, 4595, 6021, 349, 8816, 7449, 9366, 4162, 4217, 8968, 5310, 3086, 9581, 6550, 7519, 1755, 9582, 8787, 7786, 5702, 8922, 7311, 6579, 8510, 2703, 3545, 2811, 3931, 2652, 2129, 3432, 7272, 2576, 3627, 1160, 9286, 9526, 9497, 2831, 1773, 7021, 2651, 9617, 2468, 9231, 4168, 5150, 2883, 7811, 188, 9399, 7145, 2370, 6801, 4995, 448, 5845, 2574, 7675, 546, 1857, 8320, 6085, 6402, 1317, 2437, 6697, 119, 9049, 7190, 9190, 9555, 1698, 5554, 4301, 8871, 4400, 147, 7556, 7338, 7496, 8232, 6291, 8672, 7954, 8087, 7365, 8831, 432, 4876, 8428, 7737, 8520, 1880, 9017, 8043, 1560, 7724, 6634, 8786, 1009, 4027, 7652, 7880, 8635, 3728, 7915, 1905, 9003, 8188, 4028, 3140, 297, 5831, 7653, 8178, 4326, 881, 822, 2157, 5084, 2429, 5610, 3044, 4, 4174, 3542, 1029, 8218, 74, 3384, 898, 3965, 6033, 6672, 6967, 9012, 8849, 9432, 9350, 5298, 1902, 8292, 9336, 520, 8205, 7629, 5803, 3640, 2483, 3185, 8677, 1533, 7355, 4238, 7215, 3968, 4120, 5507, 6709, 9611, 6083, 1164, 6037, 6046, 4145, 4794, 305, 5029, 7788, 3989, 341, 3948, 3562, 761, 2404, 2570, 7910, 1716, 5323, 3961, 5499, 3155, 7998, 6898, 6618, 9264, 4658, 9402, 2244, 5259, 7568, 303, 6230, 6520, 5477, 5690, 4877, 2860, 6208, 2191, 5647, 4000, 892, 7179, 5449, 3134, 7075, 4891, 688, 4541, 406, 6839, 9471, 7004, 2999, 283, 6544, 9326, 9112, 5602, 3818, 5828, 6423, 9423, 5157, 4198, 8932, 4208, 9604, 2769, 6271, 3247, 1769, 831, 189, 2313, 779, 7242, 7288, 5219, 1178, 8706, 7330, 3066, 5206, 5713, 4117, 337, 2815, 3306, 8908, 7274, 8007, 4441, 3440, 2265, 2534, 7642, 7132, 9635, 4985, 8297, 5843, 1167, 263, 4089, 6278, 927, 9537, 1690, 522, 7497, 6540, 3197, 127, 3430, 2636, 4170, 326, 3428, 3420, 6666, 2899, 5609, 3101, 8111, 8380, 7053, 7459, 1416, 417, 6439, 506, 6113, 8754, 8414, 1044, 967, 4699, 1948, 941, 8088, 6833, 7564, 9717, 5952, 1922, 2724, 5064, 7392, 5136, 1236, 1976, 7175, 6058, 1149, 253, 4823, 4854, 1674, 8154, 5024, 4829, 7056, 7893, 2558, 3232, 999, 6679, 7531, 9094, 1878, 7912, 6485, 6001, 2343, 3932, 4464, 1015, 3135, 1973, 3074, 4244, 1296, 2813, 4857, 3301, 3992, 2048, 2164, 1326, 7681, 7986, 8281, 1270, 5078, 924, 8947, 1865, 7362, 2846, 2649, 9281, 1883, 6627, 5527, 8923, 458, 5812, 7823, 6228, 8316, 1822, 5758, 5686, 8854, 6728, 7955, 8914, 2023, 5737, 8526, 9161, 3918, 8554, 9734, 8850, 8207, 2523, 5964, 9543, 1209, 2518, 9100, 6941, 7817, 4634, 3719, 9431, 633, 7820, 3509, 5351, 8694, 9291, 7949, 500, 8992, 6128, 5273, 8153, 1428, 2033, 8753, 1695, 7001, 5856, 5187, 6943, 5748, 541, 949, 3934, 1968, 543, 2077, 2609, 9729, 8080, 9221, 3775, 7808, 4440, 4180, 5957, 777, 6705, 3659, 6702, 4176, 9624, 6993, 1453, 9709, 5365, 7173, 6186, 496, 7186, 5635, 4505, 6414, 9316, 2809, 5624, 2571, 1912, 3179, 3810, 7513, 6584, 3768, 442, 4434, 1073, 1102, 3593, 953, 7406, 7694, 8699, 7225, 8262, 8986, 494, 9004, 7886, 8070, 9506, 9018, 6057, 4473, 1839, 8781, 4185, 714, 2479, 2081, 5368, 9483, 7868, 4129, 2421, 6400, 7110, 4269, 9628, 8985, 1608, 5090, 9241, 8180, 9436, 9171, 2787, 7428, 2227, 2301, 7516, 42, 4583, 7103, 2002, 2105, 7054, 5163, 1093, 4610, 1136, 3091, 7487, 23, 3297, 2626, 6022, 6490, 3579, 7522, 1049, 4927, 3998, 8883, 7389, 2256, 1668, 526, 6383, 6753, 1208, 7030, 6209, 8455, 9146, 4868, 6042, 1627, 2318, 9309, 8775, 804, 3751, 1357, 4974, 3099, 8638, 1829, 612, 126, 2664, 1896, 4802, 9344, 7320, 73, 7750, 1415, 6574, 1367, 52, 9046, 6973, 6613, 5716, 2041, 2975, 4322, 9561, 5528, 6459, 2791, 6984, 6777, 9612, 8460, 5712, 5822, 262, 972, 5665, 6281, 2195, 8328, 2022, 6648, 4352, 6139, 1783, 2028, 5457, 9575, 3794, 2598, 6879, 8213, 7010, 4681, 1456, 9403, 5463, 4597, 1240, 7451, 7904, 5331, 6817, 3288, 3482, 2749, 1166, 1395, 8282, 6945, 8215, 6949, 8446, 9206, 8844, 2898, 6176, 6495, 6510, 9422, 3679, 6851, 9730, 1066, 4306, 1496, 2829, 6836, 3645, 4327, 4485, 9482, 6327, 8524, 4703, 4645, 1220, 3291, 9209, 7872, 3012, 1736, 2753, 1129, 8019, 3899, 7591, 6460, 4856, 7036, 5631, 4025, 7878, 5492, 6867, 7509, 3244, 3510, 7684, 4056, 9283, 8083, 4632, 1596, 5644, 4894, 655, 431, 7506, 746, 8641, 7794, 7666, 9193, 345, 2107, 5930, 8193, 5653, 87, 8271, 3302, 9348, 6502, 6008, 3979, 978, 2746, 36, 8378, 2981, 2525, 2325, 4906, 5990, 2328, 106, 2480, 1832, 882, 3588, 3621, 2680, 9459, 3515, 6199, 7702, 5563, 9035, 3839, 6568, 788, 9646, 7265, 8586, 2611, 57, 4221, 2967, 5342, 8152, 4633, 1380, 4347, 1052, 2541, 7015, 5717, 5583, 5959, 5573, 5275, 4923, 8276, 4707, 7256, 5606, 166, 7189, 6237, 7373, 977, 7163, 1329, 3188, 7558, 3854, 372, 1424, 4853, 5086, 327, 6447, 8162, 5651, 5349, 7454, 8427, 5875, 9388, 2924, 3884, 5622, 889, 6292, 1913, 4588, 1137, 2966, 2380, 2645, 2345, 9509, 2095, 2820, 9487, 2431, 8997, 252, 2237, 2732, 6715, 3735, 6641, 9615, 9737, 1350, 5437, 8377, 31, 9638, 8335, 9034, 6694, 8668, 1515, 2862, 5648, 6934, 7802, 5855, 538, 5035, 7677, 3662, 9643, 5419, 452, 5906, 130, 9502, 902, 8734, 6778, 4013, 9293, 3548, 749, 1426, 514, 7315, 7888, 451, 8570, 7538, 9397, 6688, 597, 2837, 4125, 7107, 5775, 8868, 685, 3132, 6443, 8879, 9270, 827, 438, 4759, 6407, 4241, 418, 6772, 4263, 8371, 3741, 6120, 9353, 2826, 4799, 5677, 4941, 4454, 219, 7807, 7605, 4564, 2455, 8894, 8300, 2015, 3151, 5197, 9569, 7409, 7495, 6303, 8951, 4700, 6683, 6910, 5222, 6360, 7381, 3283, 419, 1891, 2267, 5001, 2953, 4153, 6791, 5062, 8907, 5986, 1802, 7821, 6982, 9386, 3856, 7639, 7646, 6380, 3925, 4808, 3293, 6863, 9128, 4365, 8644, 6182, 5586, 8245, 7641, 1981, 4553, 1041, 402, 8082, 9079, 1980, 2340, 5317, 3381, 8199, 2838, 4971, 2069, 6926, 2668, 4215, 3486, 4642, 1953, 5293, 8192, 6547, 1043, 4417, 8778, 2079, 9243, 9109, 5625, 2055, 7057, 6573, 9346, 1332, 4914, 2855, 4781, 3415, 2604, 2438, 4304, 9347, 9029, 7545, 5327, 4560, 2225, 3095, 2607, 498, 913, 6765, 6126, 3014, 1366, 6260, 2378, 4351, 7637, 6819, 5121, 4677, 3488, 728, 3174, 8121, 5674, 4340, 3833, 2392, 6166, 1479, 8197, 4072, 5594, 1037, 1854, 5643, 2290, 6745, 5852, 2111, 9131, 7230, 2214, 7253, 8275, 4286, 3398, 9652, 689, 7196, 6122, 9664, 1817, 2812, 5633, 8746, 6749, 9525, 4963, 2864, 1504, 3557, 4033, 3565, 3691, 3076, 3011, 4948, 1805, 8508, 8865, 7976, 4053, 6793, 103, 5165, 8404, 2675, 3231, 9702, 7033, 3585, 7669, 916, 2025, 1050, 838, 5544, 3847, 4693, 4073, 2400, 4456, 1626, 3538, 8438, 7745, 2463, 3408, 9083, 3457, 8143, 7298, 8584, 6659, 3257, 7209, 4849, 6969, 5362, 3519, 6452, 1932, 8964, 3195, 4283, 3636, 8352, 7548, 6955, 7129, 2707, 1771, 1740, 2852, 8532, 2221, 2246, 4438, 9076, 9084, 8625, 4863, 3975, 2687, 4605, 5252, 3614, 3967, 4692, 2419, 760, 3200, 463, 9071, 833, 7074, 9205, 6105, 1592, 8893, 6552, 7327, 2956, 3328, 7996, 7970, 4639, 6809, 2443, 9469, 5051, 4381, 3108, 8598, 2828, 5032, 7296, 6386, 9043, 7640, 8791, 482, 1123, 277, 46, 8077, 6782, 5387, 4502, 7622, 1704, 6937, 1875, 2850, 2145, 5918, 2661, 4818, 782, 387, 3619, 3573, 8198, 4489, 3990, 4989, 3837, 3015, 1435, 1403, 395, 6922, 5859, 288, 4793, 1239, 3038, 9727, 5074, 2034, 7935, 3630, 3186, 2902, 7776, 789, 6714, 2358, 9493, 47, 4384, 5056, 1571, 8399, 7426, 8100, 4331, 2234, 3891, 2857, 3203, 7679, 7416, 6642, 9051, 4796, 1843, 304, 4363, 3213, 9534, 4155, 9373, 2233, 5250, 8448, 6761, 6690, 2757, 3065, 6604, 6729, 4391, 554, 8691, 7120, 734, 50, 1649, 7126, 6245, 7275, 2053, 6652, 9475, 9340, 6097, 8305, 2869, 4493, 6508, 2760, 6100, 2384, 6731, 1299, 9579, 7429, 1915, 2667, 1975, 2823, 6357, 1999, 7109, 712, 5148, 5104, 2874, 585, 1768, 7000, 1887, 8186, 2955, 1642, 1638, 1313, 4812, 1172, 5877, 2796, 6095, 3571, 6259, 9106, 6665, 5731, 8627, 3392, 2919, 5888, 4141, 4091, 4111, 812, 272, 2928, 4081, 8135, 7069, 6996, 2635, 1396, 3326, 4625, 3759, 307, 4390, 8700, 436, 9438, 1176, 2196, 7920, 265, 3792, 8379, 7072, 8270, 7561, 7431, 4360, 2396, 4074, 732, 9602, 6630, 2418, 3589, 8696, 8098, 7023, 3256, 7119, 4935, 71, 8784, 7550, 6775, 7032, 1092, 4310, 6272, 9010, 2910, 6137, 4103, 3114, 4204, 3592, 3079, 3723, 3512, 5511, 2639, 3265, 6891, 6072, 2489, 6390, 6747, 948, 5421, 6445, 973, 3465, 1901, 9700, 6732, 6133, 1954, 3575, 6959, 5337, 8588, 1213, 1211, 4816, 8241, 218, 3708, 4500, 3290, 9678, 8116, 6314, 7236, 250, 5292, 4942, 1540, 1222, 5402, 9637, 7193, 8159, 8406, 4108, 5186, 6481, 8515, 2467, 6160, 8622, 7249, 8263, 4483, 6270, 8637, 3804, 7718, 6532, 6244, 604, 4071, 4850, 5799, 8425, 9009, 291, 666, 4602, 4713, 5612, 5159, 1776, 2050, 2058, 9430, 7514, 2603, 8721, 8294, 635, 2915, 1751, 5124, 1793, 8929, 2922, 645, 362, 3173, 5246, 2782, 2719, 1779, 426, 4042, 5279, 4098, 7292, 8804, 8762, 5334, 6513, 8048, 8676, 5008, 7581, 5162, 4462, 4431, 807, 6165, 4900, 6293, 6760, 6194, 9565, 3957, 9236, 8474, 2771, 5431, 7850, 7442, 1523, 3570, 5357, 1684, 8424, 1440, 9173, 271, 1609, 5306, 7017, 3871, 8284, 6290, 9505, 5095, 7081, 748, 4102, 1979, 7252, 5407, 7007, 8445, 5207, 7958, 8977, 6217, 1065, 5618, 4036, 5034, 1285, 9689, 9535, 1470, 9213, 5330, 7729, 5996, 4839, 4465, 9016, 212, 7883, 108, 1192, 9129, 7239, 7051, 5411, 5413, 6907, 3414, 8181, 9370, 3740, 4158, 3617, 5033, 9253, 4247, 7076, 6856, 3191, 8905, 1525, 4901, 6397, 1728, 1259, 461, 1646, 4817, 7415, 3475, 8201, 805, 7554, 8456, 5370, 1430, 6362, 60, 2526, 5991, 532, 1713, 6321, 4669, 3366, 7797, 3914, 3705, 9375, 2550, 9719, 8519, 4975, 4088, 5985, 8567, 6999, 5427, 1064, 9254, 3401, 12, 1327, 3352, 997, 1014, 7968, 2970, 5975, 9226, 9093, 2072, 3555, 7077, 9259, 9220, 9620, 8113, 5599, 5025, 4955, 6827, 4272, 4029, 7334, 6938, 8941, 6718, 2211, 5956, 1109, 2971, 3001, 389, 6382, 5682, 4303, 5738, 2832, 1103, 7471, 4392, 7602, 3994, 8703, 583, 1201, 7204, 9596, 1937, 5217, 5466, 5188, 464, 1958, 3083, 7897, 3458, 9191, 6344, 7773, 1918, 3752, 4811, 220, 6002, 4313, 7149, 1661, 3958, 6, 5398, 5914, 8962, 1497, 4589, 6225, 8517, 1934, 5482, 5007, 5566, 2199, 6060, 3803, 2840, 5973, 2685, 551, 1709, 2538, 9130, 2106, 8370, 618, 7092, 8104, 2031, 1513, 4529, 5423, 2397, 6435, 4453, 6878, 8609, 4978, 5767, 6219, 2989, 7638, 4517, 567, 832, 3307, 8059, 3390, 4112, 1706, 2481, 2575, 3844, 4573, 7414, 9494, 3341, 4143, 3080, 1355, 1524, 7528, 2689, 4173, 4679, 7377, 301, 1601, 2415, 5913, 2587, 8373, 699, 7246, 9585, 3780, 5632, 4893, 4422, 3131, 697, 1946, 9594, 7323, 9468, 4369, 7767, 5762, 1140, 7698, 8540, 558, 4545, 2995, 2056, 2194, 9672, 2972, 5028, 2268, 4643, 373, 6800, 1310, 8405, 1104, 1283, 6403, 1567, 2884, 3546, 8442, 8582, 2304, 755, 5696, 4097, 7833, 9692, 3819, 5925, 4256, 5997, 4093, 2473, 7928, 4880, 5983, 1246, 410, 2617, 1648, 8122, 3956, 5714, 1431, 85, 3271, 9150, 8718, 634, 9162, 2333, 3549, 241, 2083, 7576, 6252, 3371, 2714, 7420, 885, 3411, 5761, 4827, 5530, 7544, 5294, 2949, 4895, 6795, 7884, 7853, 8859, 4650, 6205, 3141, 2287, 9376, 5679, 6741, 2289, 6331, 1715, 4683, 3331, 4771, 2113, 7135, 9151, 6633, 1174, 3522, 569, 5871, 5166, 894, 3653, 552, 8326, 5562, 3952, 9647, 4095, 2625, 8006, 1193, 5355, 94, 3361, 2737, 7293, 6531, 1163, 3355, 4581, 6591, 5098, 7289, 5278, 4637, 3116, 9398, 2360, 7714, 3978, 7079, 3973, 2564, 2068, 380, 9721, 2728, 3410, 3720, 6951, 4671, 1266, 6080, 8666, 7828, 8741, 6759, 8018, 1920, 3525, 2950, 3004, 3867, 1088, 7819, 4980, 4447, 1942, 8925, 9066, 7861, 5903, 7863, 2699, 9621, 6425, 6700, 9214, 9712, 3002, 1382, 5892, 5708, 394, 6726, 7439, 7407, 1816, 8208, 6947, 663, 3935, 7433, 4415, 4788, 5862, 2312, 9668, 4769, 7097, 8950, 4014, 3059, 705, 1438, 6674, 7212, 6675, 3582, 6719, 5235, 7862, 7012, 5193, 37, 285, 8605, 1610, 3303, 4676, 5726, 4044, 5729, 9287, 5117, 3897, 3305, 8548, 9725, 9607, 6196, 5126, 3480, 5841, 5590, 1450, 7529, 56, 3529, 7441, 1334, 4295, 4815, 1993, 3216, 4705, 4838, 1352, 5733, 8873, 9653, 5153, 6315, 7207, 7540, 5050, 4209, 3806, 6616, 7378, 1241, 3284, 8287, 9495, 1069, 4843, 7648, 8563, 6188, 2406, 279, 4048, 4300, 986, 6269, 4652, 4152, 1818, 3664, 1529, 3600, 3788, 6248, 2254, 9389, 5776, 7716, 4085, 5230, 5811, 7134, 7325, 2889, 3704, 7582, 7849, 879, 5721, 4242, 7390, 4043, 1306, 2187, 9080, 4358, 895, 6826, 4951, 9704, 2504, 9307, 4346, 9405, 6897, 9156, 7687, 9429, 8585, 5743, 1218, 7100, 5895, 7947, 9015, 1844, 2790, 5442, 123, 5271, 2424, 8464, 3533, 5, 2485, 6125, 3738, 3449, 7091, 3895, 3767, 5111, 6701, 7518, 7137, 8522, 8494, 2449, 1268, 3162, 9465, 7169, 7240, 1579, 2060, 3103, 5397, 2432, 6695, 5780, 9321, 7570, 9408, 5399, 9580, 8788, 2859, 7063, 6079, 1165, 3462, 8268, 8693, 3790, 5827, 8348, 5882, 3209, 744, 7412, 7122, 3286, 7491, 9136, 2088, 7223, 1033, 9547, 4006, 5557, 8740, 950, 6440, 299, 8109, 589, 453, 6746, 9708, 7136, 3995, 26, 4904, 499, 400, 6119, 7183, 1657, 3178, 9225, 6766, 9418, 9456, 3832, 5639, 2347, 736, 4869, 1542, 8243, 8005, 1346, 1842, 9327, 4944, 4943, 1990, 112, 5288, 2402, 1761, 8917, 3470, 9644, 2339, 4613, 208, 8904, 9271, 4472, 2248, 7688, 867, 2516, 6181, 7756, 4116, 150, 9056, 4513, 4779, 5133, 5637, 1126, 1254, 9101, 6150, 8812, 77, 5403, 7309, 5818, 1423, 1929, 1835, 1938, 8027, 7484, 7922, 2282, 1084, 5671, 3129, 1184, 4494, 236, 4194, 3228, 9387, 4898, 8449, 7071, 4604, 3694, 2560, 8093, 7473, 7595, 9548, 8981, 8747, 4870, 5080, 4934, 7846, 5257, 3763, 5865, 9196, 5740, 3976, 8806, 1289, 7999, 1203, 891, 1262, 7903, 3951, 2908, 7039, 329, 4556, 6032, 111, 7790, 8826, 1633, 9282, 5595, 1110, 5129, 915, 321, 5350, 2044, 1053, 1799, 9117, 8766, 943, 8016, 6109, 8565, 587, 3222, 3938, 7383, 3601, 6519, 2204, 29, 1297, 8993, 3791, 2817, 8229, 3859, 4835, 5621, 5512, 4267, 7592, 325, 6051, 5238, 5314, 5103, 8497, 9204, 3540, 8398, 1840, 5608, 2037, 5779, 7485, 405, 2121, 7052, 6388, 605, 5227, 1455, 8306, 5304, 5795, 2167, 5543, 1742, 3340, 4514, 408, 5225, 5778, 1290, 6203, 6098, 7221, 8498, 4039, 1421, 5322, 4463, 7262, 4539, 6727, 2323, 4905, 4837, 5472, 7690, 1923, 631, 6872, 899, 6256, 5538, 2220, 6451, 3742, 3926, 5102, 6075, 5556, 1970, 8815, 9233, 5904, 4744, 6387, 7800, 7705, 5345, 7661, 7617, 5003, 5266, 2822, 2879, 8733, 1006, 2000, 8602, 4425, 7417, 8227, 4508, 7257, 6178, 3325, 8036, 2937, 6586, 1870, 6172, 8187, 7865, 817, 1866, 2382, 7697, 1625, 5675, 3944, 4335, 7445, 646, 3697, 4719, 7899, 3596, 9676, 2720, 3809, 2238, 4939, 3208, 270, 3724, 3594, 4124, 1214, 9479, 1271, 566, 1859, 9058, 6682, 3309, 4130, 676, 7571, 4181, 9728, 5077, 2722, 1460, 6093, 9457, 6462, 2990, 4251, 6149, 2176, 2696, 2965, 9670, 8014, 5567, 1611, 6389, 3421, 1655, 1520, 2174, 8362, 5192, 4371, 3243, 2568, 3943, 9391, 5630, 2182, 8117, 7837, 8314, 5661, 1258, 7887, 3338, 9227, 5258, 6780, 3029, 5837, 5248, 9363, 8079, 5718, 4859, 143, 148, 8247, 6712, 8194, 1910, 7114, 8134, 7020, 6201, 1451, 5500, 7873, 4099, 533, 1563, 9184, 3193, 5169, 4086, 7400, 679, 6158, 8796, 2354, 9522, 6511, 5405, 5966, 2440, 622, 6431, 6689, 7310, 6366, 7824, 2789, 4146, 7977, 1914, 2309, 3493, 1906, 8634, 4319, 8440, 6892, 162, 6372, 3085, 5901, 6862, 7759, 4455, 2748, 2346, 8583, 2920, 4740, 3039, 5825, 1362, 5878, 6195, 1737, 1420, 934, 9234, 6170, 4753, 1162, 3634, 4396, 3654, 4785, 6054, 9005, 7397, 3526, 2500, 3911, 7470, 9019, 385, 3438, 7923, 6525, 6555, 6763, 8837, 4348, 3210, 1741, 9636, 5158, 1553, 3273, 5406, 7447, 7799, 835, 1105, 2446, 6691, 7670, 7013, 5641, 3378, 8619, 5797, 1653, 228, 8553, 7742, 2903, 2315, 4592, 8179, 6603, 961, 4918, 8538, 9285, 8499, 6582, 5623, 7584, 39, 1076, 2770, 3536, 2326, 7601, 4577, 4334, 7974, 560, 8422, 2723, 3435, 4092, 7551, 1831, 7354, 718, 1182, 2739, 8349, 1048, 3524, 1275, 4452, 1292, 4647, 3385, 1528, 2690, 8408, 888, 7762, 9102, 19, 9521, 194, 7979, 5276, 4667, 41, 4790, 7205, 7148, 6861, 6821, 8580, 62, 6563, 5297, 6441, 8360, 3566, 813, 8692, 3793, 3022, 254, 8147, 9584, 4182, 2393, 3733, 9274, 920, 1448, 80, 3119, 2142, 7982, 8546, 9163, 8707, 8386, 8579, 8838, 9496, 8889, 1944, 2379, 1787, 5607, 3145, 1659, 591, 4892, 5373, 2217, 9251, 8459, 1018, 8612, 6350, 450, 2870, 53, 8182, 8184, 1639, 4616, 6014, 6013, 8329, 4991, 6639, 5291, 8552, 1720, 9560, 1143, 9132, 5452, 125, 5988, 9152, 1539, 2969, 3323, 8989, 1021, 7517, 5333, 4413, 5801, 7983, 5344, 8008, 6174, 1884, 2451, 2514, 1359, 6374, 662, 4979, 3901, 7357, 2403, 511, 35, 6823, 7161, 9097, 7214, 3706, 1370, 7855, 4169, 7771, 4163, 8973, 8031, 5234, 5450, 7156, 4512, 5823, 9588, 557, 8525, 44, 4338, 2491, 6288, 7384, 27, 67, 4766, 1680, 1705, 497, 8978, 6040, 7394, 921, 4582, 6859, 7693, 9684, 6803, 6917, 8664, 3136, 7659, 3226, 4778, 7408, 4047, 1322, 2983, 5285, 1141, 9055, 8984, 7709, 9159, 3919, 2428, 4735, 7665, 1012, 6905, 8936, 474, 1867, 7255, 9077, 2066, 8822, 8695, 1483, 6503, 493, 8959, 9179, 6589, 6816, 8257, 1640, 1848, 5697, 6928, 8381, 4023, 2295, 355, 5963, 5907, 2035, 3250, 2399, 1734, 392, 5049, 7210, 8315, 3045, 104, 1007, 9544, 264, 8708, 4836, 6202, 6335, 3826, 3637, 4218, 5897, 2170, 5000, 7418, 6815, 5254, 8793, 1379, 2305, 1971, 3539, 3920, 6364, 244, 5263, 4433, 1512, 6255, 5685, 3217, 4687, 6370, 8107, 1080, 863, 9268, 7003, 9563, 716, 7307, 8202, 903, 809, 1173, 8235, 6483, 5927, 4207, 4865, 3009, 3764, 6638, 5467, 847, 2599, 4110, 5251, 8764, 8539, 214, 472, 8272, 4519, 6587, 6222, 2665, 3732, 1838, 8723, 8330, 1047, 2891, 7224, 370, 3870, 8374, 3007, 5971, 4298, 9595, 9238, 3730, 7200, 8233, 5693, 5464, 3035, 6280, 5495, 3042, 858, 4373, 9331, 6250, 1311, 3146, 1197, 4179, 6617, 1100, 160, 5303, 9649, 8062, 3349, 853, 4133, 3988, 7159, 9230, 7728, 1886, 3177, 4675, 3731, 3268, 9540, 5774, 4287, 5444, 480, 8288, 356, 7465, 872, 4593, 530, 4791, 4171, 5502, 594, 830, 6052, 3760, 9216, 1926, 6902, 5763, 6813, 8491, 1752, 7024, 7934, 476, 2584, 7965, 2439, 1493, 4001, 607, 1732, 936, 2878, 9332, 6026, 4599, 7347, 5834, 3115, 2059, 2097, 3900, 6287, 445, 2226, 938, 8956, 268, 672, 8295, 4296, 9306, 2880, 5408, 1760, 6405, 4864, 134, 3684, 7623, 7267, 1179, 7753, 2093, 1759, 9614, 199, 2387, 3755, 4916, 3828, 2738, 1877, 8581, 7574, 3822, 8025, 2153, 318, 5684, 639, 6767, 6581, 1676, 1020, 9175, 6886, 7128, 6448, 9072, 4757, 4949, 3695, 6112, 6300, 7593, 2230, 1548, 9067, 3500, 596, 4336, 1336, 6274, 2130, 774, 7944, 7167, 4598, 2947, 6023, 5458, 9339, 3651, 7740, 7836, 2563, 8745, 7555, 8527, 8407, 5542, 4871, 2793, 2388, 113, 5857, 2251, 9290, 1940, 9677, 9090, 5784, 4357, 7573, 9529, 8176, 1700, 7460, 1314, 2168, 9123, 8105, 4187, 1226, 8120, 8710, 2125, 7199, 8633, 6601, 5353, 2414, 4240, 1669, 7006, 2149, 2338, 5885, 1260, 1664, 2269, 196, 3993, 8230, 801, 128, 556, 6768, 9183, 2745, 2465, 8139, 8298, 3591, 7801, 4197, 6663, 2271, 758, 3707, 8890, 8075, 7505, 6888, 3280, 5268, 5769, 9546, 3189, 86, 1707, 5833, 8983, 4183, 238, 7736, 3736, 4409, 5680, 3334, 4429, 7892, 8056, 332, 7042, 8157, 433, 7869, 2498, 6377, 8642, 6061, 2747, 1804, 2435, 4403, 4810, 6393, 7061, 2143, 6931, 9025, 5719, 2758, 5673, 1806, 2797, 4065, 2824, 9576, 2858, 2682, 3054, 6497, 5019, 9706, 7845, 4795, 2676, 151, 163, 3876, 9491, 649, 205, 4364, 6692, 4309, 5969, 43, 3872, 4479, 2701, 3359, 5745, 1893, 4689, 4421, 2018, 4144, 6138, 8429, 9154, 5933, 9656, 6518, 4930, 2046, 4410, 766, 2801, 6240, 1889, 6213, 1056, 1011, 197, 8211, 8748, 5847, 8628, 1868, 6990, 4057, 8346, 9686, 8479, 2362, 3478, 503, 5070, 1169, 5921, 2678, 1803, 8712, 8485, 6909, 6312, 8393, 4468, 8534, 6527, 3638, 79, 8164, 3317, 6453, 6734, 5794, 6486, 4213, 794, 8343, 1117, 9367, 9428, 2156, 9698, 6698, 965, 1568, 7854, 8919, 7743, 7936, 3003, 7735, 7691, 5346, 8952, 178, 7995, 1038, 6487, 5889, 6179, 4623, 72, 7273, 9065, 5105, 1219, 4572, 8803, 8818, 7231, 340, 5669, 4076, 2423, 7364, 6607, 5725, 3739, 1551, 8363, 3686, 2528, 13, 7294, 8029, 5301, 1330, 3240, 3523, 4544, 6101, 6580, 2784, 4257, 3521, 8351, 2653, 6788, 519, 4657, 1888, 2457, 5151, 7703, 7018, 8551, 3289, 4789, 8876, 5416, 4054, 7419, 2117, 6944, 6496, 8290, 733, 5315, 9536, 690, 8054, 7651, 9420, 932, 3663, 4020, 7711, 1994, 2546, 4321, 564, 998, 6808, 235, 6200, 5170, 5781, 1538, 8091, 2988, 9278, 9252, 9266, 5687, 4672, 5358, 8547, 627, 5869, 2412, 7059, 1196, 1991, 1644, 6654, 5052, 1543, 9155, 3153, 8039, 5744, 278, 443, 319, 3702, 3246, 5950, 580, 5754, 6392, 4078, 2559, 7142, 9323, 7111, 669, 317, 3112, 1353, 5551, 9063, 8737, 1339, 7655, 7913, 7405, 2336, 7318, 3765, 3423, 1013, 9564, 5692, 5067, 8953, 7827, 390, 5109, 6491, 4026, 667, 9426, 8884, 8946, 901, 6092, 4041, 1969, 1001, 508, 6276, 8589, 2627, 8994, 5676, 2839, 5951, 8068, 2413, 2067, 9427, 5753, 3885, 8340, 8411, 2040, 2486, 6319, 5934, 9199, 1885, 6528, 6468, 9666, 3766, 1748, 8752, 5021, 3605, 407, 6556, 966, 1168, 8469, 2986, 7870, 1441, 1127, 1183, 1452, 3402, 8342, 7401, 5347, 1505, 6378, 1321, 3849, 2128, 7959, 2579, 3156, 1531, 5570, 5660, 9167, 7626, 3853, 7385, 647, 2644, 5764, 8851, 8972, 1573, 7094, 8839, 3320, 8616, 6963, 7259, 4228, 2141, 6515, 5656, 818, 7525, 8970, 2767, 8210, 2292, 2725, 2688, 6521, 6064, 8488, 7009, 4426, 2372, 6035, 2332, 3485, 3196, 1778, 7908, 3725, 6831, 1381, 2868, 1348, 4032, 4998, 3811, 1256, 2948, 886, 4628, 5239, 955, 1354, 1618, 960, 261, 3953, 9419, 7353, 3105, 980, 4726, 9140, 1974, 3266, 6341, 5320, 7689, 1598, 7182, 2783, 1807, 7263, 750, 5958, 4784, 1342, 5817, 110, 4710, 4907, 6594, 361, 5736, 3642, 398, 2482, 9627, 1191, 1004, 1678, 6130, 3337, 1447, 5469, 4121, 5043, 1150, 4752, 869, 4388, 3671, 4736, 8648, 6539, 7991, 1253, 5336, 2612, 1144, 20, 4754, 3426, 2349, 487, 7567, 6011, 3272, 6916, 5120, 5360, 4931, 3563, 6157, 9435, 2160, 1845, 9630, 5454, 9352, 7358, 4323, 3974, 9054, 1492, 747, 1393, 5707, 5854, 2383, 7160, 617, 6090, 5190, 3942, 2273, 609, 1933, 2643, 8606, 2471, 9026, 8240, 2011, 9006, 5465, 1307, 333, 4046, 762, 6294, 3982, 535, 1189, 1003, 1558, 8702, 4140, 6739, 4709, 4188, 4809, 32, 1437, 3403, 2752, 2101, 3564, 8531, 1587, 1712, 7664, 7206, 1035, 7611, 3278, 8254, 6913, 9298, 5694, 7180, 492, 1155, 8251, 9262, 958, 8467, 727, 5138, 3377, 9447, 4842, 509, 7932, 3172, 6190, 190, 8017, 5728, 2887, 8974, 1925, 368, 22, 8979, 1276, 416, 5307, 8960, 1645, 4332, 875, 4406, 4999, 7726, 1708, 6738, 5893, 6499, 6770, 4954, 9134, 2364, 1909, 7098, 3122, 501, 7287, 1407, 9359, 553, 896, 5978, 2890, 7494, 883, 1058, 3254, 6421, 620, 352, 6643, 9014, 3296, 5232, 4862, 516, 247, 2193, 7115, 8322, 3692, 6284, 9364, 6223, 6307, 5267, 3084, 1575, 4803, 8076, 8829, 7361, 2659, 7290, 723, 7453, 6565, 2765, 2642, 172, 2565, 3109, 5549, 6912, 5493, 7898, 5989, 9007, 2361, 3088, 9322, 9113, 5026, 7154, 3087, 3380, 351, 2407, 3835, 5212, 1907, 5541, 7680, 5849, 8820, 8347, 6620, 6624, 2845, 5555, 7997, 935, 3388, 5999, 6373, 9501, 4902, 988, 5496, 7815, 2735, 2594, 7553, 233, 7930, 701, 6231, 2, 5094, 6041, 3572, 5785, 9073, 4337, 3249, 2263, 8253, 8124, 135, 6895, 9732, 5962, 3166, 3339, 2146, 8549, 7763, 8649, 6135, 6629, 5663, 3669, 2452, 9166, 2503, 8463, 1054, 1216, 5240, 3930, 2099, 1589, 5198, 7238, 5568, 6609, 1133, 2210, 3917, 8285, 6267, 7187, 5107, 3761, 4011, 8800, 5093, 7917, 7501, 5658, 2008, 8434, 814, 3527, 4100, 3678, 6132, 1613, 292, 8166, 8195, 7663, 8222, 3789, 4355, 5688, 5478, 4405, 7809, 8044, 3502, 5061, 7879, 8477, 3625, 1388, 4017, 8511, 897, 226, 905, 3329, 6669, 1858, 6261, 6893, 3292, 9488, 8067, 713, 3618, 8542, 4341, 856, 834, 1392, 7533, 4191, 9587, 1956, 6074, 1765, 3924, 6467, 4965, 9578, 8961, 5116, 4896, 7985, 5705, 8137, 6434, 3237, 4374, 8971, 4952, 6232, 6904, 4147, 9235, 2743, 9048, 8938, 7078, 2914, 3251, 3863, 1554, 8861, 9675, 1094, 217, 7946, 6428, 7536, 4150, 5290, 1722, 2043, 3019, 1860, 5394, 8656, 7610, 8102, 7609, 136, 7233, 8615, 3635, 33, 3396, 3945, 710, 8131, 9242, 7141, 386, 2239, 7250, 8160, 2951, 7152, 2359, 5703, 1841, 5965, 4521, 5076, 8437, 6413, 6191, 859, 1727, 868, 7026, 8988, 2721, 8489, 4851, 8678, 4874, 2259, 8337, 6849, 4229, 6566, 4427, 1503, 3452, 9319, 5325, 381, 545, 2610, 1106, 4986, 6576, 6915, 9310, 2827, 7989, 929, 3537, 7038, 7468, 3395, 1743, 8722, 1418, 140, 4037, 9177, 4024, 9107, 3530, 7838, 8375, 9267, 1153, 5861, 4797, 1090, 517, 4159, 6873, 4474, 6881, 4328, 4933, 5638, 3285, 8430, 3354, 460, 5218, 8220, 6375, 4192, 2308, 7158, 1036, 100, 7526, 9480, 4506, 6458, 5058, 5645, 3963, 118, 4094, 2933, 1120, 4038, 628, 6355, 4959, 6619, 8785, 3985, 5371, 6368, 4397, 1985, 9192, 6127, 3351, 8289, 7699, 2171, 2496, 3836, 7751, 3972, 9062, 8760, 3211, 4792, 3580, 8307, 8073, 4698, 1488, 8169, 6329, 2844, 3620, 7744, 5490, 2524, 6131, 2350, 207, 7411, 2161, 9036, 1028, 8435, 5415, 3598, 3081, 684, 2730, 4167, 5270, 9625, 6493, 455, 5096, 1361, 3279, 5954, 7458, 962, 9687, 3264, 9255, 3037, 4702, 5455, 8909, 4747, 8189, 8597, 6812, 7775, 7456, 4580, 6551, 7162, 1873, 3603, 5905, 2583, 8887, 5835, 2660, 5742, 2206, 9371, 5261, 1235, 2619, 3096, 1252, 7741, 7748, 1532, 4232, 518, 5757, 1851, 6664, 6152, 1425, 1614, 9589, 9185, 4248, 8185, 9434, 5446, 2036, 1972, 6206, 4230, 6598, 7620, 8097, 5335, 887, 376, 7317, 9711, 8667, 9078, 5316, 3947, 4734, 9697, 2261, 3649, 3889, 9453, 614, 6239, 9551, 82, 435, 243, 4186, 5655, 2572, 9311, 471, 122, 9263, 5790, 8611, 3611, 6559, 7572, 7696, 7543, 668, 9030, 38, 2218, 3504, 8256, 7624, 6824, 8410, 3875, 680, 8590, 5722, 3314, 8802, 4308, 7140, 5520, 5771, 7534, 778, 3190, 9360, 2896, 572, 206, 9382, 7462, 9541, 6275, 3713, 5839, 8724, 3639, 7683, 3830, 7251, 4446, 5576, 7746, 2047, 9440, 8350, 3696, 3304, 8577, 1170, 1249, 2320, 527, 7279, 7686, 4728, 6212, 2798, 8744, 9690, 8864, 7874, 1461, 4814, 7247, 6455, 2595, 9462, 5850, 2158, 3491, 6285, 8731, 7201, 5727, 4828, 9461, 8495, 5787, 3744, 5519, 6396, 5015, 6433, 8085, 9121, 6016, 4424, 2029, 3447, 1062, 5453, 1251, 5161, 9245, 9641, 5130, 7539, 4566, 7336, 2411, 7138, 1917, 922, 4805, 7841, 2849, 1467, 3406, 298, 449, 1187, 7314, 704, 9532, 1827, 8942, 396, 2020, 2321, 6615, 9629, 7281, 4956, 810, 9731, 227, 2917, 2039, 3747, 1159, 229, 571, 3061, 923, 5038, 1212, 7857, 5328, 8304, 954, 3358, 783, 1231, 412, 864, 2940, 2547, 4408, 7818, 5489, 5615, 4772, 6279, 9556, 3223, 8392, 1061, 286, 91, 4601, 5385, 1620, 5902, 4407, 2925, 8840, 593, 9181, 367, 7905, 6754, 9669, 4776, 4617, 6953, 8092, 3416, 6361, 5436, 8544, 6207, 5558, 154, 8376, 89, 4685, 771, 3927, 5920, 6979, 4712, 9590, 1874, 8354, 4127, 6048, 5361, 5434, 5211, 1217, 665, 7064, 8452, 5037, 565, 1227, 8743, 4109, 989, 3070, 4075, 1400, 382, 1309, 9550, 6233, 7016, 6297, 8749, 8969, 5890, 8110, 606, 7060, 5156, 3198, 8175, 369, 2070, 8927, 3180, 5277, 6409, 7810, 5642, 3437, 8475, 3445, 9444, 5750, 8345, 8685, 8863, 8896, 9105, 849, 1132, 4491, 8055, 844, 7125, 1600, 2679, 4743, 6304, 619, 2805, 9549, 8807, 8046, 4066, 3903, 5864, 3463, 1135, 9228, 7907, 3399, 9609, 7413, 164, 6044, 9626, 9478, 5145, 5491, 6404, 1577, 4626, 6685, 287, 6671, 1731, 4847, 1580, 6704, 8687, 7636, 4184, 6401, 310, 3160, 5532, 5830, 1500, 1097, 5002, 681, 8834, 2026, 9335, 3861, 3778, 9410, 652, 8593, 6465, 1282, 2262, 5935, 6349, 8226, 6197, 5896, 5425, 7095, 5796, 2663, 2003, 1024, 4458, 3199, 2461, 1650, 7284, 99, 2266, 6764, 6359, 6869, 421, 1510, 9726, 7219, 706, 6437, 1820, 7953, 4190, 1570, 660, 7542, 8916, 181, 4804, 5439, 9531, 8049, 1474, 2968, 7333, 6385, 9116, 6116, 2007, 9608, 1522, 1365, 7507, 3748, 350, 2115, 6657, 7871, 1040, 3785, 4224, 1077, 8170, 1287, 4973, 4156, 3400, 9520, 6994, 4226, 314, 149, 8827, 5031, 5168, 6843, 735, 5180, 8311, 6498, 7585, 2252, 8671, 360, 3495, 2475, 2853, 5376, 6923, 2427, 686, 3046, 8569, 752, 7803, 6333, 9351, 1911, 1710, 8238, 2497, 2549, 6363, 2445, 8735, 5305, 2624, 4654, 2257, 2092, 469, 9135, 4663, 5807, 6978, 2154, 6794, 88, 2052, 2228, 3624, 5309, 9189, 6050, 399, 6758, 529, 2623, 2702, 5980, 2109, 1147, 8595, 3175, 5777, 8443, 7328, 1247, 7011, 1000, 1427, 2848, 5977, 700, 3795, 6059, 5940, 5634, 8158, 2159, 4996, 537, 6107, 4008, 6114, 1499, 9516, 437, 769, 8418, 5177, 6479, 6796, 7472, 4988, 9013, 4356, 7785, 5773, 3133, 5960, 8571, 6241, 7046, 5783, 568, 5204, 4277, 8301, 5142, 7034, 107, 2114, 2764, 6857, 4945, 6590, 2448, 9519, 3646, 7851, 2742, 5174, 9379, 711, 9211, 5042, 7599, 2548, 2476, 4090, 2178, 3167, 7994, 6215, 9574, 4640, 274, 1300, 363, 1113, 2352, 6323, 4571, 780, 6883, 4533, 7896, 4302, 9570, 5359, 2763, 9317, 1607, 4656, 3950, 3721, 8697, 6180, 5241, 3543, 8332, 3492, 4624, 4450, 7952, 6545, 2960, 5173, 3862, 5711, 1936, 4819, 2027, 2522, 6381, 5552, 4558, 9141, 3622, 9559, 2695, 7313, 8420, 6985, 5592, 6564, 7398, 3807, 9695, 2236, 1894, 266, 5497, 2527, 124, 8279, 4214, 6399, 1767, 2648, 4379, 5506, 729, 754, 9330, 6356, 6081, 6164, 9358, 7569, 2501, 3142, 981, 9362, 2155, 5741, 865, 8809, 4925, 6177, 9031, 3192, 7424, 5473, 8064, 8119, 6542, 7050, 914, 4825, 772, 3281, 6877, 2761, 8397, 5369, 3425, 1574, 3214, 8394, 2386, 8631, 8975, 8613, 5384, 8771, 8940, 1204, 4897, 8024, 6156, 6417, 5908, 9605, 2881, 3484, 3726, 3413, 4813, 2799, 78, 7171, 6858, 7041, 3894, 6975, 5620, 4058, 5569, 2010, 3118, 6546, 6348, 1175, 6163, 6175, 2993, 9448, 2098, 502, 1291, 4881, 1814, 1978, 6703, 3712, 5085, 8099, 6572, 9481, 9299, 3407, 2484, 8662, 790, 6868, 4212, 1345, 5515, 3261, 7933, 1898, 9138, 3783, 5587, 7704, 5147, 7105, 2772, 8536, 5189, 1257, 963, 4035, 5014, 5747, 1303, 4920, 1749, 4051, 6484, 9124, 7804, 1703, 9421, 3040, 2973, 9464, 5842, 3498, 9610, 6147, 4077, 3204, 8874, 8814, 6415, 1591, 2921, 1124, 1561, 3980, 2713, 2700, 1255, 58, 1935, 5154, 9597, 7732, 7981, 6583, 4219, 15, 3138, 561, 8114, 9600, 5746, 7919, 7329, 3606, 9143, 9091, 3130, 1250, 5503, 4926, 7814, 2633, 1083, 3983, 6541, 8047, 9215, 6154, 1556, 6419, 970, 4004, 3149, 3682, 2897, 9396, 2876, 5545, 3404, 9517, 9240, 4644, 2264, 6991, 6835, 6282, 2581, 157, 1042, 4575, 3517, 4660, 6523, 4394, 4010, 6853, 8423, 1784, 6221, 1409, 2353, 2521, 1785, 4131, 4367, 8504, 8009, 2656, 7025, 2247, 3771, 1389, 9249, 1138, 6842, 2074, 8248, 1098, 4166, 7269, 6908, 3330, 1399, 8264, 4151, 1813, 534, 7375, 7594, 2615, 5006, 1295, 515, 8779, 7914, 2024, 7660, 3784, 9341, 5513, 4200, 158, 155, 9224, 4113, 2013, 2001, 9658, 3442, 9591, 7537, 4968, 7461, 8847, 1264, 3154, 5284, 1398, 5898, 9511, 4299, 876, 3298, 7184, 3336, 2543, 615, 5404, 3929, 4333, 6266, 7422, 4540, 793, 2420, 5597, 5571, 5079, 8720, 177, 721, 893, 2477, 9473, 6744, 5471, 3071, 9024, 1696, 8433, 4128, 7483, 2736, 862, 5367, 6711, 9592, 2726, 7073, 8926, 161, 7022, 7521, 3949, 4701, 5205, 6968, 7588, 3568, 429, 6117, 8680, 2334, 5311, 857, 5414, 7277, 5723, 5868, 2116, 1967, 7093, 4250, 225, 9318, 4507, 8150, 5137, 579, 7643, 7780, 3628, 3147, 4873, 3852, 8050, 6227, 6173, 2556, 1821, 917, 316, 7700, 1312, 137, 3469, 1301, 8713, 4469, 4135, 9069, 330, 6436, 1002, 5484, 2511, 2954, 10, 8126, 946, 3494, 2777, 1331, 7993, 8575, 9342, 1481, 8877, 7825, 2094, 4770, 1730, 5900, 2590, 9542, 4724, 8037, 9222, 5987, 5340, 9061, 6142, 6962, 168, 9119, 1134, 2405, 5699, 2979, 1855, 8670, 2926, 5626, 8108, 7421, 9070, 3067, 306, 1823, 3453, 7925, 5860, 231, 8655, 1697, 8277, 8862, 7188, 2307, 315, 5432, 9508, 4614, 996, 3424, 6885, 1590, 3668, 2367, 2166, 180, 4970, 6249, 444, 1941, 4402, 7840, 8898, 6735, 6832, 7345, 3551, 6894, 4466, 477, 8421, 6710, 1111, 6427, 6850, 1687, 3092, 7777, 6094, 3121, 2389, 5088, 8910, 6997, 6169, 7363, 3661, 2987, 7131, 6814, 8468, 8915, 1156, 4554, 4574, 2444, 3183, 8244, 6611, 2978, 1564, 722, 4059, 2792, 3258, 2814, 5540, 3864, 2376, 242, 1957, 3923, 5533, 383, 1384, 3960, 1576, 2693, 7226, 2466, 7938, 8543, 4084, 3584, 1729, 96, 3698, 465, 423, 5022, 2834, 8901, 8913, 8924, 4236, 3966, 3476, 4448, 9001, 9515, 4019, 9186, 2014, 9103, 9023, 4266, 7524, 1850, 9655, 3652, 7213, 7299, 8355, 3660, 248, 2260, 7090, 8996, 4961, 9685, 6087, 1692, 1244, 1215, 9736, 4612, 8140, 7547, 559, 485, 5582, 4739, 4562, 624, 7971, 9510, 186, 4763, 3270, 7202, 5598, 7621, 347, 2907, 7482, 1085, 2366, 9598, 342, 4467, 7276, 6507, 21, 3560, 4349, 9304, 8183, 8869, 3633, 5564, 8161, 3048, 3117, 495, 2209, 55, 2621, 1987, 2335, 9361, 3511, 803, 3429, 3710, 6505, 7123, 5523, 6265, 9042, 7895, 3959, 826, 7049, 5579, 9527, 8142, 1308, 7826, 3139, 3008, 7616, 1401, 6420, 9269, 6306, 702, 187, 3287, 5027, 1908, 8239, 8636, 3902, 1631, 7486, 8308, 8358, 4872, 4423, 3477, 7434, 8011, 7489, 6561, 5269, 5233, 4721, 3236, 2941, 7229, 49, 8653, 8912, 8015, 6471, 505, 1198, 1892, 2365, 4596, 4293, 4695, 8250, 8660, 7450, 2401, 4366, 7393, 6522, 2300, 7356, 6264, 626, 3417, 993, 4437, 3350, 6825, 1770, 7619, 2231, 7761, 4314, 1986, 8103, 5689, 6305, 8096, 1745, 9288, 8066, 5155, 8679, 4243, 6852, 671, 1660, 3137, 8361, 359, 6289, 3010, 3743, 2712, 8136, 1320, 4386, 7157, 6936, 3102, 5652, 4161, 5185, 1545, 4885, 9705, 8069, 1671, 9458, 1288, 1081, 703, 6118, 6610, 6752, 4845, 5881, 3714, 6989, 8209, 6298, 8514, 2646, 6971, 92, 8023, 2776, 5319, 6721, 8856, 5089, 6670, 7731, 6751, 4899, 4967, 1635, 5391, 4555, 8206, 7779, 3245, 919, 2882, 1031, 6501, 4107, 3055, 173, 4807, 7842, 574, 773, 5505, 8714, 871, 2096, 542, 8832, 8366, 9673, 5128, 9202, 9011, 5911, 8223, 8545, 1457, 4235, 6896, 6560, 3893, 8682, 4245, 3886, 4233, 510, 2139, 4496, 3913, 5160, 7435, 6286, 7765, 8303, 3434, 595, 4831, 9691, 2578, 2911, 845, 2952, 309, 730, 8296, 5700, 2709, 311, 9294, 2750, 6725, 5735, 2186, 6977, 5247, 384, 1371, 6346, 5264, 2938, 4297, 2842, 4578, 7606, 3825, 8132, 2487, 1501, 5220, 3648, 4118, 6887, 5429, 1943, 6077, 539, 2082, 8610, 2316, 3602, 7789, 9467, 5596, 3106, 1900, 1063, 1719, 5967, 3814, 8805, 2804, 9568, 3812, 7220, 98, 1498, 473, 8447, 2532, 6543, 1152, 7062, 2038, 9229, 2674, 2085, 9557, 1819, 6028, 3578, 5640, 9349, 8556, 964, 4670, 8841, 1585, 2474, 8646, 3922, 6708, 2507, 6096, 2821, 7984, 6748, 775, 2923, 5428, 2756, 590, 4552, 7143, 8789, 8995, 249, 1087, 1691, 3260, 3041, 2375, 7106, 8312, 4730, 3912, 7717, 7146, 8965, 6512, 4661, 3623, 5853, 5836, 377, 1789, 6840, 698, 2299, 8658, 5295, 224, 489, 8630, 2470, 2934, 8359, 3489, 8123, 258, 2327, 1072, 8431, 1546, 7339, 6829, 1683, 6299, 8035, 2781, 7511, 5280, 768, 1372, 6844, 7331, 3274, 6570, 6004, 3553, 3986, 9207, 7208, 2134, 5071, 3412, 8204, 3996, 5710, 4691, 466, 2369, 7829, 8848, 5805, 1966, 5894, 8566, 2152, 9489, 1673, 7395, 8945, 2803, 4531, 8384, 4264, 6068, 7835, 9703, 6661, 2112, 1699, 2342, 6693, 8060, 2492, 5260, 8599, 6986, 6476, 6036, 7796, 6489, 2144, 8021, 5435, 8572, 9153, 8493, 8500, 7322, 3100, 1200, 2515, 9720, 4530, 2016, 1118, 6009, 1224, 1157, 2505, 165, 8255, 2306, 6129, 3587, 3148, 4801, 1863, 9392, 1795, 1686, 334, 428, 8053, 6961, 5046, 8777, 54, 942, 3754, 2495, 3176, 7499, 9170, 2381, 9523, 2021, 3006, 657, 6444, 343, 3690, 7972, 2331, 4395, 7942, 6454, 3483, 8470, 6088, 6818, 8321, 6787, 4884, 984, 2863, 8813, 4387, 7676, 6167, 7369, 7632, 4922, 5400, 6062, 7055, 8990, 7492, 2377, 866, 4265, 7587, 4860, 1578, 2127, 5249, 5629, 8388, 4105, 3531, 6958, 5537, 2249, 6136, 7258, 2180, 484, 7211, 2816, 5265, 9187, 4525, 1471, 4960, 7604, 9583, 7467, 8267, 8003, 6680, 5045, 2593, 7839, 6538, 3387, 2442, 8681, 2555, 2517, 5134, 842, 171, 3715, 8457, 8129, 6918, 6045, 3855, 8341, 1781, 2124, 3441, 2698, 7117, 3507, 8701, 8473, 678, 6466, 4940, 8155, 5739, 1477, 7019, 4651, 8509, 9380, 2240, 9108, 4615, 2006, 708, 95, 693, 7368, 5060, 4758, 4570, 2596, 322, 1808, 4711, 3215, 3025, 7070, 1478, 7722, 956, 1597, 8739, 6509, 5591, 4442, 7734, 6024, 2123, 9633, 2071, 3933, 3869, 9601, 7043, 5584, 4706, 2061, 8557, 2201, 1801, 7608, 8783, 799, 573, 3294, 4832, 599, 5501, 4177, 8759, 2409, 9707, 2330, 7885, 3866, 30, 767, 3749, 4339, 3969, 3451, 7733, 5055, 5880, 1774, 578, 4664, 7481, 1071, 5208, 694, 4497, 8390, 3075, 3456, 6774, 6268, 3418, 6155, 3171, 1879, 2030, 2103, 2324, 7963, 5488, 7374, 8765, 276, 512, 691, 411, 1735, 6706, 1718, 8761, 7951, 7155, 3672, 598, 6946, 257, 6529, 1108, 3321, 1593, 6757, 6696, 5223, 5377, 1161, 324, 4139, 273, 5734, 239, 1417, 959, 9490, 912, 4696, 4648, 2417, 312, 7843, 5995, 1996, 34, 255, 4344, 7770, 9683, 1358, 365, 6838, 2802, 9613, 8177, 4655, 8249, 7500, 9122, 6229, 8403, 1656, 230, 5516, 6637, 9276, 815, 5302, 2089, 3541, 6311, 8333, 3693, 7755, 251, 1665, 2618, 8453, 7144, 216, 6320, 3128, 3056, 9699, 7774, 4993, 1074, 7127, 3202, 8541, 1945, 7918, 2075, 2788, 3964, 7967, 1055, 3858, 6707, 4543, 1230, 3516, 2754, 2281, 3860, 4284, 415, 550, 9198, 4471, 3050, 6658, 4259, 2658, 5381, 925, 9411, 4606, 1414, 8817, 4723, 5924, 785, 625, 1344, 1095, 8943, 6855, 5139, 5231, 3520, 8955, 5756, 4526, 7346, 3756, 4225, 3997, 18, 5840, 7864, 7541, 3120, 3069, 6647, 738, 1833, 1026, 4777, 2683, 2932, 467, 8286, 6578, 6391, 756, 4806, 9369, 7618, 4731, 3497, 6864, 3898, 4542, 8518, 7028, 9157, 8560, 7909, 1517, 2961, 8845, 6353, 7139, 5321, 4725, 7261, 4976, 2935, 2291, 3595, 8266, 4668, 7860, 3808, 4629, 8502, 3776, 358, 5948, 3479, 548, 7964, 784, 8324, 6592, 2628, 5183, 5004, 3880, 4550, 5110, 3608, 7172, 182, 4611, 4741, 8503, 1733, 281, 2019, 1654, 7508, 2553, 1378, 7931, 9528, 3848, 2426, 4330, 9217, 3023, 613, 5296, 7066, 742, 4738, 7147, 7902, 3299, 5872, 75, 3773, 8078, 8258, 5816, 5053, 8234, 5383, 643, 328, 3505, 1963, 4557, 4376, 792, 4534, 3383, 5832, 5380, 7425, 8389, 8382, 4824, 9312, 1605, 6687, 2906, 1017, 6039, 695, 6820, 2163, 2200, 1526, 8236, 8875, 5005, 5943, 1552, 6145, 5724, 1010, 9052, 6424, 2913, 153, 6933, 6980, 1459, 5486, 4684, 3060, 6771, 59, 8852, 1408, 8954, 1846, 3834, 5972, 5955, 8336, 2073, 9075, 8291, 8529, 1202, 6608, 7040, 3936, 7335, 6974, 2122, 3353, 9002, 6457, 2102, 740, 4994, 7116, 7388, 877, 1422, 8690, 295, 8168, 40, 8417, 2091, 3670, 2520, 3840, 4480, 2716, 1206, 7975, 3225, 371, 8935, 7725, 5066, 9180, 8931, 4938, 2395, 6828, 3444, 1663, 9302, 4022, 2542, 2946, 1584, 9395, 1623, 8436, 4704, 3798, 5928, 3259, 4475, 8751, 6226, 409, 5709, 5123, 6889, 4370, 3868, 6981, 3026, 5181, 5023, 3163, 475, 9665, 16, 6071, 9372, 7089, 8858, 7099, 4329, 8318, 8432, 644, 3644, 9449, 6185, 8564, 5087, 4722, 4646, 8395, 9378, 3018, 2192, 6799, 1261, 5926, 3224, 9718, 3699, 4732, 8156, 3877, 4439, 2535, 8481, 2065, 9735, 9273, 2222, 3333, 4680, 2464, 5200, 7197, 1899, 4523, 339, 6134, 7436, 2734, 3239, 9377, 5565, 4160, 1763, 3063, 1569, 1485, 481, 5879, 2666, 5191, 3357, 4782, 9125, 7520, 641, 6438, 6789, 531, 8224, 4787, 985, 7440, 1881, 5531, 1519, 8486, 6376, 1404, 4603, 3787, 5636, 8632, 6418, 7966, 6106, 6514, 709, 1473, 5851, 8574, 3324, 4154, 7916, 5945, 4398, 7504, 2654, 2794, 6140, 1324, 7723, 4282, 7455, 5099, 9118, 5112, 715, 6567, 8191, 5821, 1780, 8061, 3829, 5476, 2807, 7248, 4882, 1112, 654, 4665, 4569, 8716, 6220, 880, 952, 9632, 3468, 6263, 4746, 3104, 7784, 7583, 2939, 5201, 2977, 1351, 3838, 5574, 4252, 2169, 7479, 9538, 9081, 6108, 2867, 6301, 159, 4748, 1630, 1997, 3937, 133, 1413, 7045, 1864, 3590, 1853, 202, 2841, 7245, 8483, 8763, 8592, 193, 2655, 8252, 
Train Sentence: 499
Number of candidates => 1559
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 1559
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:46:02 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:46:02 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:02 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 2152
...+...+...+...+...500
            ...+...+...+...+...1000
            ...+...+...+...+...1500
            ..
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2152 distinct candidates (about 1 per sentence):
newCandidatesAdded[it=1] = 2152 (about 1 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.8675691466570294)

Final lambda[j=1]: {0.1, -0.0688740706513941, -0.14168793510624172}
(Final BLEU[j=1]: 0.8739834464823246)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7913473104828488)

Final lambda[j=2]: {-0.03732421184965656, -0.6577977590753961, -0.868670251397469}
(Final BLEU[j=2]: 0.8734129183618726)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.8562365411498122)

Final lambda[j=3]: {0.054751771757673665, -0.038204817119486735, -0.07846689818987947}
(Final BLEU[j=3]: 0.8739834464823246)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.8630503712727439)

Final lambda[j=4]: {-0.01154781009884034, -0.8743327758402839, -0.6659467618978698}
(Final BLEU[j=4]: 0.8759666873721603)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.8673236027165451)

Final lambda[j=5]: {0.534117603783562, -0.37165225850864525, -0.7600599648431701}
(Final BLEU[j=5]: 0.8739834464823246)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.7928743177245839)

Final lambda[j=6]: {0.07041038200512698, -0.0517190016750889, -0.09960764128602158}
(Final BLEU[j=6]: 0.8739834464823246)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.8663053403785753)

Final lambda[j=7]: {0.6551499436674544, -0.45253169369211066, -0.9330111163449395}
(Final BLEU[j=7]: 0.8739834464823246)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.795851466450815)

Final lambda[j=8]: {2.425280256942088, -1.781457082028321, -3.430977633866071}
(Final BLEU[j=8]: 0.8739834464823246)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.8343483265437591)

Final lambda[j=9]: {-0.009374350437743963, -0.7104443972703325, -0.5412507120659531}
(Final BLEU[j=9]: 0.8759666873721603)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.8711670678555464)

Final lambda[j=10]: {0.6199524492797865, -0.46972627850199045, -0.8739358729264859}
(Final BLEU[j=10]: 0.8739834464823246)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.8706393734197323)

Final lambda[j=11]: {-0.022758570764140784, -0.399205792984231, -0.5477913765324938}
(Final BLEU[j=11]: 0.8734129183618726)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.7996855816408774)

Final lambda[j=12]: {0.45923733938724864, -0.3203282639502383, -0.6582595044369242}
(Final BLEU[j=12]: 0.8739834464823246)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.8645103772321665)

Final lambda[j=13]: {0.5727734104031035, -0.4207230259015947, -0.8102868749874037}
(Final BLEU[j=13]: 0.8739834464823246)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.7967769551680864)

Final lambda[j=14]: {0.17371525727904105, -0.12760021216116993, -0.24575022234220723}
(Final BLEU[j=14]: 0.8739834464823246)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.7876897753821381)

Final lambda[j=15]: {0.011294758251590459, -0.008296413175136552, -0.015978385520686908}
(Final BLEU[j=15]: 0.8739834464823246)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.8635717009654293)

Final lambda[j=16]: {0.48834639936694535, -0.3655494526454033, -0.6893751690924723}
(Final BLEU[j=16]: 0.8739834464823246)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.8609073797037813)

Final lambda[j=17]: {-0.011206646898739566, -0.8495287747458757, -0.647255237911446}
(Final BLEU[j=17]: 0.8759666873721603)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.8595124531968952)

Final lambda[j=18]: {0.8015997429828848, -0.5593539518361967, -1.1487930705886265}
(Final BLEU[j=18]: 0.8739834464823246)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.8530914257899511)

Final lambda[j=19]: {0.13766496177394233, -0.0995180158116756, -0.19509627251570538}
(Final BLEU[j=19]: 0.8739834464823246)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:46:03 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.8624074875940174)

Final lambda[j=20]: {-0.010583999886555059, -0.8022503514522665, -0.6112185836509705}
(Final BLEU[j=20]: 0.8759666873721603)

Best final lambda is lambda[j=4] (BLEU: 0.8760).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:46:03 JST 2015  ---

Next iteration will decode with lambda: {-0.01154781009884034, -0.8743327758402839, -0.6659467618978698}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:46:03 JST 2015 ---
Redecoding using weight vector {-0.01154781009884034, -0.8743327758402839, -0.6659467618978698}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:04 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+...+...+...+...500
            ...+...+...+...+...1000
            ...+...+...+...+...1500
            ..
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2152 distinct candidates (about 1 per sentence):
newCandidatesAdded[it=1] = 2152 (about 1 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:46:04 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:46:04 JST 2015
----------------------------------------------------

FINAL lambda: {-0.01154781009884034, -0.8743327758402839, -0.6659467618978698} (BLEU: 0.8759666873721603)

(OP Lamda) : [-0.01154781009884034,-0.8743327758402839,-0.6659467618978698]
Number of candidates => 908
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 908
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:46:05 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:46:05 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:05 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 9061
...+...+...+...+...500
            ...+...+...+...+
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 9061 distinct candidates (about 9 per sentence):
newCandidatesAdded[it=1] = 9061 (about 9 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:46:07 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.798749182691751)

Final lambda[j=1]: {0.1, 0.2, -0.1}
(Final BLEU[j=1]: 0.798749182691751)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:46:07 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7446121824550415)

Final lambda[j=2]: {-0.020819481130564022, -0.6577977590753961, -0.18068152473576585}
(Final BLEU[j=2]: 0.7968180363067656)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:46:07 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7852795600615127)

Final lambda[j=3]: {0.05838589600434535, 0.08547906555934875, -0.059473491599185246}
(Final BLEU[j=3]: 0.7985366367545012)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:46:07 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7809550289350784)

Final lambda[j=4]: {0.08229017261951611, 0.78330692119097, -0.24108207325578457}
(Final BLEU[j=4]: 0.7997143146950412)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:46:07 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.7944225599325724)

Final lambda[j=5]: {0.5553625547827716, 0.8314955687484247, -0.5778133794884044}
(Final BLEU[j=5]: 0.7985366367545012)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:46:07 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.754855684798522)

Final lambda[j=6]: {0.2815800420089276, 0.4153816484238994, -0.2886661877185727}
(Final BLEU[j=6]: 0.7985366367545012)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:46:07 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.7897974106994814)

Final lambda[j=7]: {0.32669861662639876, 3.005165592852067, -0.9330111163449395}
(Final BLEU[j=7]: 0.7995018718686684)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:46:07 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.7563297886050386)

Final lambda[j=8]: {0.6999162790884548, 1.4456071239041473, -0.6985483761532648}
(Final BLEU[j=8]: 0.798749182691751)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:46:07 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7687924117358511)

Final lambda[j=9]: {0.0027220358695327846, 0.02606952122189273, -0.01427276865927607}
(Final BLEU[j=9]: 0.7993196054876964)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:46:07 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.7935135410100818)

Final lambda[j=10]: {0.5365436362562377, 1.4331251154143683, -0.8739358729264859}
(Final BLEU[j=10]: 0.7986331033955101)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:46:08 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.7910683308342806)

Final lambda[j=11]: {0.16643408895703876, 0.9938966470720727, -0.5477913765324938}
(Final BLEU[j=11]: 0.7988757696358755)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:46:08 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.76031446459273)

Final lambda[j=12]: {0.5845257475301335, 0.8859710977494706, -0.6147411371985259}
(Final BLEU[j=12]: 0.7985366367545012)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:46:08 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7892665334382815)

Final lambda[j=13]: {0.7818755498145629, 1.1679013909013916, -0.8102868749874037}
(Final BLEU[j=13]: 0.7985366367545012)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:46:08 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.7606724150743297)

Final lambda[j=14]: {0.04092445215809372, 0.08288025530417939, -0.04092227334361069}
(Final BLEU[j=14]: 0.798749182691751)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:46:08 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.7505303062395406)

Final lambda[j=15]: {0.01673024458901933, 0.02541025170060851, -0.017516590664802104}
(Final BLEU[j=15]: 0.7985366367545012)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:46:08 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.793107520960052)

Final lambda[j=16]: {0.6576106932759427, 0.9890106099039098, -0.6893751690924723}
(Final BLEU[j=16]: 0.7985366367545012)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:46:08 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7792665722035442)

Final lambda[j=17]: {0.08368686172412391, 0.5665391089543363, -0.3964594524802063}
(Final BLEU[j=17]: 0.7988757696358755)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:46:08 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7840696244254811)

Final lambda[j=18]: {0.8318914624517211, 1.249890291562954, -0.8706362780408492}
(Final BLEU[j=18]: 0.7985366367545012)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:46:08 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7833640984012202)

Final lambda[j=19]: {0.033023166603961686, 0.312225655297118, -0.09544878730861298}
(Final BLEU[j=19]: 0.7993378767718742)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:46:08 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7832365237787844)

Final lambda[j=20]: {0.06506079377548019, 0.6217618898411658, -0.33852947919372234}
(Final BLEU[j=20]: 0.7995320800134341)

Best final lambda is lambda[j=4] (BLEU: 0.7997).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:46:08 JST 2015  ---

Next iteration will decode with lambda: {0.08229017261951611, 0.78330692119097, -0.24108207325578457}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:46:08 JST 2015 ---
Redecoding using weight vector {0.08229017261951611, 0.78330692119097, -0.24108207325578457}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:09 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+...+...+...+...500
            ...+...+...+...+
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 9061 distinct candidates (about 9 per sentence):
newCandidatesAdded[it=1] = 9061 (about 9 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:46:10 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:46:10 JST 2015
----------------------------------------------------

FINAL lambda: {0.08229017261951611, 0.78330692119097, -0.24108207325578457} (BLEU: 0.7997143146950412)

(OP Lamda) : [0.08229017261951611,0.78330692119097,-0.24108207325578457]
Number of candidates => 608
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 608
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:46:10 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:46:10 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:10 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 3143
...+...+...+...+...500
            ...+
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 3143 distinct candidates (about 5 per sentence):
newCandidatesAdded[it=1] = 3143 (about 5 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.7821626576133575)

Final lambda[j=1]: {0.03614028860386524, -0.42277123174578896, -0.1}
(Final BLEU[j=1]: 0.7894391208260589)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7586874915775716)

Final lambda[j=2]: {0.02029062761299608, -0.25753606284288494, -0.06315278985858824}
(Final BLEU[j=2]: 0.7896070867732485)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.783777486923842)

Final lambda[j=3]: {-0.004808918191418617, -0.5985554007406735, -0.059473491599185246}
(Final BLEU[j=3]: 0.7869611015697573)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7795321314587885)

Final lambda[j=4]: {0.10794441649321274, -0.8743327758402839, -0.17275813697787396}
(Final BLEU[j=4]: 0.78907155028864)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.7789679771006734)

Final lambda[j=5]: {0.7737957524668755, -8.822793672983476, -2.129359961545965}
(Final BLEU[j=5]: 0.7894391208260589)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.7598095764738335)

Final lambda[j=6]: {0.21351014693687353, -1.7398764402921971, -0.33968857807853114}
(Final BLEU[j=6]: 0.78907155028864)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.7798034777273324)

Final lambda[j=7]: {0.2996316942381058, -3.8030741760917874, -0.9330111163449395}
(Final BLEU[j=7]: 0.7896070867732485)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.7594679140061515)

Final lambda[j=8]: {0.6515944996339995, -7.112534807460591, -1.7316719283792361}
(Final BLEU[j=8]: 0.7894391208260589)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.771646735724082)

Final lambda[j=9]: {0.0044941587938917785, -0.0570184425341697, -0.013984840733135351}
(Final BLEU[j=9]: 0.7896070867732485)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.7811651442911866)

Final lambda[j=10]: {0.5053771649119221, -4.135427763861448, -0.8739358729264859}
(Final BLEU[j=10]: 0.78907155028864)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.7772228427990588)

Final lambda[j=11]: {0.7980226618452084, -8.605981116285653, -2.1004912696182423}
(Final BLEU[j=11]: 0.7894391208260589)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.7575144550478031)

Final lambda[j=12]: {0.6024015217412397, -7.254692065204585, -1.6853821919746377}
(Final BLEU[j=12]: 0.7894391208260589)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7789048999152285)

Final lambda[j=13]: {-0.06482709801764988, -8.05413212028372, -0.8102868749874037}
(Final BLEU[j=13]: 0.7869611015697573)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.7580334345990271)

Final lambda[j=14]: {0.03942978540788345, -0.4551471237031588, -0.10855724505023898}
(Final BLEU[j=14]: 0.7894391208260589)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.7624120312018523)

Final lambda[j=15]: {0.009954244141451962, -0.1261800326299552, -0.030961705697594648}
(Final BLEU[j=15]: 0.7896070867732485)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.7799716609206906)

Final lambda[j=16]: {0.24722204227993608, -2.9515562378989806, -0.6893751690924723}
(Final BLEU[j=16]: 0.7894391208260589)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7829654867149577)

Final lambda[j=17]: {0.10231506379882205, -0.8495287747458757, -0.17500460966461745}
(Final BLEU[j=17]: 0.78907155028864)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7790481467994294)

Final lambda[j=18]: {0.25500032842722986, -3.2411663760680636, -0.7942320560662748}
(Final BLEU[j=18]: 0.7896070867732485)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7827235610095149)

Final lambda[j=19]: {0.02600934083112389, -0.3298837115799338, -0.08092275548507938}
(Final BLEU[j=19]: 0.7896070867732485)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:46:11 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7822768365586203)

Final lambda[j=20]: {0.10522600106926558, -1.3283412076675944, -0.3321460862774445}
(Final BLEU[j=20]: 0.789243817857092)

Best final lambda is lambda[j=2] (BLEU: 0.7896).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:46:11 JST 2015  ---

Next iteration will decode with lambda: {0.02029062761299608, -0.25753606284288494, -0.06315278985858824}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:46:11 JST 2015 ---
Redecoding using weight vector {0.02029062761299608, -0.25753606284288494, -0.06315278985858824}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:11 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+...+...+...+...500
            ...+
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 3143 distinct candidates (about 5 per sentence):
newCandidatesAdded[it=1] = 3143 (about 5 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:46:12 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:46:12 JST 2015
----------------------------------------------------

FINAL lambda: {0.02029062761299608, -0.25753606284288494, -0.06315278985858824} (BLEU: 0.7896070867732485)

(OP Lamda) : [0.02029062761299608,-0.25753606284288494,-0.06315278985858824]
Number of candidates => 417
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 417
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:46:12 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:46:12 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:13 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 3460
...+...+...+...+
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 3460 distinct candidates (about 8 per sentence):
newCandidatesAdded[it=1] = 3460 (about 8 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:46:13 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.781459833496604)

Final lambda[j=1]: {0.04134791080174885, -1.0662464026378018, -0.1}
(Final BLEU[j=1]: 0.7870605136279808)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:46:13 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7618636929860854)

Final lambda[j=2]: {0.014974812699148328, -0.7193915364125699, -0.0657480888291131}
(Final BLEU[j=2]: 0.7868675311255638)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:46:13 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7781352317468735)

Final lambda[j=3]: {0.02759750110087659, -0.6886426869128792, -0.059473491599185246}
(Final BLEU[j=3]: 0.7870605136279808)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:46:13 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7815349274171126)

Final lambda[j=4]: {0.14623253381377638, -7.716367024452643, -0.6393613928095458}
(Final BLEU[j=4]: 0.7871723889497272)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:46:13 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.7792764389724194)

Final lambda[j=5]: {0.28489782662029467, -6.994631425879597, -0.5778133794884044}
(Final BLEU[j=5]: 0.7870605136279808)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:46:13 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.7672192143400035)

Final lambda[j=6]: {0.2815800420089276, -14.859915171622454, -1.2303521126716306}
(Final BLEU[j=6]: 0.7871723889497272)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:46:13 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.7816745033798047)

Final lambda[j=7]: {0.44646329764751913, -11.048408927424788, -0.9330111163449395}
(Final BLEU[j=7]: 0.7870605136279808)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:46:13 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.7676526962708923)

Final lambda[j=8]: {0.6515944996339995, -34.386545811594914, -2.8472458630514668}
(Final BLEU[j=8]: 0.7871723889497272)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:46:13 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7716604778854411)

Final lambda[j=9]: {-7.058101308380104E-4, -0.1886940449729046, -0.01427276865927607}
(Final BLEU[j=9]: 0.788439940150387)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:46:13 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.7814012020537117)

Final lambda[j=10]: {0.5053771649119221, -10.362346834167994, -0.8739358729264859}
(Final BLEU[j=10]: 0.7869080925220652)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:46:13 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.7791059530147695)

Final lambda[j=11]: {0.13564120158639914, -7.099916176000612, -0.5477913765324938}
(Final BLEU[j=11]: 0.7874974710048475)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:46:13 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.7662413561416169)

Final lambda[j=12]: {0.594593012768021, -31.378048245332494, -2.598349828640692}
(Final BLEU[j=12]: 0.7871723889497272)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:46:13 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7757606154703808)

Final lambda[j=13]: {0.20361287603402428, -10.58216980712778, -0.8102868749874037}
(Final BLEU[j=13]: 0.7874974710048475)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:46:13 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.7689780726455108)

Final lambda[j=14]: {0.03753776574773382, -1.8029126176607724, -0.16437506388173775}
(Final BLEU[j=14]: 0.7868675311255638)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:46:13 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.760084486819461)

Final lambda[j=15]: {0.009954244141451962, -0.5346618626091739, -0.040726528367599164}
(Final BLEU[j=15]: 0.7875385260565213)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:46:14 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.78191034012654)

Final lambda[j=16]: {0.15698922825497008, -7.5418728454118025, -0.6893751690924723}
(Final BLEU[j=16]: 0.7868675311255638)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:46:14 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.781668082041968)

Final lambda[j=17]: {0.10428258084870608, -5.303067626561496, -0.3964594524802063}
(Final BLEU[j=17]: 0.7870199654840235)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:46:14 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7772970738501721)

Final lambda[j=18]: {0.22126578354613607, -11.068975638275273, -0.8706362780408492}
(Final BLEU[j=18]: 0.7871926699772406)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:46:14 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7781352317468735)

Final lambda[j=19]: {0.020349900665698105, -1.0674871579890992, -0.08254985860243691}
(Final BLEU[j=19]: 0.7874974710048475)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:46:14 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.780962414446485)

Final lambda[j=20]: {0.08380348560010381, -4.3870998761635835, -0.33852947919372234}
(Final BLEU[j=20]: 0.7874974710048475)

Best final lambda is lambda[j=9] (BLEU: 0.7884).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:46:14 JST 2015  ---

Next iteration will decode with lambda: {-7.058101308380104E-4, -0.1886940449729046, -0.01427276865927607}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:46:14 JST 2015 ---
Redecoding using weight vector {-7.058101308380104E-4, -0.1886940449729046, -0.01427276865927607}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:14 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+...+...+...+
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 3460 distinct candidates (about 8 per sentence):
newCandidatesAdded[it=1] = 3460 (about 8 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:46:15 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:46:15 JST 2015
----------------------------------------------------

FINAL lambda: {-7.058101308380104E-4, -0.1886940449729046, -0.01427276865927607} (BLEU: 0.788439940150387)

(OP Lamda) : [-7.058101308380104E-4,-0.1886940449729046,-0.01427276865927607]
Number of candidates => 290
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 290
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:46:15 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:46:15 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:16 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 7763
...+...+...
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 7763 distinct candidates (about 26 per sentence):
newCandidatesAdded[it=1] = 7763 (about 26 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:46:17 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.7819576232915575)

Final lambda[j=1]: {0.1, 4.042882164995636, -0.6893144105474058}
(Final BLEU[j=1]: 0.7845402520547092)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:46:17 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7736949313445843)

Final lambda[j=2]: {29.111582942990637, 0.08576024688617657, 0.46016061237234096}
(Final BLEU[j=2]: 0.7837571945475339)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:46:17 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7762271843850813)

Final lambda[j=3]: {0.008525573566853694, 0.3759053726037934, -0.059473491599185246}
(Final BLEU[j=3]: 0.7845402520547092)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:46:17 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7777731452898836)

Final lambda[j=4]: {0.09165898515734403, 4.048159921484489, -0.6393613928095458}
(Final BLEU[j=4]: 0.7845402520547092)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:46:17 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.7807533723299229)

Final lambda[j=5]: {0.7737957524668755, 37.63788709859227, -5.6893669881213125}
(Final BLEU[j=5]: 0.7845402520547092)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:46:17 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.7804425030480088)

Final lambda[j=6]: {41.31678033703301, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.7837571945475339)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:46:17 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.7824785348888919)

Final lambda[j=7]: {0.32669861662639876, 19.52727639291622, -0.5687262146950123}
(Final BLEU[j=7]: 0.7847943369534228)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:46:17 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.7807074330277335)

Final lambda[j=8]: {14.69605743442257, 0.08804197789590318, 0.23341834353332247}
(Final BLEU[j=8]: 0.7837571945475339)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:46:17 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7781924800348404)

Final lambda[j=9]: {0.010667833667804263, 0.7361316638002987, -0.022109168640897684}
(Final BLEU[j=9]: 0.7848363911473392)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:46:18 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.7819808713060743)

Final lambda[j=10]: {0.5053771649119221, 29.96047698002826, -0.8739358729264859}
(Final BLEU[j=10]: 0.7847943369534228)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:46:18 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.7805200707488035)

Final lambda[j=11]: {0.0756966992313566, 3.4739755256990246, -0.5477913765324938}
(Final BLEU[j=11]: 0.7845402520547092)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:46:18 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.7803626172274565)

Final lambda[j=12]: {0.594593012768021, 26.948188794936886, -4.253709013035938}
(Final BLEU[j=12]: 0.7845402520547092)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:46:18 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.777171771177876)

Final lambda[j=13]: {0.11615465945004788, 5.121463939212749, -0.8102868749874037}
(Final BLEU[j=13]: 0.7845402520547092)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:46:18 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.7809029724223299)

Final lambda[j=14]: {2.288332072827305, 0.00790321507749736, 0.03620028521108054}
(Final BLEU[j=14]: 0.7837571945475339)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:46:18 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.7750244210019647)

Final lambda[j=15]: {8.759800121094901, 0.02541025170060851, 0.1384543976610042}
(Final BLEU[j=15]: 0.7837571945475339)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:46:18 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.7814974095701624)

Final lambda[j=16]: {0.4764012133031911, 24.006865132159653, -0.6893751690924723}
(Final BLEU[j=16]: 0.7847943369534228)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:46:18 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.778038743122716)

Final lambda[j=17]: {0.02861351133895934, 1.9689322444619317, -0.05953463132776107}
(Final BLEU[j=17]: 0.7848363911473392)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:46:18 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7778150477841923)

Final lambda[j=18]: {0.12180353462537835, 5.515246032175645, -0.8706362780408492}
(Final BLEU[j=18]: 0.7845402520547092)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:46:18 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7769934860679094)

Final lambda[j=19]: {0.006665380571668919, 0.3073471861931083, -0.047952155443115936}
(Final BLEU[j=19]: 0.7845402520547092)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:46:18 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7782343870588775)

Final lambda[j=20]: {0.024453429217661735, 1.6826375955255752, -0.05088040056390061}
(Final BLEU[j=20]: 0.7848363911473392)

Best final lambda is lambda[j=9] (BLEU: 0.7848).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:46:18 JST 2015  ---

Next iteration will decode with lambda: {0.010667833667804263, 0.7361316638002987, -0.022109168640897684}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:46:18 JST 2015 ---
Redecoding using weight vector {0.010667833667804263, 0.7361316638002987, -0.022109168640897684}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:19 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+...+...
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 7763 distinct candidates (about 26 per sentence):
newCandidatesAdded[it=1] = 7763 (about 26 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:46:20 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:46:20 JST 2015
----------------------------------------------------

FINAL lambda: {0.010667833667804263, 0.7361316638002987, -0.022109168640897684} (BLEU: 0.7848363911473392)

(OP Lamda) : [0.010667833667804263,0.7361316638002987,-0.022109168640897684]
Number of candidates => 200
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 200
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:46:20 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:46:20 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:20 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 2425
...+...+
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2425 distinct candidates (about 12 per sentence):
newCandidatesAdded[it=1] = 2425 (about 12 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.7969471687773688)

Final lambda[j=1]: {0.1, -3111.427844477889, 0.007778082100762592}
(Final BLEU[j=1]: 0.8038817183020814)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7877635517658133)

Final lambda[j=2]: {7.227994881862584, -223017.73581937034, 0.46016061237234096}
(Final BLEU[j=2]: 0.8038817183020814)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7951367590856653)

Final lambda[j=3]: {-0.0025134081849562733, -0.8718704772700298, -0.059473491599185246}
(Final BLEU[j=3]: 0.8017463247763648)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7946771276153269)

Final lambda[j=4]: {0.5538887638929867, -17710.371164892225, 0.04089487375336241}
(Final BLEU[j=4]: 0.8038817183020814)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.7970736278444942)

Final lambda[j=5]: {0.7737957524668755, -24033.86648925179, 0.06038028812690495}
(Final BLEU[j=5]: 0.8038817183020814)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.7933887277612652)

Final lambda[j=6]: {0.2815800420089276, -8559.572332899897, 0.02282673452075388}
(Final BLEU[j=6]: 0.8038817183020814)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.7987703006648254)

Final lambda[j=7]: {0.32669861662639876, -10295.639109418642, 0.0248112663106963}
(Final BLEU[j=7]: 0.8038817183020814)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.7942884748483419)

Final lambda[j=8]: {0.6515944996339995, -19151.806866780687, 1.9080206053022832}
(Final BLEU[j=8]: 0.8033558455177046)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7974528831517499)

Final lambda[j=9]: {-9.960211894492782E-4, -0.33860525614157627, -0.024517218940201528}
(Final BLEU[j=9]: 0.8017463247763648)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.7989460029387035)

Final lambda[j=10]: {0.5053771649119221, -15803.862352593223, 0.038944160051358596}
(Final BLEU[j=10]: 0.8038817183020814)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.7982431112187403)

Final lambda[j=11]: {0.7980226618452084, -24775.979812430458, 0.06231832716635248}
(Final BLEU[j=11]: 0.8038817183020814)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.7974512212167143)

Final lambda[j=12]: {0.594593012768021, -18686.260053434606, 0.045394598360610626}
(Final BLEU[j=12]: 0.8038817183020814)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7968759163998325)

Final lambda[j=13]: {-0.06550519858638351, -22.080391544361543, -1.4736574777798392}
(Final BLEU[j=13]: 0.8017463247763648)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.7956852829261767)

Final lambda[j=14]: {0.03753776574773382, -1195.7728099275184, 0.0027920706114867304}
(Final BLEU[j=14]: 0.8038817183020814)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.7861544637515763)

Final lambda[j=15]: {4.415083091878278, -136570.03548903792, 0.593645578752771}
(Final BLEU[j=15]: 0.8038817183020814)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.7978641041949476)

Final lambda[j=16]: {0.6576106932759427, -20467.926784446605, 0.05111808574060136}
(Final BLEU[j=16]: 0.8038817183020814)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7944820355491472)

Final lambda[j=17]: {0.152320156332121, -4845.857908659598, 0.011358673507896534}
(Final BLEU[j=17]: 0.8038817183020814)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7947378447549641)

Final lambda[j=18]: {19.000207348460734, -585979.596763178, 1.5017091774930917}
(Final BLEU[j=18]: 0.8038817183020814)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7951367590856653)

Final lambda[j=19]: {-0.006073419774290739, -2.0647704064349246, -0.14948946180619277}
(Final BLEU[j=19]: 0.8017463247763648)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:46:21 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7949026052028627)

Final lambda[j=20]: {0.12942390232021425, -4101.4850673299725, 0.009724524948288274}
(Final BLEU[j=20]: 0.8038817183020814)

Best final lambda is lambda[j=1] (BLEU: 0.8039).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:46:21 JST 2015  ---

Next iteration will decode with lambda: {0.1, -3111.427844477889, 0.007778082100762592}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:46:21 JST 2015 ---
Redecoding using weight vector {0.1, -3111.427844477889, 0.007778082100762592}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:21 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+...+
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2425 distinct candidates (about 12 per sentence):
newCandidatesAdded[it=1] = 2425 (about 12 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:46:22 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:46:22 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, -3111.427844477889, 0.007778082100762592} (BLEU: 0.8038817183020814)

(OP Lamda) : [0.1,-3111.427844477889,0.007778082100762592]
Number of candidates => 118
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 118
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:46:22 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:46:22 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:22 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 2526
...+
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2526 distinct candidates (about 21 per sentence):
newCandidatesAdded[it=1] = 2526 (about 21 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.7970723622717774)

Final lambda[j=1]: {0.04802334517102416, -972.0823886461662, -0.1}
(Final BLEU[j=1]: 0.803046104333626)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.791274059269817)

Final lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, -0.11530464493618953}
(Final BLEU[j=2]: 0.8013748838671926)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.798770529931865)

Final lambda[j=3]: {12.026500560317828, -237806.62466987857, -26.984959141680278}
(Final BLEU[j=3]: 0.803046104333626)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.798949459955829)

Final lambda[j=4]: {-0.1183915971434157, -2.367676437490615, -0.6393613928095458}
(Final BLEU[j=4]: 0.8013748838671926)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.7980263792671133)

Final lambda[j=5]: {0.35572286027362704, -7176.291785946325, -0.5778133794884044}
(Final BLEU[j=5]: 0.803046104333626)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.7917493449303427)

Final lambda[j=6]: {-0.11106255019558353, -440306.3414227528, 0.6539535776649275}
(Final BLEU[j=6]: 0.8004221383588883)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.7970468827927991)

Final lambda[j=7]: {0.32669861662639876, -6681.813644063592, -0.9330111163449395}
(Final BLEU[j=7]: 0.803046104333626)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.7930018966870578)

Final lambda[j=8]: {0.783853239218365, -15077.711790995469, -2.1831193460645246}
(Final BLEU[j=8]: 0.803046104333626)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7978282115387586)

Final lambda[j=9]: {-0.004553671263154424, -0.08368296732864104, -0.021652385558302317}
(Final BLEU[j=9]: 0.8013748838671926)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.796893300191509)

Final lambda[j=10]: {0.5053771649119221, -10175.632257594005, -0.8739358729264859}
(Final BLEU[j=10]: 0.803046104333626)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.7979554215125093)

Final lambda[j=11]: {0.7980226618452084, -7103.266709948071, -0.5477913765324938}
(Final BLEU[j=11]: 0.8026185914106343)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.7938498252043396)

Final lambda[j=12]: {0.594593012768021, -11849.373321547382, -1.3378274714267595}
(Final BLEU[j=12]: 0.803046104333626)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7993979412633848)

Final lambda[j=13]: {-0.15006158860440433, -3.047687130618659, -0.8102868749874037}
(Final BLEU[j=13]: 0.8013748838671926)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.7937788855829527)

Final lambda[j=14]: {0.03753776574773382, -747.8202163863348, -0.084449434718797}
(Final BLEU[j=14]: 0.803046104333626)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.7900205911603778)

Final lambda[j=15]: {0.009954244141451962, -198.6579698531541, -0.022408305427163194}
(Final BLEU[j=15]: 0.803046104333626)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.7970723622717774)

Final lambda[j=16]: {0.3161875424747594, -6397.682651358864, -0.6893751690924723}
(Final BLEU[j=16]: 0.803046104333626)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7994743215428654)

Final lambda[j=17]: {-0.0731573960334511, -0.8495287747458757, -0.3964594524802063}
(Final BLEU[j=17]: 0.8013748838671926)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.7989239731683835)

Final lambda[j=18]: {-0.1739731287134254, -3.3839704164998268, -0.8706362780408492}
(Final BLEU[j=18]: 0.8013748838671926)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.798770529931865)

Final lambda[j=19]: {20.420524372153558, -403781.72792778356, -45.819215826383065}
(Final BLEU[j=19]: 0.803046104333626)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:46:23 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7994743215428654)

Final lambda[j=20]: {-0.06249952269581503, -0.8022503514522665, -0.33852947919372234}
(Final BLEU[j=20]: 0.8013748838671926)

Best final lambda is lambda[j=1] (BLEU: 0.8030).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:46:23 JST 2015  ---

Next iteration will decode with lambda: {0.04802334517102416, -972.0823886461662, -0.1}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:46:23 JST 2015 ---
Redecoding using weight vector {0.04802334517102416, -972.0823886461662, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:24 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ...+
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2526 distinct candidates (about 21 per sentence):
newCandidatesAdded[it=1] = 2526 (about 21 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:46:24 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:46:24 JST 2015
----------------------------------------------------

FINAL lambda: {0.04802334517102416, -972.0823886461662, -0.1} (BLEU: 0.803046104333626)

(OP Lamda) : [0.04802334517102416,-972.0823886461662,-0.1]
Number of candidates => 67
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 67
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:46:24 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:46:24 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:25 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 841
..
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 841 distinct candidates (about 12 per sentence):
newCandidatesAdded[it=1] = 841 (about 12 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.7851036448654822)

Final lambda[j=1]: {0.1, -606769.0760568177, 0.11764741435254436}
(Final BLEU[j=1]: 0.7923118306283721)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.7848174751842202)

Final lambda[j=2]: {0.222836490870382, -1351729.0508900625, 0.46016061237234096}
(Final BLEU[j=2]: 0.7923118306283721)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.7818576462887094)

Final lambda[j=3]: {0.001545911779179432, -3499.521692115366, 0.001818725198516008}
(Final BLEU[j=3]: 0.7923118306283721)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.7830238291391977)

Final lambda[j=4]: {0.2972696710334086, -1803748.341996816, 0.3497300712367958}
(Final BLEU[j=4]: 0.7923118306283721)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.7851036448654822)

Final lambda[j=5]: {0.7737957524668755, -4695148.8086972935, 0.9103506832960307}
(Final BLEU[j=5]: 0.7923118306283721)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.7904014196668079)

Final lambda[j=6]: {0.2815800420089276, -1708010.113953502, 0.6539535776649275}
(Final BLEU[j=6]: 0.7923118306283721)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.7850013445642863)

Final lambda[j=7]: {0.32669861662639876, -1982320.1893532195, 0.38435246371529197}
(Final BLEU[j=7]: 0.7923118306283721)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.7900024524204821)

Final lambda[j=8]: {0.6515944996339995, -3952874.0154401786, 0.9795005730080566}
(Final BLEU[j=8]: 0.7923118306283721)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.7821185186739462)

Final lambda[j=9]: {0.006665792840334632, -40446.14542482727, 0.007842132782528674}
(Final BLEU[j=9]: 0.7923118306283721)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.7868524339786626)

Final lambda[j=10]: {0.5053771649119221, -3066480.87145515, 0.5945631821014543}
(Final BLEU[j=10]: 0.7923118306283721)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.7865923261928178)

Final lambda[j=11]: {0.7980226618452084, -4842148.949207973, 0.9388530140776179}
(Final BLEU[j=11]: 0.7923118306283721)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.7903642406780061)

Final lambda[j=12]: {0.594593012768021, -3607530.350203748, 0.3275255626321376}
(Final BLEU[j=12]: 0.7923118306283721)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.7834892236134176)

Final lambda[j=13]: {0.37681172156011933, -2286387.0189069444, 0.44330923845179676}
(Final BLEU[j=13]: 0.7923118306283721)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.7888792906094344)

Final lambda[j=14]: {0.03753776574773382, -227737.68044905568, 0.03620028521108054}
(Final BLEU[j=14]: 0.7923118306283721)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.7827464460170541)

Final lambda[j=15]: {0.2875118147620353, -1744050.4630196348, 0.593645578752771}
(Final BLEU[j=15]: 0.7923118306283721)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.7851036448654822)

Final lambda[j=16]: {0.6576106932759427, -3990179.061711858, 0.773661950337329}
(Final BLEU[j=16]: 0.7923118306283721)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.7826224859558762)

Final lambda[j=17]: {0.1843225621703914, -1118417.2098860182, 0.21685072060090096}
(Final BLEU[j=17]: 0.7923118306283721)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.781494362387969)

Final lambda[j=18]: {0.4048371754713388, -2456437.5536646834, 0.4762804662132424}
(Final BLEU[j=18]: 0.7923118306283721)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.7818576462887094)

Final lambda[j=19]: {0.0023219017692566284, -5939.792190582482, 0.0027316573339004196}
(Final BLEU[j=19]: 0.7923118306283721)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:46:25 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.7830238291391977)

Final lambda[j=20]: {0.1573870693204965, -954980.2525132082, 0.18516181193726666}
(Final BLEU[j=20]: 0.7923118306283721)

Best final lambda is lambda[j=1] (BLEU: 0.7923).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:46:25 JST 2015  ---

Next iteration will decode with lambda: {0.1, -606769.0760568177, 0.11764741435254436}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:46:25 JST 2015 ---
Redecoding using weight vector {0.1, -606769.0760568177, 0.11764741435254436}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:25 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: ..
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 841 distinct candidates (about 12 per sentence):
newCandidatesAdded[it=1] = 841 (about 12 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:46:26 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:46:26 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, -606769.0760568177, 0.11764741435254436} (BLEU: 0.7923118306283721)

(OP Lamda) : [0.1,-606769.0760568177,0.11764741435254436]
Number of candidates => 28
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 28
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:46:26 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:46:26 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:26 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 361
.
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 361 distinct candidates (about 12 per sentence):
newCandidatesAdded[it=1] = 361 (about 12 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.8102830047670828)

Final lambda[j=1]: {0.1, -1084360.5479897321, -0.1}
(Final BLEU[j=1]: 0.8175162425335318)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.8089135423523189)

Final lambda[j=2]: {-0.017230818545013626, -33943.9764312808, -0.21125252567682617}
(Final BLEU[j=2]: 0.8203390351849537)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.8082726347691156)

Final lambda[j=3]: {-0.5203200954373215, -1024429.8462970983, -6.383127740575084}
(Final BLEU[j=3]: 0.8203390351849537)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.8099465579749459)

Final lambda[j=4]: {-0.04516310404796672, -88888.24384598467, -0.6393613928095458}
(Final BLEU[j=4]: 0.8203390351849537)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.8102830047670828)

Final lambda[j=5]: {0.7737957524668755, -8399893.277812755, -0.5778133794884044}
(Final BLEU[j=5]: 0.8175162425335318)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8106185238987442)

Final lambda[j=6]: {0.2815800420089276, -3097056.361632669, -8.057644172080572}
(Final BLEU[j=6]: 0.8175162425335318)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.8105152108389944)

Final lambda[j=7]: {0.32669861662639876, -3514260.5258949553, -0.9330111163449395}
(Final BLEU[j=7]: 0.8175162425335318)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8122187601597111)

Final lambda[j=8]: {0.6515944996339995, -7141847.766549751, -18.63508274604869}
(Final BLEU[j=8]: 0.8175162425335318)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.8121157755335673)

Final lambda[j=9]: {-0.004553671263154424, -8961.198445700144, -0.05589203880608078}
(Final BLEU[j=9]: 0.8203390351849537)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.8102830047670828)

Final lambda[j=10]: {0.5053771649119221, -5462889.426573416, -0.8739358729264859}
(Final BLEU[j=10]: 0.8175162425335318)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.8102830047670828)

Final lambda[j=11]: {0.7980226618452084, -8665135.144197214, -0.5477913765324938}
(Final BLEU[j=11]: 0.8175162425335318)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8122187601597111)

Final lambda[j=12]: {0.594593012768021, -6490618.699131157, -16.99339747071599}
(Final BLEU[j=12]: 0.8175162425335318)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.8115476010955286)

Final lambda[j=13]: {-0.21052430917292875, -415444.7453432005, -2.576173769031085}
(Final BLEU[j=13]: 0.8203390351849537)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8122187601597111)

Final lambda[j=14]: {0.03753776574773382, -410490.18534472625, -1.073139787883572}
(Final BLEU[j=14]: 0.8175162425335318)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8050985674279957)

Final lambda[j=15]: {-0.0020707523222083133, -4101.59424302515, -0.02523655585438058}
(Final BLEU[j=15]: 0.8203390351849537)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.8102830047670828)

Final lambda[j=16]: {0.6576106932759427, -7129386.699477026, -0.6893751690924723}
(Final BLEU[j=16]: 0.8175162425335318)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.8082726347691156)

Final lambda[j=17]: {0.117209309087281, -1257923.3443598598, -0.3964594524802063}
(Final BLEU[j=17]: 0.8175162425335318)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.8115476010955286)

Final lambda[j=18]: {-0.057192559639920655, -112561.17124973363, -0.8706362780408492}
(Final BLEU[j=18]: 0.8203390351849537)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.8082726347691156)

Final lambda[j=19]: {-0.8834296944416113, -1739337.422507347, -10.837639118908832}
(Final BLEU[j=19]: 0.8203390351849537)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:46:26 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.8099465579749459)

Final lambda[j=20]: {-0.023241800375732397, -45743.13337369204, -0.33852947919372234}
(Final BLEU[j=20]: 0.8203390351849537)

Best final lambda is lambda[j=2] (BLEU: 0.8203).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:46:26 JST 2015  ---

Next iteration will decode with lambda: {-0.017230818545013626, -33943.9764312808, -0.21125252567682617}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:46:26 JST 2015 ---
Redecoding using weight vector {-0.017230818545013626, -33943.9764312808, -0.21125252567682617}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:26 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: .
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 361 distinct candidates (about 12 per sentence):
newCandidatesAdded[it=1] = 361 (about 12 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:46:26 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:46:26 JST 2015
----------------------------------------------------

FINAL lambda: {-0.017230818545013626, -33943.9764312808, -0.21125252567682617} (BLEU: 0.8203390351849537)

(OP Lamda) : [-0.017230818545013626,-33943.9764312808,-0.21125252567682617]
Number of candidates => 15
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 15
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:46:26 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:46:26 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:27 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 74

Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 74 distinct candidates (about 4 per sentence):
newCandidatesAdded[it=1] = 74 (about 4 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.8444737982240109)

Final lambda[j=1]: {0.1, -18472.163411124973, -0.1}
(Final BLEU[j=1]: 0.855805997158232)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.8263299335238166)

Final lambda[j=2]: {0.058811101289754406, -8320.81128929788, -0.25107884872525343}
(Final BLEU[j=2]: 0.855805997158232)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.8281354275442528)

Final lambda[j=3]: {0.006113547411662774, -61880.554485458255, -0.059473491599185246}
(Final BLEU[j=3]: 0.8529250955811386)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.8281354275442528)

Final lambda[j=4]: {0.06565137722376861, -665237.6957433678, -0.6393613928095458}
(Final BLEU[j=4]: 0.8529250955811386)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.8444737982240109)

Final lambda[j=5]: {0.7737957524668755, -144548.63692642053, -0.5778133794884044}
(Final BLEU[j=5]: 0.855805997158232)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8355464440318844)

Final lambda[j=6]: {0.5005932573644605, -75673.28277437169, -1.1949184828775725}
(Final BLEU[j=6]: 0.855805997158232)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.8444737982240109)

Final lambda[j=7]: {0.32669861662639876, -55361.79643385536, -0.9330111163449395}
(Final BLEU[j=7]: 0.855805997158232)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8459202112481061)

Final lambda[j=8]: {0.12992103469971514, -18410.140000814157, -0.5491376517219378}
(Final BLEU[j=8]: 0.855805997158232)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.8326108682749941)

Final lambda[j=9]: {0.001498661898739866, -14850.466568653925, -0.01427276865927607}
(Final BLEU[j=9]: 0.8529250955811386)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.8444737982240109)

Final lambda[j=10]: {0.5053771649119221, -90322.95228413232, -0.8739358729264859}
(Final BLEU[j=10]: 0.855805997158232)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.8444737982240109)

Final lambda[j=11]: {0.7980226618452084, -149470.0314406812, -0.5477913765324938}
(Final BLEU[j=11]: 0.855805997158232)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8459202112481061)

Final lambda[j=12]: {0.594593012768021, -83754.75446284679, -1.7598073716325158}
(Final BLEU[j=12]: 0.855805997158232)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.8326108682749941)

Final lambda[j=13]: {0.08328116918498323, -843080.8790201833, -0.8102868749874037}
(Final BLEU[j=13]: 0.8529250955811386)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8459202112481061)

Final lambda[j=14]: {0.025998635308226777, -3819.799923802133, -0.08350645280866839}
(Final BLEU[j=14]: 0.855805997158232)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8369986988272725)

Final lambda[j=15]: {0.009954244141451962, -1595.9208625970587, -0.039481729102096505}
(Final BLEU[j=15]: 0.855805997158232)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.8444737982240109)

Final lambda[j=16]: {0.6576106932759427, -121213.6807686192, -0.6893751690924723}
(Final BLEU[j=16]: 0.855805997158232)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.8281354275442528)

Final lambda[j=17]: {0.04069796742195213, -412505.0210019011, -0.3964594524802063}
(Final BLEU[j=17]: 0.8529250955811386)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.8281354275442528)

Final lambda[j=18]: {0.08944027215578668, -905872.74487382, -0.8706362780408492}
(Final BLEU[j=18]: 0.8529250955811386)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.8281354275442528)

Final lambda[j=19]: {0.00849226936792954, -85890.86888698158, -0.08254985860243691}
(Final BLEU[j=19]: 0.8529250955811386)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.8281354275442528)

Final lambda[j=20]: {0.03474835650044282, -352230.5041473056, -0.33852947919372234}
(Final BLEU[j=20]: 0.8529250955811386)

Best final lambda is lambda[j=1] (BLEU: 0.8558).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:46:27 JST 2015  ---

Next iteration will decode with lambda: {0.1, -18472.163411124973, -0.1}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:46:27 JST 2015 ---
Redecoding using weight vector {0.1, -18472.163411124973, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:27 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: 
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 74 distinct candidates (about 4 per sentence):
newCandidatesAdded[it=1] = 74 (about 4 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:46:27 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:46:27 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, -18472.163411124973, -0.1} (BLEU: 0.855805997158232)

(OP Lamda) : [0.1,-18472.163411124973,-0.1]
Number of candidates => 8
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 8
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:46:27 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:46:27 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:27 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 52

Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 52 distinct candidates (about 6 per sentence):
newCandidatesAdded[it=1] = 52 (about 6 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.8323519335204085)

Final lambda[j=1]: {-0.1606425610120449, 548390.8096473007, 7.606520941952468}
(Final BLEU[j=1]: 0.8628416930896428)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.860123862708645)

Final lambda[j=2]: {-0.017230818545013626, 58380.15343677503, 0.46016061237234096}
(Final BLEU[j=2]: 0.8628416930896428)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.8528958785506814)

Final lambda[j=3]: {-0.5203200954373215, 1775986.0845796433, 24.635098478667672}
(Final BLEU[j=3]: 0.8628416930896428)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.851370119528461)

Final lambda[j=4]: {-0.6919952429797738, 2362482.458118977, 32.76825487578382}
(Final BLEU[j=4]: 0.8628416930896428)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.8323519335204085)

Final lambda[j=5]: {-0.6893065838457904, 2353247.986505469, 32.6404090936192}
(Final BLEU[j=5]: 0.8628416930896428)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.8345981638960608)

Final lambda[j=6]: {-0.05808610518293093, 197643.76064958895, 0.6539535776649275}
(Final BLEU[j=6]: 0.8628416930896428)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.8323519335204085)

Final lambda[j=7]: {-1.0823039566293737, 3694939.6827935562, 51.25005878710368}
(Final BLEU[j=7]: 0.8628416930896428)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.8345981638960608)

Final lambda[j=8]: {-0.0621114622389674, 211077.8419859219, 0.9795005730080566}
(Final BLEU[j=8]: 0.8628416930896428)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.851370119528461)

Final lambda[j=9]: {-0.004553671263154424, 15555.736264914962, 0.21572147494561467}
(Final BLEU[j=9]: 0.8628416930896428)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.8323519335204085)

Final lambda[j=10]: {-1.0169407383600666, 3471789.878635532, 48.154906835692024}
(Final BLEU[j=10]: 0.8628416930896428)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.8323519335204085)

Final lambda[j=11]: {-0.6560876410426663, 2239838.630296967, 31.06738870028379}
(Final BLEU[j=11]: 0.8628416930896428)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.8330710287340123)

Final lambda[j=12]: {-0.05404979648636289, 150827.61187801854, 0.3275255626321376}
(Final BLEU[j=12]: 0.8628416930896428)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.8413507694443011)

Final lambda[j=13]: {-0.21052430917292875, 719310.2791248767, 9.974533988698655}
(Final BLEU[j=13]: 0.8628416930896428)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.8345981638960608)

Final lambda[j=14]: {-0.050447614737939434, 172151.3360265378, 2.3881192125284474}
(Final BLEU[j=14]: 0.8628416930896428)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.8422482115407249)

Final lambda[j=15]: {-0.05734038669625075, 195155.02548343694, 0.593645578752771}
(Final BLEU[j=15]: 0.8628416930896428)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.8323519335204085)

Final lambda[j=16]: {-0.8127401144838355, 2774649.6414218107, 38.48537124215455}
(Final BLEU[j=16]: 0.8628416930896428)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.8528958785506814)

Final lambda[j=17]: {-0.871756060640207, 2975805.4110704865, 41.27685941009545}
(Final BLEU[j=17]: 0.8628416930896428)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.851370119528461)

Final lambda[j=18]: {-0.4629095033959656, 1580795.269521917, 21.92425677291235}
(Final BLEU[j=18]: 0.8628416930896428)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.8528958785506814)

Final lambda[j=19]: {-0.8834296944416113, 3015355.1107391445, 41.826736794869426}
(Final BLEU[j=19]: 0.8628416930896428)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:46:27 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.851370119528461)

Final lambda[j=20]: {-0.2916952155637593, 995916.6540226294, 13.813347678433752}
(Final BLEU[j=20]: 0.8628416930896428)

Best final lambda is lambda[j=1] (BLEU: 0.8628).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:46:27 JST 2015  ---

Next iteration will decode with lambda: {-0.1606425610120449, 548390.8096473007, 7.606520941952468}

--- Starting Z-MERT iteration #2 @ Tue Oct 27 10:46:27 JST 2015 ---
Redecoding using weight vector {-0.1606425610120449, 548390.8096473007, 7.606520941952468}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:27 JST 2015
Reading candidate translations from iterations 1-2
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: 
Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 52 distinct candidates (about 6 per sentence):
newCandidatesAdded[it=1] = 52 (about 6 per sentence)
newCandidatesAdded[it=2] = 0 (about 0 per sentence)

No new candidates added in this iteration; exiting Z-MERT.

---  Z-MERT iteration #2 ending @ Tue Oct 27 10:46:27 JST 2015  ---


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:46:27 JST 2015
----------------------------------------------------

FINAL lambda: {-0.1606425610120449, 548390.8096473007, 7.606520941952468} (BLEU: 0.8628416930896428)

(OP Lamda) : [-0.1606425610120449,548390.8096473007,7.606520941952468]
Number of candidates => 3
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 3
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:46:27 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:46:27 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:27 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 4

Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 4 distinct candidates (about 1 per sentence):
newCandidatesAdded[it=1] = 4 (about 1 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.9037664163933351)

Final lambda[j=1]: {0.1, 0.2, -0.1}
(Final BLEU[j=1]: 0.9037664163933351)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.8830710861889355)

Final lambda[j=2]: {0.05543437910298102, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.9037664163933351)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.8830710861889355)

Final lambda[j=3]: {0.04929764744379454, 0.09518077619965482, -0.059473491599185246}
(Final BLEU[j=3]: 0.9037664163933351)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.8830710861889355)

Final lambda[j=4]: {0.04244681342709934, -0.8743327758402839, -0.6393613928095458}
(Final BLEU[j=4]: 0.9037664163933351)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.9037664163933351)

Final lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Final BLEU[j=5]: 0.9037664163933351)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.9037664163933351)

Final lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.9037664163933351)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.9037664163933351)

Final lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Final BLEU[j=7]: 0.9037664163933351)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.9037664163933351)

Final lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.9037664163933351)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.8830710861889355)

Final lambda[j=9]: {0.04983261141547929, 0.8593774528720801, -0.01427276865927607}
(Final BLEU[j=9]: 0.9037664163933351)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.9037664163933351)

Final lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Final BLEU[j=10]: 0.9037664163933351)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.9037664163933351)

Final lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Final BLEU[j=11]: 0.9037664163933351)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.9037664163933351)

Final lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Final BLEU[j=12]: 0.9037664163933351)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.8830710861889355)

Final lambda[j=13]: {0.04043047339067127, 0.981831267676273, -0.8102868749874037}
(Final BLEU[j=13]: 0.9037664163933351)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.9037664163933351)

Final lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Final BLEU[j=14]: 0.9037664163933351)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.9037664163933351)

Final lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.9037664163933351)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.9037664163933351)

Final lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Final BLEU[j=16]: 0.9037664163933351)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.8830710861889355)

Final lambda[j=17]: {0.04531594164564713, -0.8495287747458757, -0.3964594524802063}
(Final BLEU[j=17]: 0.9037664163933351)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.8830710861889355)

Final lambda[j=18]: {0.03971613049312114, -0.10291979788243277, -0.8706362780408492}
(Final BLEU[j=18]: 0.9037664163933351)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.8830710861889355)

Final lambda[j=19]: {0.04902537114995345, 0.3073471861931083, -0.08254985860243691}
(Final BLEU[j=19]: 0.9037664163933351)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.8830710861889355)

Final lambda[j=20]: {0.04600026109615006, -0.8022503514522665, -0.33852947919372234}
(Final BLEU[j=20]: 0.9037664163933351)

Best final lambda is lambda[j=1] (BLEU: 0.9038).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:46:28 JST 2015  ---

No parameter value changed in this iteration; exiting Z-MERT.


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:46:28 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, 0.2, -0.1} (BLEU: 0.9037664163933351)

(OP Lamda) : [0.1,0.2,-0.1]
Number of candidates => 1
tmpDirPrefix: ZMERT.
Processed the following args array:
  -r ref -rps 4 -p params.txt -m BLEU 4 closest -maxIt 15 -ipi 20 -cmd SDecoder_cmd.bat -decOut nbest.out -dcfg SDecoder_cfg.txt -N 10 -v 1 -seed 12341234 

----------------------------------------------------
Initializing...
----------------------------------------------------

Random number generator initialized using seed: 12341234

$$ decoderCommand: java SimpleDecoder SDecoder_cfg.txt nbest.out
Number of sentences: 1
Number of documents: 1
Optimizing BLEU
docSubsetInfo: {0, 1, 1, 1, 1, 0, 0}
Number of features: 3
Feature names: {"RD","Word Model","Headword Model"}

c    Default value	Optimizable?	Crit. val. range	Rand. val. range
1     0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
2     0.2000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]
3     -0.1000		 Yes		 [-Infinity,Infinity]		 [-1.0,1.0]

Weight vector normalization method: none.

----------------------------------------------------

----------------------------------------------------
Z-MERT run started @ Tue Oct 27 10:46:28 JST 2015
----------------------------------------------------

Initial lambda[]: {0.1, 0.2, -0.1}

--- Starting Z-MERT iteration #1 @ Tue Oct 27 10:46:28 JST 2015 ---
Decoding using initial weight vector {0.1, 0.2, -0.1}
Running decoder...
...finished decoding @ Tue Oct 27 10:46:28 JST 2015
Reading candidate translations from iterations 1-1
(and computing BLEU sufficient statistics for previously unseen candidates)
  Progress: Size: 2

Warning: attempt to delete ZMERT.temp.stats.unknown was unsuccessful!
Warning: attempt to delete ZMERT.temp.stats.mergedKnown was unsuccessful!
Processed 2 distinct candidates (about 2 per sentence):
newCandidatesAdded[it=1] = 2 (about 2 per sentence)

+++ Optimization of lambda[j=1] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=1]: {0.1, 0.2, -0.1}
(Initial BLEU[j=1]: 0.9426092134060646)

Final lambda[j=1]: {0.1, 0.2, -0.1}
(Final BLEU[j=1]: 0.9426092134060646)

+++ Optimization of lambda[j=2] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=2]: {-0.017230818545013626, -0.6577977590753961, 0.46016061237234096}
(Initial BLEU[j=2]: 0.8916809066702428)

Final lambda[j=2]: {0.03518185779521722, -0.6577977590753961, 0.46016061237234096}
(Final BLEU[j=2]: 0.9426092134060646)

+++ Optimization of lambda[j=3] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=3]: {-0.5203200954373215, 0.09518077619965482, -0.059473491599185246}
(Initial BLEU[j=3]: 0.8916809066702428)

Final lambda[j=3]: {0.051915181622963644, 0.09518077619965482, -0.059473491599185246}
(Final BLEU[j=3]: 0.9426092134060646)

+++ Optimization of lambda[j=4] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=4]: {-0.6919952429797738, -0.8743327758402839, -0.6393613928095458}
(Initial BLEU[j=4]: 0.8916809066702428)

Final lambda[j=4]: {0.07058706749076554, -0.8743327758402839, -0.6393613928095458}
(Final BLEU[j=4]: 0.9426092134060646)

+++ Optimization of lambda[j=5] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Initial BLEU[j=5]: 0.9426092134060646)

Final lambda[j=5]: {0.7737957524668755, 0.6493837878144797, -0.5778133794884044}
(Final BLEU[j=5]: 0.9426092134060646)

+++ Optimization of lambda[j=6] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Initial BLEU[j=6]: 0.9426092134060646)

Final lambda[j=6]: {0.2815800420089276, 0.15636862697411513, 0.6539535776649275}
(Final BLEU[j=6]: 0.9426092134060646)

+++ Optimization of lambda[j=7] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Initial BLEU[j=7]: 0.9426092134060646)

Final lambda[j=7]: {0.32669861662639876, 0.009245077311689887, -0.9330111163449395}
(Final BLEU[j=7]: 0.9426092134060646)

+++ Optimization of lambda[j=8] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Initial BLEU[j=8]: 0.9426092134060646)

Final lambda[j=8]: {0.6515944996339995, 0.08804197789590318, 0.9795005730080566}
(Final BLEU[j=8]: 0.9426092134060646)

+++ Optimization of lambda[j=9] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=9]: {-0.004553671263154424, 0.8593774528720801, -0.01427276865927607}
(Initial BLEU[j=9]: 0.8916809066702428)

Final lambda[j=9]: {0.05046041914463138, 0.8593774528720801, -0.01427276865927607}
(Final BLEU[j=9]: 0.9426092134060646)

+++ Optimization of lambda[j=10] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Initial BLEU[j=10]: 0.9426092134060646)

Final lambda[j=10]: {0.5053771649119221, -0.5713556223455967, -0.8739358729264859}
(Final BLEU[j=10]: 0.9426092134060646)

+++ Optimization of lambda[j=11] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Initial BLEU[j=11]: 0.9426092134060646)

Final lambda[j=11]: {0.7980226618452084, -0.399205792984231, -0.5477913765324938}
(Final BLEU[j=11]: 0.9426092134060646)

+++ Optimization of lambda[j=12] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Initial BLEU[j=12]: 0.9426092134060646)

Final lambda[j=12]: {0.594593012768021, -0.2974935383022126, 0.3275255626321376}
(Final BLEU[j=12]: 0.9426092134060646)

+++ Optimization of lambda[j=13] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=13]: {-0.21052430917292875, 0.981831267676273, -0.8102868749874037}
(Initial BLEU[j=13]: 0.8916809066702428)

Final lambda[j=13]: {0.07609277677107433, 0.981831267676273, -0.8102868749874037}
(Final BLEU[j=13]: 0.9426092134060646)

+++ Optimization of lambda[j=14] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Initial BLEU[j=14]: 0.9426092134060646)

Final lambda[j=14]: {0.03753776574773382, 0.00790321507749736, 0.03620028521108054}
(Final BLEU[j=14]: 0.9426092134060646)

+++ Optimization of lambda[j=15] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Initial BLEU[j=15]: 0.9426092134060646)

Final lambda[j=15]: {0.009954244141451962, 0.02541025170060851, 0.593645578752771}
(Final BLEU[j=15]: 0.9426092134060646)

+++ Optimization of lambda[j=16] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Initial BLEU[j=16]: 0.9426092134060646)

Final lambda[j=16]: {0.6576106932759427, 0.17534745529356877, -0.6893751690924723}
(Final BLEU[j=16]: 0.9426092134060646)

+++ Optimization of lambda[j=17] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=17]: {-0.871756060640207, -0.8495287747458757, -0.3964594524802063}
(Initial BLEU[j=17]: 0.8916809066702428)

Final lambda[j=17]: {0.06276547008940801, -0.8495287747458757, -0.3964594524802063}
(Final BLEU[j=17]: 0.9426092134060646)

+++ Optimization of lambda[j=18] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=18]: {-0.4629095033959656, -0.10291979788243277, -0.8706362780408492}
(Initial BLEU[j=18]: 0.8916809066702428)

Final lambda[j=18]: {0.07803502998245095, -0.10291979788243277, -0.8706362780408492}
(Final BLEU[j=18]: 0.9426092134060646)

+++ Optimization of lambda[j=19] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=19]: {-0.8834296944416113, 0.3073471861931083, -0.08254985860243691}
(Initial BLEU[j=19]: 0.8916809066702428)

Final lambda[j=19]: {0.05265846142239927, 0.3073471861931083, -0.08254985860243691}
(Final BLEU[j=19]: 0.9426092134060646)

+++ Optimization of lambda[j=20] starting @ Tue Oct 27 10:46:28 JST 2015 +++
Initial lambda[j=20]: {-0.2916952155637593, -0.8022503514522665, -0.33852947919372234}
(Initial BLEU[j=20]: 0.8916809066702428)

Final lambda[j=20]: {0.06090012773374498, -0.8022503514522665, -0.33852947919372234}
(Final BLEU[j=20]: 0.9426092134060646)

Best final lambda is lambda[j=1] (BLEU: 0.9426).

---  Z-MERT iteration #1 ending @ Tue Oct 27 10:46:28 JST 2015  ---

No parameter value changed in this iteration; exiting Z-MERT.


----------------------------------------------------
Z-MERT run ended @ Tue Oct 27 10:46:28 JST 2015
----------------------------------------------------

FINAL lambda: {0.1, 0.2, -0.1} (BLEU: 0.9426092134060646)

(OP Lamda) : [0.1,0.2,-0.1]
Number of candidates => 0
Processing 526 sentences...

Corpus level score:
 See data/results/bleu_results.txt
